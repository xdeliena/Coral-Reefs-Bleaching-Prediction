{"cells":[{"cell_type":"markdown","metadata":{"id":"Cy9zkmnnL7If"},"source":["# Coral Reef Bleaching Prediction Using BCO-DMO Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WOhPtpRwJoNx","executionInfo":{"status":"ok","timestamp":1753342498091,"user_tz":-480,"elapsed":7693,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n","from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import svm, metrics\n","from sklearn.svm import SVC\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns # Import seaborn for barplot\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qKFBPKfDkHG"},"outputs":[],"source":["#original_df = df\n","# Calculate the number of missing values per row\n","#missing_per_row = df.isnull().sum(axis=1)\n","\n","# Create a boolean mask for rows with more than 10 missing values\n","#rows_to_drop = missing_per_row >= 10\n","\n","# Filter the DataFrame to keep rows with 10 or fewer missing values\n","#df = df[~rows_to_drop]\n","\n","#print(f\"Original number of rows: {len(original_df)}\")\n","#print(f\"Number of rows after dropping: {len(df)}\")\n","\n","# Save the preprocessed dataframe to a CSV file\n","#df.to_csv('/preprocessed_data.csv', index=False)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1753342498498,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"Bfk5_gUQL0Ev","outputId":"52b5b5e6-1fe2-415e-f85e-c84acddb1456"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 '1']\n"]}],"source":["# Load dataset\n","df = pd.read_csv('/content/global_bleaching_environmental-fixed.csv')\n","\n","# Replace 'nd' with -9999 and convert to numeric where applicable\n","df.replace('nd', -9999, inplace=True)\n","df=df.replace(to_replace=-9999,value=0)\n","df=df.replace(to_replace=\"-9999\",value=\"0\")\n","df=df.replace(to_replace=\"Population\",value=\"1\")\n","print(df[\"Bleaching_Level\"].unique())\n","\n","# Select all relevant columns except --- as features\n","X = df.drop(columns=['Bleaching_Level', 'Exposure','Percent_Bleaching','Percent_Cover'])\n","y = df['Bleaching_Level']\n","\n","# Encode target variable\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y.astype(str))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":39,"status":"ok","timestamp":1750833293591,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"Wa5Vkqp0jDQv","outputId":"4a473f0b-9164-4bb9-becc-8700b5cfb454"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 41228 entries, 0 to 41227\n","Data columns (total 41 columns):\n"," #   Column                                 Non-Null Count  Dtype  \n","---  ------                                 --------------  -----  \n"," 0   Distance_to_Shore                      41228 non-null  float64\n"," 1   Exposure                               41228 non-null  object \n"," 2   Turbidity                              41228 non-null  float64\n"," 3   Cyclone_Frequency                      41228 non-null  float64\n"," 4   Depth_m                                41228 non-null  float64\n"," 5   Percent_Cover                          41228 non-null  float64\n"," 6   Bleaching_Level                        41228 non-null  object \n"," 7   Percent_Bleaching                      41228 non-null  float64\n"," 8   ClimSST                                41228 non-null  float64\n"," 9   Temperature_Kelvin                     41228 non-null  float64\n"," 10  Temperature_Mean                       41228 non-null  float64\n"," 11  Temperature_Minimum                    41228 non-null  float64\n"," 12  Temperature_Maximum                    41228 non-null  float64\n"," 13  Temperature_Kelvin_Standard_Deviation  41228 non-null  float64\n"," 14  Windspeed                              41228 non-null  float64\n"," 15  SSTA                                   41228 non-null  float64\n"," 16  SSTA_Standard_Deviation                41228 non-null  float64\n"," 17  SSTA_Mean                              41228 non-null  int64  \n"," 18  SSTA_Minimum                           41228 non-null  float64\n"," 19  SSTA_Maximum                           41228 non-null  float64\n"," 20  SSTA_Frequency                         41228 non-null  float64\n"," 21  SSTA_Frequency_Standard_Deviation      41228 non-null  float64\n"," 22  SSTA_FrequencyMax                      41228 non-null  float64\n"," 23  SSTA_FrequencyMean                     41228 non-null  float64\n"," 24  SSTA_DHW                               41228 non-null  float64\n"," 25  SSTA_DHW_Standard_Deviation            41228 non-null  float64\n"," 26  SSTA_DHWMax                            41228 non-null  float64\n"," 27  SSTA_DHWMean                           41228 non-null  float64\n"," 28  TSA                                    41228 non-null  float64\n"," 29  TSA_Standard_Deviation                 41228 non-null  float64\n"," 30  TSA_Minimum                            41228 non-null  float64\n"," 31  TSA_Maximum                            41228 non-null  float64\n"," 32  TSA_Mean                               41228 non-null  float64\n"," 33  TSA_Frequency                          41228 non-null  float64\n"," 34  TSA_Frequency_Standard_Deviation       41228 non-null  float64\n"," 35  TSA_FrequencyMax                       41228 non-null  float64\n"," 36  TSA_FrequencyMean                      41228 non-null  float64\n"," 37  TSA_DHW                                41228 non-null  float64\n"," 38  TSA_DHW_Standard_Deviation             41228 non-null  float64\n"," 39  TSA_DHWMax                             41228 non-null  float64\n"," 40  TSA_DHWMean                            41228 non-null  float64\n","dtypes: float64(38), int64(1), object(2)\n","memory usage: 12.9+ MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fJNavivYAq84","executionInfo":{"status":"ok","timestamp":1753342502065,"user_tz":-480,"elapsed":58,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}}},"outputs":[],"source":["# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n","\n","# Scale features\n","scaler = MinMaxScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GxcnAA8xMU-E","executionInfo":{"status":"ok","timestamp":1753342504572,"user_tz":-480,"elapsed":8,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}}},"outputs":[],"source":["# --- Evaluation Metrics ---\n","def evaluate(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n","    return acc, mae, rmse, mape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2685718,"status":"ok","timestamp":1750841276281,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"j4FXB1FcJf2-","outputId":"1a9c2707-24ef-4339-c98b-7bdb09c3f0d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 24 candidates, totalling 72 fits\n","                                               Params  Accuracy       MAE  \\\n","0   {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  0.946641  0.053359   \n","1   {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  0.946156  0.053844   \n","2   {'max_depth': 10, 'max_features': 'log2', 'n_e...  0.946277  0.053723   \n","3   {'max_depth': 10, 'max_features': 'log2', 'n_e...  0.946520  0.053480   \n","4   {'max_depth': 10, 'max_features': None, 'n_est...  0.948945  0.051055   \n","5   {'max_depth': 10, 'max_features': None, 'n_est...  0.949551  0.050449   \n","6   {'max_depth': 20, 'max_features': 'sqrt', 'n_e...  0.987873  0.012127   \n","7   {'max_depth': 20, 'max_features': 'sqrt', 'n_e...  0.988601  0.011399   \n","8   {'max_depth': 20, 'max_features': 'log2', 'n_e...  0.988601  0.011399   \n","9   {'max_depth': 20, 'max_features': 'log2', 'n_e...  0.989449  0.010551   \n","10  {'max_depth': 20, 'max_features': None, 'n_est...  0.985811  0.014189   \n","11  {'max_depth': 20, 'max_features': None, 'n_est...  0.985690  0.014310   \n","12  {'max_depth': 30, 'max_features': 'sqrt', 'n_e...  0.991996  0.008004   \n","13  {'max_depth': 30, 'max_features': 'sqrt', 'n_e...  0.992117  0.007883   \n","14  {'max_depth': 30, 'max_features': 'log2', 'n_e...  0.991754  0.008246   \n","15  {'max_depth': 30, 'max_features': 'log2', 'n_e...  0.992239  0.007761   \n","16  {'max_depth': 30, 'max_features': None, 'n_est...  0.991632  0.008368   \n","17  {'max_depth': 30, 'max_features': None, 'n_est...  0.991632  0.008368   \n","18  {'max_depth': None, 'max_features': 'sqrt', 'n...  0.992117  0.007883   \n","19  {'max_depth': None, 'max_features': 'sqrt', 'n...  0.991996  0.008004   \n","20  {'max_depth': None, 'max_features': 'log2', 'n...  0.991632  0.008368   \n","21  {'max_depth': None, 'max_features': 'log2', 'n...  0.991875  0.008125   \n","22  {'max_depth': None, 'max_features': None, 'n_e...  0.991511  0.008489   \n","23  {'max_depth': None, 'max_features': None, 'n_e...  0.991632  0.008368   \n","\n","        RMSE      MAPE  \n","0   0.230996  5.335920  \n","1   0.232044  5.384429  \n","2   0.231782  5.372302  \n","3   0.231258  5.348048  \n","4   0.225954  5.105506  \n","5   0.224608  5.044870  \n","6   0.110123  1.212709  \n","7   0.106768  1.139947  \n","8   0.106768  1.139947  \n","9   0.102716  1.055057  \n","10  0.119116  1.418870  \n","11  0.119624  1.430997  \n","12  0.089464  0.800388  \n","13  0.088784  0.788261  \n","14  0.090810  0.824642  \n","15  0.088098  0.776134  \n","16  0.091475  0.836769  \n","17  0.091475  0.836769  \n","18  0.088784  0.788261  \n","19  0.089464  0.800388  \n","20  0.091475  0.836769  \n","21  0.090140  0.812515  \n","22  0.092136  0.848896  \n","23  0.091475  0.836769  \n"]}],"source":["# --- Random Forest ---\n","\n","# Experiment 2.1 - Default\n","#rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","#rf.fit(X_train_scaled, y_train)\n","#rf_pred = rf.predict(X_test_scaled)\n","\n","# Experiments X\n","grid_search = GridSearchCV(\n","    estimator=RandomForestClassifier(random_state=42),\n","    param_grid={\n","        'n_estimators': [100, 200],\n","        'max_depth': [10, 20, 30, None],\n","        'max_features': ['sqrt', 'log2', None]\n","    },\n","    scoring='accuracy',\n","    cv=3,\n","    verbose=1,\n","    n_jobs=-1\n",")\n","\n","grid_search.fit(X_train_scaled, y_train)\n","\n","# Evaluate Every Iteration Manually\n","all_metrics = []\n","\n","# Iterate through each parameter combination tested\n","for i in range(len(grid_search.cv_results_['params'])):\n","    params = grid_search.cv_results_['params'][i]\n","\n","    # Train model with that parameter set\n","    model = RandomForestClassifier(**params, random_state=42)\n","    model.fit(X_train_scaled, y_train)\n","\n","    # Predict\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # Evaluate\n","    acc, mae, rmse, mape = evaluate(y_test, y_pred)\n","\n","    # Store results\n","    all_metrics.append({\n","        'Params': params,\n","        'Accuracy': acc,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'MAPE': mape\n","    })\n","\n","# Show as Table\n","metrics_df = pd.DataFrame(all_metrics)\n","print(metrics_df)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmQlqfOQyopP","outputId":"11799e36-9ed3-4f9d-af88-f3ba31337e7a","executionInfo":{"status":"ok","timestamp":1753347751534,"user_tz":-480,"elapsed":5239840,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 24 candidates, totalling 72 fits\n","\n","ğŸ“‹ All SVM Experiment Results:\n","       C   Kernel  Gamma  Degree  Accuracy       MAE      RMSE       MAPE\n","0    0.1   linear  scale       3  0.760005  0.239995  0.489893  23.999515\n","1    0.1      rbf  scale       3  0.761703  0.238297  0.488157  23.829736\n","2    0.1     poly  scale       3  0.799539  0.200461  0.447729  20.046083\n","3    0.1  sigmoid  scale       3  0.444215  0.555785  0.745510  55.578462\n","4    0.1   linear    0.1       3  0.760005  0.239995  0.489893  23.999515\n","5    0.1      rbf    0.1       3  0.721926  0.278074  0.527327  27.807422\n","6    0.1     poly    0.1       3  0.686757  0.313243  0.559681  31.324278\n","7    0.1  sigmoid    0.1       3  0.670386  0.329614  0.574121  32.961436\n","8    1.0   linear  scale       3  0.790323  0.209677  0.457905  20.967742\n","9    1.0      rbf  scale       3  0.826098  0.173902  0.417016  17.390250\n","10   1.0     poly  scale       3  0.850352  0.149648  0.386844  14.964831\n","11   1.0  sigmoid  scale       3  0.437060  0.562940  0.750293  56.293961\n","12   1.0   linear    0.1       3  0.790323  0.209677  0.457905  20.967742\n","13   1.0      rbf    0.1       3  0.777589  0.222411  0.471605  22.241087\n","14   1.0     poly    0.1       3  0.766917  0.233083  0.482786  23.308271\n","15   1.0  sigmoid    0.1       3  0.703129  0.296871  0.544859  29.687121\n","16  10.0   linear  scale       3  0.793354  0.206646  0.454583  20.664565\n","17  10.0      rbf  scale       3  0.872059  0.127941  0.357688  12.794082\n","18  10.0     poly  scale       3  0.876910  0.123090  0.350842  12.308998\n","19  10.0  sigmoid  scale       3  0.436454  0.563546  0.750697  56.354596\n","20  10.0   linear    0.1       3  0.793354  0.206646  0.454583  20.664565\n","21  10.0      rbf    0.1       3  0.829251  0.170749  0.413218  17.074945\n","22  10.0     poly    0.1       3  0.817123  0.182877  0.427641  18.287655\n","23  10.0  sigmoid    0.1       3  0.620665  0.379335  0.615902  37.933544\n"]}],"source":["# --- SVM ---\n","\n","# Experiments V2 (24)\n","# Define parameter grid\n","param_grid = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n","    'gamma': ['scale', 0.1],        # applicable to rbf, poly, sigmoid\n","    'degree': [3]                   # only used for poly\n","}\n","\n","# Create GridSearchCV (to get all param combinations)\n","svm_grid = GridSearchCV(\n","    estimator=SVC(),\n","    param_grid=param_grid,\n","    scoring='accuracy',\n","    cv=3,\n","    verbose=1,\n","    n_jobs=-1\n",")\n","\n","svm_grid.fit(X_train_scaled, y_train)\n","\n","# Manual Evaluation for Each Combination\n","all_svm_metrics = []\n","\n","for i in range(len(svm_grid.cv_results_['params'])):\n","    params = svm_grid.cv_results_['params'][i]\n","\n","    # Create and fit model with current parameters\n","    model = SVC(**params)\n","    model.fit(X_train_scaled, y_train)\n","\n","    # Predict on test set\n","    y_pred = model.predict(X_test_scaled)\n","\n","    # Evaluate\n","    acc, mae, rmse, mape = evaluate(y_test, y_pred)\n","\n","    # Store results\n","    all_svm_metrics.append({\n","    'C': params['C'],\n","    'Kernel': params['kernel'],\n","    'Gamma': params['gamma'],\n","    'Degree': params.get('degree', '-'),  # Only used in poly\n","    'Accuracy': acc,\n","    'MAE': mae,\n","    'RMSE': rmse,\n","    'MAPE': mape\n","})\n","\n","# Create DataFrame to display\n","svm_metrics_df = pd.DataFrame(all_svm_metrics)\n","\n","# Show the full result table\n","print(\"\\nğŸ“‹ All SVM Experiment Results:\")\n","print(svm_metrics_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":218713,"status":"ok","timestamp":1750766911794,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"vHwrALxpM1Kc","outputId":"e16a8260-0bad-45b3-9bb2-ec985cbce9a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6165 - loss: 0.6525\n","Epoch 2/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7352 - loss: 0.5551\n","Epoch 3/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7553 - loss: 0.5340\n","Epoch 4/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7659 - loss: 0.5171\n","Epoch 5/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7740 - loss: 0.4999\n","Epoch 6/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.4893\n","Epoch 7/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7874 - loss: 0.4800\n","Epoch 8/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7865 - loss: 0.4765\n","Epoch 9/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7920 - loss: 0.4648\n","Epoch 10/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7914 - loss: 0.4655\n","Epoch 11/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7980 - loss: 0.4570\n","Epoch 12/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7992 - loss: 0.4529\n","Epoch 13/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8130 - loss: 0.4402\n","Epoch 14/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8066 - loss: 0.4438\n","Epoch 15/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8158 - loss: 0.4353\n","Epoch 16/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8144 - loss: 0.4314\n","Epoch 17/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8208 - loss: 0.4271\n","Epoch 18/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8243 - loss: 0.4184\n","Epoch 19/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8253 - loss: 0.4176\n","Epoch 20/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8275 - loss: 0.4129\n","Epoch 21/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8293 - loss: 0.4091\n","Epoch 22/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8335 - loss: 0.4053\n","Epoch 23/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8352 - loss: 0.3987\n","Epoch 24/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8398 - loss: 0.3924\n","Epoch 25/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8452 - loss: 0.3862\n","Epoch 26/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8431 - loss: 0.3837\n","Epoch 27/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8513 - loss: 0.3771\n","Epoch 28/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8540 - loss: 0.3696\n","Epoch 29/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8487 - loss: 0.3752\n","Epoch 30/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8528 - loss: 0.3692\n","Epoch 31/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8567 - loss: 0.3597\n","Epoch 32/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8566 - loss: 0.3607\n","Epoch 33/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8591 - loss: 0.3595\n","Epoch 34/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8607 - loss: 0.3549\n","Epoch 35/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.3578\n","Epoch 36/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8621 - loss: 0.3528\n","Epoch 37/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8662 - loss: 0.3456\n","Epoch 38/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8635 - loss: 0.3441\n","Epoch 39/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.3369\n","Epoch 40/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8673 - loss: 0.3402\n","Epoch 41/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8679 - loss: 0.3387\n","Epoch 42/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8715 - loss: 0.3302\n","Epoch 43/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8676 - loss: 0.3318\n","Epoch 44/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8745 - loss: 0.3248\n","Epoch 45/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8735 - loss: 0.3268\n","Epoch 46/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8730 - loss: 0.3253\n","Epoch 47/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8734 - loss: 0.3272\n","Epoch 48/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8745 - loss: 0.3217\n","Epoch 49/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8693 - loss: 0.3255\n","Epoch 50/50\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8802 - loss: 0.3135\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"]}],"source":["# --- LSTM ---\n","X_train_lstm = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n","X_test_lstm = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n","\n","lstm = Sequential()\n","lstm.add(LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n","lstm.add(Dense(1, activation='sigmoid'))\n","lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n","lstm_pred_prob = lstm.predict(X_test_lstm)\n","lstm_pred = [1 if x > 0.5 else 0 for x in lstm_pred_prob]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2806650,"status":"ok","timestamp":1753277653601,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"6L7NRCWoGhvj","outputId":"d0287fb2-1a13-420a-f361-b794a14e726c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training LSTM with config: {'units': 64, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.6086 - loss: 0.6565\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7241 - loss: 0.5665\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7550 - loss: 0.5356\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7622 - loss: 0.5194\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.5067\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7784 - loss: 0.4926\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7839 - loss: 0.4859\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7860 - loss: 0.4844\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7904 - loss: 0.4722\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7944 - loss: 0.4650\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8018 - loss: 0.4586\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.4524\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8118 - loss: 0.4427\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8091 - loss: 0.4423\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8111 - loss: 0.4391\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8186 - loss: 0.4242\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.4286\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8258 - loss: 0.4164\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8312 - loss: 0.4097\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8247 - loss: 0.4134\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.4030\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8358 - loss: 0.3974\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8371 - loss: 0.3940\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8404 - loss: 0.3892\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8431 - loss: 0.3900\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8443 - loss: 0.3876\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8456 - loss: 0.3781\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8420 - loss: 0.3855\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8517 - loss: 0.3709\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8516 - loss: 0.3755\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5949 - loss: 0.6647\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7163 - loss: 0.5762\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7441 - loss: 0.5468\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7553 - loss: 0.5304\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.5129\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.5105\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7774 - loss: 0.4991\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7792 - loss: 0.4939\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.4920\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7844 - loss: 0.4841\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7847 - loss: 0.4816\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7891 - loss: 0.4755\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7919 - loss: 0.4730\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7943 - loss: 0.4698\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7943 - loss: 0.4663\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7962 - loss: 0.4636\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7957 - loss: 0.4624\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8042 - loss: 0.4499\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8016 - loss: 0.4514\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8091 - loss: 0.4470\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8134 - loss: 0.4436\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8142 - loss: 0.4380\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8097 - loss: 0.4386\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8144 - loss: 0.4385\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8203 - loss: 0.4306\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8188 - loss: 0.4237\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8212 - loss: 0.4271\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8255 - loss: 0.4223\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8230 - loss: 0.4256\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8299 - loss: 0.4108\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5867 - loss: 0.6682\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7091 - loss: 0.5808\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7450 - loss: 0.5454\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7588 - loss: 0.5288\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.5139\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7746 - loss: 0.5043\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7808 - loss: 0.4986\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7745 - loss: 0.4943\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7846 - loss: 0.4829\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7869 - loss: 0.4802\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.4736\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7944 - loss: 0.4645\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7996 - loss: 0.4586\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8014 - loss: 0.4562\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7995 - loss: 0.4534\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8033 - loss: 0.4504\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8092 - loss: 0.4462\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8060 - loss: 0.4471\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8137 - loss: 0.4403\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8167 - loss: 0.4367\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8187 - loss: 0.4346\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8215 - loss: 0.4312\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8255 - loss: 0.4246\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8267 - loss: 0.4227\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8256 - loss: 0.4176\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8256 - loss: 0.4155\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8303 - loss: 0.4091\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8308 - loss: 0.4091\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8312 - loss: 0.4103\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8397 - loss: 0.3954\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5770 - loss: 0.6738\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6954 - loss: 0.6018\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7186 - loss: 0.5689\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7436 - loss: 0.5441\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7575 - loss: 0.5332\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.5207\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7652 - loss: 0.5170\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7771 - loss: 0.5028\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7741 - loss: 0.5014\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7779 - loss: 0.4971\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7785 - loss: 0.4964\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.4905\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.4815\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7783 - loss: 0.4913\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7877 - loss: 0.4749\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7827 - loss: 0.4799\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7881 - loss: 0.4740\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.4806\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7920 - loss: 0.4728\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7898 - loss: 0.4787\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7940 - loss: 0.4621\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7955 - loss: 0.4659\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7947 - loss: 0.4615\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7984 - loss: 0.4617\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7993 - loss: 0.4575\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7982 - loss: 0.4609\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8033 - loss: 0.4516\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7997 - loss: 0.4565\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8083 - loss: 0.4409\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8051 - loss: 0.4461\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6025 - loss: 0.6628\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7160 - loss: 0.5764\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7369 - loss: 0.5518\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7557 - loss: 0.5336\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7587 - loss: 0.5284\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.5152\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.5026\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.4919\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7792 - loss: 0.4891\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7865 - loss: 0.4806\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.4731\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7886 - loss: 0.4721\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7934 - loss: 0.4656\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8007 - loss: 0.4533\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8010 - loss: 0.4496\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8033 - loss: 0.4488\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8076 - loss: 0.4457\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8120 - loss: 0.4418\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8120 - loss: 0.4361\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8153 - loss: 0.4330\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8158 - loss: 0.4359\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8227 - loss: 0.4299\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8250 - loss: 0.4185\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8317 - loss: 0.4077\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8324 - loss: 0.4107\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8302 - loss: 0.4106\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8315 - loss: 0.4095\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8343 - loss: 0.4056\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8407 - loss: 0.3991\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8358 - loss: 0.3989\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5889 - loss: 0.6692\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7024 - loss: 0.5911\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7314 - loss: 0.5602\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7472 - loss: 0.5416\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7574 - loss: 0.5333\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7629 - loss: 0.5225\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7657 - loss: 0.5169\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.5039\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 0.4992\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7810 - loss: 0.4973\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7746 - loss: 0.4961\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7840 - loss: 0.4912\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.4843\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7867 - loss: 0.4842\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7862 - loss: 0.4751\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7900 - loss: 0.4746\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7910 - loss: 0.4710\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7962 - loss: 0.4640\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7940 - loss: 0.4643\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7982 - loss: 0.4595\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8018 - loss: 0.4549\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8056 - loss: 0.4498\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8049 - loss: 0.4526\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8056 - loss: 0.4488\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4447\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8101 - loss: 0.4482\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8150 - loss: 0.4390\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8150 - loss: 0.4312\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.4397\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8211 - loss: 0.4310\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5854 - loss: 0.6694\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7046 - loss: 0.5939\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7268 - loss: 0.5649\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.5480\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7567 - loss: 0.5295\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7632 - loss: 0.5223\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.5117\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7735 - loss: 0.5058\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7782 - loss: 0.4998\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7807 - loss: 0.4933\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.4896\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7858 - loss: 0.4809\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7919 - loss: 0.4764\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7917 - loss: 0.4706\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7908 - loss: 0.4689\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7891 - loss: 0.4694\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7950 - loss: 0.4631\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8023 - loss: 0.4564\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7991 - loss: 0.4547\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8026 - loss: 0.4560\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8052 - loss: 0.4450\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8121 - loss: 0.4400\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8116 - loss: 0.4457\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8081 - loss: 0.4453\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8133 - loss: 0.4370\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8114 - loss: 0.4341\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8174 - loss: 0.4285\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8159 - loss: 0.4351\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8198 - loss: 0.4267\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8221 - loss: 0.4236\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 64, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5686 - loss: 0.6788\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6899 - loss: 0.6135\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7112 - loss: 0.5787\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7372 - loss: 0.5550\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7427 - loss: 0.5463\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.5292\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7593 - loss: 0.5248\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7653 - loss: 0.5249\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7690 - loss: 0.5148\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.5129\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7731 - loss: 0.5059\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7747 - loss: 0.5023\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7763 - loss: 0.4947\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7776 - loss: 0.4984\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7827 - loss: 0.4882\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7863 - loss: 0.4871\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7835 - loss: 0.4843\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7904 - loss: 0.4802\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7840 - loss: 0.4812\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7801 - loss: 0.4853\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7868 - loss: 0.4764\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7915 - loss: 0.4679\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7903 - loss: 0.4692\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7884 - loss: 0.4703\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7923 - loss: 0.4711\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7911 - loss: 0.4684\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7933 - loss: 0.4659\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7945 - loss: 0.4648\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7989 - loss: 0.4581\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7978 - loss: 0.4609\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6277 - loss: 0.6454\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7337 - loss: 0.5520\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7614 - loss: 0.5275\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7701 - loss: 0.5088\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7797 - loss: 0.4937\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7828 - loss: 0.4821\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7870 - loss: 0.4807\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7894 - loss: 0.4739\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7982 - loss: 0.4624\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7988 - loss: 0.4564\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8050 - loss: 0.4477\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8148 - loss: 0.4427\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8121 - loss: 0.4386\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8174 - loss: 0.4320\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8204 - loss: 0.4245\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8212 - loss: 0.4217\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8257 - loss: 0.4173\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8300 - loss: 0.4097\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8340 - loss: 0.4058\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8364 - loss: 0.3985\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8379 - loss: 0.3975\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8411 - loss: 0.3950\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8393 - loss: 0.3939\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8430 - loss: 0.3889\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8456 - loss: 0.3828\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8445 - loss: 0.3777\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8468 - loss: 0.3801\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8520 - loss: 0.3755\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8539 - loss: 0.3670\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8526 - loss: 0.3656\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5994 - loss: 0.6613\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7132 - loss: 0.5705\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7441 - loss: 0.5412\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7594 - loss: 0.5242\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.5183\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7741 - loss: 0.5045\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7710 - loss: 0.4972\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7824 - loss: 0.4852\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7756 - loss: 0.4928\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7808 - loss: 0.4803\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7873 - loss: 0.4776\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7809 - loss: 0.4849\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7890 - loss: 0.4703\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7910 - loss: 0.4731\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7960 - loss: 0.4635\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7969 - loss: 0.4627\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7976 - loss: 0.4600\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8038 - loss: 0.4523\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8052 - loss: 0.4485\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8078 - loss: 0.4413\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8068 - loss: 0.4437\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8092 - loss: 0.4410\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8135 - loss: 0.4386\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8174 - loss: 0.4365\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8180 - loss: 0.4286\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8187 - loss: 0.4301\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8231 - loss: 0.4181\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8267 - loss: 0.4235\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8274 - loss: 0.4113\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8261 - loss: 0.4168\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6034 - loss: 0.6614\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7189 - loss: 0.5723\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7452 - loss: 0.5398\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7618 - loss: 0.5271\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7720 - loss: 0.5083\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7811 - loss: 0.4950\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7862 - loss: 0.4863\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7886 - loss: 0.4795\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7879 - loss: 0.4791\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7845 - loss: 0.4744\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7891 - loss: 0.4706\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7949 - loss: 0.4643\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7960 - loss: 0.4606\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8015 - loss: 0.4567\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8071 - loss: 0.4458\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8128 - loss: 0.4432\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8106 - loss: 0.4412\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8128 - loss: 0.4363\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8094 - loss: 0.4390\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8209 - loss: 0.4280\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8213 - loss: 0.4255\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8201 - loss: 0.4238\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8231 - loss: 0.4223\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8261 - loss: 0.4179\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8323 - loss: 0.4115\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8303 - loss: 0.4125\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8307 - loss: 0.4085\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8369 - loss: 0.4028\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8385 - loss: 0.3941\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8385 - loss: 0.3965\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5845 - loss: 0.6720\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7021 - loss: 0.5908\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7353 - loss: 0.5539\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7511 - loss: 0.5393\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7593 - loss: 0.5326\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7627 - loss: 0.5180\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7750 - loss: 0.5077\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7719 - loss: 0.5014\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7772 - loss: 0.4983\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7815 - loss: 0.4909\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7825 - loss: 0.4903\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7786 - loss: 0.4884\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7767 - loss: 0.4920\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7796 - loss: 0.4887\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7831 - loss: 0.4799\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7830 - loss: 0.4898\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7857 - loss: 0.4751\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7885 - loss: 0.4743\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7897 - loss: 0.4719\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7924 - loss: 0.4639\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7889 - loss: 0.4663\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7912 - loss: 0.4664\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7950 - loss: 0.4611\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7984 - loss: 0.4599\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7975 - loss: 0.4569\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7953 - loss: 0.4609\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8034 - loss: 0.4505\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8019 - loss: 0.4512\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8004 - loss: 0.4584\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8089 - loss: 0.4462\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6051 - loss: 0.6608\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.5656\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7532 - loss: 0.5353\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7577 - loss: 0.5249\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7715 - loss: 0.5091\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7726 - loss: 0.5047\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7800 - loss: 0.4903\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7817 - loss: 0.4888\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7846 - loss: 0.4797\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7855 - loss: 0.4802\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7920 - loss: 0.4639\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7970 - loss: 0.4598\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7977 - loss: 0.4610\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7995 - loss: 0.4574\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8055 - loss: 0.4549\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8102 - loss: 0.4408\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8093 - loss: 0.4389\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8136 - loss: 0.4388\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8205 - loss: 0.4255\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8147 - loss: 0.4333\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8209 - loss: 0.4215\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8209 - loss: 0.4196\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8262 - loss: 0.4157\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8235 - loss: 0.4186\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8295 - loss: 0.4089\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8340 - loss: 0.4065\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8345 - loss: 0.4001\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8380 - loss: 0.3948\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8406 - loss: 0.3926\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8407 - loss: 0.3904\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5957 - loss: 0.6659\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7085 - loss: 0.5865\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7365 - loss: 0.5526\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7544 - loss: 0.5306\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.5281\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7638 - loss: 0.5179\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7685 - loss: 0.5095\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7748 - loss: 0.5014\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7803 - loss: 0.4958\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7774 - loss: 0.4922\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7754 - loss: 0.4952\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7812 - loss: 0.4807\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.4798\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7857 - loss: 0.4733\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7855 - loss: 0.4706\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7899 - loss: 0.4704\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7906 - loss: 0.4695\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7942 - loss: 0.4669\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7925 - loss: 0.4598\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7980 - loss: 0.4617\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8007 - loss: 0.4533\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8029 - loss: 0.4498\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8032 - loss: 0.4524\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8135 - loss: 0.4419\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8114 - loss: 0.4455\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8149 - loss: 0.4395\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8107 - loss: 0.4441\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8161 - loss: 0.4346\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8159 - loss: 0.4358\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8177 - loss: 0.4270\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.5910 - loss: 0.6680\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7086 - loss: 0.5830\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7367 - loss: 0.5525\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7524 - loss: 0.5395\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7627 - loss: 0.5276\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7688 - loss: 0.5123\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7717 - loss: 0.5079\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7757 - loss: 0.4962\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7760 - loss: 0.5033\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7867 - loss: 0.4791\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7872 - loss: 0.4834\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7875 - loss: 0.4760\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7920 - loss: 0.4702\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7935 - loss: 0.4661\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7882 - loss: 0.4714\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7989 - loss: 0.4569\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7965 - loss: 0.4582\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8015 - loss: 0.4564\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8022 - loss: 0.4541\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8048 - loss: 0.4492\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8108 - loss: 0.4444\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8120 - loss: 0.4396\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8135 - loss: 0.4391\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8123 - loss: 0.4370\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8180 - loss: 0.4290\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8197 - loss: 0.4302\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8213 - loss: 0.4258\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8224 - loss: 0.4220\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8208 - loss: 0.4275\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8289 - loss: 0.4144\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 128, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5758 - loss: 0.6753\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6927 - loss: 0.6060\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7234 - loss: 0.5711\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7437 - loss: 0.5472\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7521 - loss: 0.5413\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7592 - loss: 0.5279\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7606 - loss: 0.5256\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7701 - loss: 0.5120\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7716 - loss: 0.5061\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7750 - loss: 0.5053\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7766 - loss: 0.5030\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7773 - loss: 0.4951\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7726 - loss: 0.4956\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7778 - loss: 0.4910\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7787 - loss: 0.4934\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7829 - loss: 0.4861\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7807 - loss: 0.4856\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7816 - loss: 0.4789\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7813 - loss: 0.4824\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7852 - loss: 0.4763\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7855 - loss: 0.4804\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7840 - loss: 0.4730\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7879 - loss: 0.4763\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7905 - loss: 0.4701\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7917 - loss: 0.4623\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7903 - loss: 0.4667\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7975 - loss: 0.4620\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7952 - loss: 0.4629\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7986 - loss: 0.4555\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7984 - loss: 0.4610\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5956 - loss: 0.6630\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7118 - loss: 0.5753\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7438 - loss: 0.5422\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7583 - loss: 0.5280\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.5125\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.5054\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7787 - loss: 0.4911\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.4869\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7898 - loss: 0.4803\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7933 - loss: 0.4685\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7961 - loss: 0.4657\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7971 - loss: 0.4601\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7980 - loss: 0.4572\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8053 - loss: 0.4451\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8078 - loss: 0.4446\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8081 - loss: 0.4437\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8179 - loss: 0.4301\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8194 - loss: 0.4283\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8231 - loss: 0.4237\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8280 - loss: 0.4199\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.4150\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8309 - loss: 0.4052\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.4085\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8330 - loss: 0.4059\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8422 - loss: 0.3914\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8436 - loss: 0.3919\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8399 - loss: 0.3901\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8432 - loss: 0.3883\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8441 - loss: 0.3833\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8475 - loss: 0.3814\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.2, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5815 - loss: 0.6701\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7048 - loss: 0.5861\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7349 - loss: 0.5519\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7533 - loss: 0.5362\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7644 - loss: 0.5240\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7700 - loss: 0.5139\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.5087\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7732 - loss: 0.5047\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7758 - loss: 0.5033\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7819 - loss: 0.4925\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7849 - loss: 0.4831\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7826 - loss: 0.4874\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7890 - loss: 0.4747\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7883 - loss: 0.4768\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7933 - loss: 0.4662\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7950 - loss: 0.4687\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7970 - loss: 0.4654\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7985 - loss: 0.4629\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8025 - loss: 0.4529\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8064 - loss: 0.4449\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8057 - loss: 0.4503\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8070 - loss: 0.4413\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8106 - loss: 0.4482\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8157 - loss: 0.4352\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8174 - loss: 0.4290\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8166 - loss: 0.4329\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8195 - loss: 0.4314\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8192 - loss: 0.4302\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8263 - loss: 0.4229\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8203 - loss: 0.4225\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5715 - loss: 0.6752\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6984 - loss: 0.5946\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7288 - loss: 0.5608\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7458 - loss: 0.5449\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.5264\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.5218\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.5071\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7785 - loss: 0.4984\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7808 - loss: 0.4938\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.4900\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.4741\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7890 - loss: 0.4790\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7935 - loss: 0.4733\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7894 - loss: 0.4743\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7938 - loss: 0.4677\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7934 - loss: 0.4617\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8017 - loss: 0.4510\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8046 - loss: 0.4550\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8078 - loss: 0.4459\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8045 - loss: 0.4514\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4485\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8122 - loss: 0.4406\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8155 - loss: 0.4350\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8179 - loss: 0.4319\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8207 - loss: 0.4252\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8237 - loss: 0.4235\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8220 - loss: 0.4206\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8247 - loss: 0.4202\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8263 - loss: 0.4218\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8293 - loss: 0.4130\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.2, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5585 - loss: 0.6812\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6818 - loss: 0.6206\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7105 - loss: 0.5778\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7339 - loss: 0.5551\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7383 - loss: 0.5514\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7554 - loss: 0.5310\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7654 - loss: 0.5240\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7637 - loss: 0.5243\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7725 - loss: 0.5095\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7756 - loss: 0.5048\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7761 - loss: 0.4987\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7756 - loss: 0.5006\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7852 - loss: 0.4893\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7805 - loss: 0.4894\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7835 - loss: 0.4873\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7895 - loss: 0.4804\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7880 - loss: 0.4817\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7857 - loss: 0.4786\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7879 - loss: 0.4791\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7865 - loss: 0.4773\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7887 - loss: 0.4728\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7936 - loss: 0.4682\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7913 - loss: 0.4689\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7944 - loss: 0.4629\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7900 - loss: 0.4675\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7956 - loss: 0.4645\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7990 - loss: 0.4596\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8018 - loss: 0.4536\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8005 - loss: 0.4587\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7999 - loss: 0.4557\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5809 - loss: 0.6722\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.5921\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7318 - loss: 0.5602\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7481 - loss: 0.5415\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.5288\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7630 - loss: 0.5209\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7709 - loss: 0.5125\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.4945\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7749 - loss: 0.5040\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7753 - loss: 0.4958\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7806 - loss: 0.4862\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7837 - loss: 0.4780\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7863 - loss: 0.4783\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7896 - loss: 0.4727\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7939 - loss: 0.4700\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7981 - loss: 0.4588\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7934 - loss: 0.4578\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8013 - loss: 0.4576\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7996 - loss: 0.4534\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8040 - loss: 0.4496\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8034 - loss: 0.4490\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8087 - loss: 0.4440\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.4475\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8109 - loss: 0.4383\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8161 - loss: 0.4331\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8140 - loss: 0.4338\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8182 - loss: 0.4263\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8209 - loss: 0.4262\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8254 - loss: 0.4214\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8238 - loss: 0.4260\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.5, 'batch_size': 32, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5739 - loss: 0.6737\n","Epoch 2/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6962 - loss: 0.6010\n","Epoch 3/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7240 - loss: 0.5693\n","Epoch 4/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7377 - loss: 0.5578\n","Epoch 5/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.5390\n","Epoch 6/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.5298\n","Epoch 7/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7646 - loss: 0.5170\n","Epoch 8/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7669 - loss: 0.5191\n","Epoch 9/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.5078\n","Epoch 10/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7717 - loss: 0.5060\n","Epoch 11/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7751 - loss: 0.5016\n","Epoch 12/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7780 - loss: 0.4957\n","Epoch 13/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7810 - loss: 0.4935\n","Epoch 14/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7826 - loss: 0.4876\n","Epoch 15/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7855 - loss: 0.4831\n","Epoch 16/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7862 - loss: 0.4792\n","Epoch 17/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7884 - loss: 0.4774\n","Epoch 18/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7879 - loss: 0.4779\n","Epoch 19/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7960 - loss: 0.4637\n","Epoch 20/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7945 - loss: 0.4670\n","Epoch 21/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7926 - loss: 0.4677\n","Epoch 22/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7999 - loss: 0.4626\n","Epoch 23/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8016 - loss: 0.4617\n","Epoch 24/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7997 - loss: 0.4600\n","Epoch 25/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8003 - loss: 0.4615\n","Epoch 26/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8060 - loss: 0.4561\n","Epoch 27/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8081 - loss: 0.4454\n","Epoch 28/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8073 - loss: 0.4478\n","Epoch 29/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8073 - loss: 0.4482\n","Epoch 30/30\n","\u001b[1m1031/1031\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8093 - loss: 0.4444\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'adam'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5699 - loss: 0.6780\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6888 - loss: 0.6104\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7215 - loss: 0.5722\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7414 - loss: 0.5528\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7478 - loss: 0.5438\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.5258\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7638 - loss: 0.5291\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.5158\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7775 - loss: 0.5068\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7761 - loss: 0.5015\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7789 - loss: 0.4969\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7819 - loss: 0.4945\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7821 - loss: 0.4894\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7844 - loss: 0.4866\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7854 - loss: 0.4817\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7852 - loss: 0.4836\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7894 - loss: 0.4751\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7849 - loss: 0.4766\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8003 - loss: 0.4615\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7955 - loss: 0.4668\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7944 - loss: 0.4642\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7946 - loss: 0.4610\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7925 - loss: 0.4635\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8085 - loss: 0.4508\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8072 - loss: 0.4500\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7984 - loss: 0.4545\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8054 - loss: 0.4482\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8094 - loss: 0.4441\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8061 - loss: 0.4479\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4401\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","Training LSTM with config: {'units': 32, 'dropout': 0.5, 'batch_size': 64, 'optimizer': 'rmsprop'}\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5575 - loss: 0.6825\n","Epoch 2/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.6319\n","Epoch 3/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6970 - loss: 0.5975\n","Epoch 4/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7136 - loss: 0.5752\n","Epoch 5/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7285 - loss: 0.5631\n","Epoch 6/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7391 - loss: 0.5515\n","Epoch 7/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7516 - loss: 0.5415\n","Epoch 8/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7579 - loss: 0.5331\n","Epoch 9/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7599 - loss: 0.5318\n","Epoch 10/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7704 - loss: 0.5167\n","Epoch 11/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.5181\n","Epoch 12/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7712 - loss: 0.5167\n","Epoch 13/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7731 - loss: 0.5073\n","Epoch 14/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7805 - loss: 0.5015\n","Epoch 15/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7777 - loss: 0.4974\n","Epoch 16/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7763 - loss: 0.4989\n","Epoch 17/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7778 - loss: 0.4972\n","Epoch 18/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7806 - loss: 0.4876\n","Epoch 19/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7804 - loss: 0.4895\n","Epoch 20/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7858 - loss: 0.4826\n","Epoch 21/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7832 - loss: 0.4799\n","Epoch 22/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7886 - loss: 0.4826\n","Epoch 23/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7827 - loss: 0.4803\n","Epoch 24/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7851 - loss: 0.4812\n","Epoch 25/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7854 - loss: 0.4792\n","Epoch 26/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7898 - loss: 0.4752\n","Epoch 27/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7910 - loss: 0.4792\n","Epoch 28/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7917 - loss: 0.4672\n","Epoch 29/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7956 - loss: 0.4647\n","Epoch 30/30\n","\u001b[1m516/516\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7913 - loss: 0.4691\n","\u001b[1m258/258\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n","\n","ğŸ“Š LSTM Tuning Results:\n","    units  dropout  batch_size optimizer  Accuracy       MAE      RMSE  \\\n","8     128      0.2          32      adam  0.862964  0.137036  0.370184   \n","0      64      0.2          32      adam  0.852292  0.147708  0.384328   \n","16     32      0.2          32      adam  0.849624  0.150376  0.387783   \n","4      64      0.5          32      adam  0.847320  0.152680  0.390743   \n","12    128      0.5          32      adam  0.844652  0.155348  0.394142   \n","20     32      0.5          32      adam  0.842954  0.157046  0.396290   \n","14    128      0.5          64      adam  0.841499  0.158501  0.398122   \n","6      64      0.5          64      adam  0.841378  0.158622  0.398274   \n","1      64      0.2          32   rmsprop  0.841378  0.158622  0.398274   \n","18     32      0.2          64      adam  0.839073  0.160927  0.401156   \n","10    128      0.2          64      adam  0.837861  0.162139  0.402665   \n","17     32      0.2          32   rmsprop  0.835314  0.164686  0.405815   \n","2      64      0.2          64      adam  0.832889  0.167111  0.408793   \n","22     32      0.5          64      adam  0.825734  0.174266  0.417452   \n","5      64      0.5          32   rmsprop  0.823915  0.176085  0.419625   \n","21     32      0.5          32   rmsprop  0.821489  0.178511  0.422505   \n","11    128      0.2          64   rmsprop  0.820155  0.179845  0.424081   \n","9     128      0.2          32   rmsprop  0.819306  0.180694  0.425081   \n","19     32      0.2          64   rmsprop  0.818579  0.181421  0.425936   \n","15    128      0.5          64   rmsprop  0.817245  0.182755  0.427499   \n","13    128      0.5          32   rmsprop  0.815789  0.184211  0.429198   \n","3      64      0.2          64   rmsprop  0.810332  0.189668  0.435509   \n","23     32      0.5          64   rmsprop  0.808513  0.191487  0.437592   \n","7      64      0.5          64   rmsprop  0.807179  0.192821  0.439114   \n","\n","         MAPE  \n","8   49.347175  \n","0   49.696305  \n","16  49.755290  \n","4   49.677972  \n","12  49.786377  \n","20  49.357538  \n","14  49.336813  \n","6   49.375871  \n","1   49.466740  \n","18  49.319277  \n","10  49.902754  \n","17  49.734566  \n","2   49.246741  \n","22  49.655653  \n","5   49.018770  \n","21  49.744131  \n","11  49.508987  \n","9   49.017176  \n","19  49.176596  \n","15  49.430871  \n","13  48.933481  \n","3   49.952971  \n","23  49.314494  \n","7   49.791957  \n"]}],"source":["# --- LSTM Experiments ---\n","#24 experiments\n","from tensorflow.keras.callbacks import EarlyStopping\n","from itertools import product\n","\n","# Reshape input for LSTM\n","X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n","X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n","\n","# Hyperparameters to test\n","units = [64, 128, 32]\n","dropouts = [0.2, 0.5]\n","batch_sizes = [32, 64]\n","optimizers = ['adam', 'rmsprop']\n","\n","# Generate all configurations\n","lstm_configs = [\n","    {'units': u, 'dropout': d, 'batch_size': b, 'optimizer': o}\n","    for u, d, b, o in product(units, dropouts, batch_sizes, optimizers)\n","]\n","\n","# Store results\n","lstm_results = []\n","\n","for cfg in lstm_configs:\n","    print(f\"\\nTraining LSTM with config: {cfg}\")\n","\n","    # Build model\n","    model = Sequential()\n","    model.add(LSTM(units=cfg['units'], input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n","    model.add(Dropout(cfg['dropout']))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=cfg['optimizer'], metrics=['accuracy'])\n","\n","    # Train\n","    history = model.fit(\n","        X_train_lstm, y_train,\n","        epochs=30,\n","        batch_size=cfg['batch_size'],\n","        verbose=1\n","    )\n","\n","    # Predict\n","    y_pred_prob = model.predict(X_test_lstm)\n","    y_pred = (y_pred_prob > 0.5).astype('int32')\n","\n","    # Evaluate\n","    acc, mae, rmse, mape = evaluate(y_test, y_pred)\n","\n","    lstm_results.append({\n","        'units': cfg['units'],\n","        'dropout': cfg['dropout'],\n","        'batch_size': cfg['batch_size'],\n","        'optimizer': cfg['optimizer'],\n","        'Accuracy': acc,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'MAPE': mape\n","    })\n","\n","# Save results\n","lstm_results_df = pd.DataFrame(lstm_results)\n","print(\"\\nğŸ“Š LSTM Tuning Results:\")\n","print(lstm_results_df.sort_values(by='Accuracy', ascending=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzi9YkOzkQsE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753263260164,"user_tz":-480,"elapsed":89050,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}},"outputId":"79a9edb1-a67f-4df1-a0ed-f417e5372423"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:51] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:54] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:54] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:55] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:57] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:32:59] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:00] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:02] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:07] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:12] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:15] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:22] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:30] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:39] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:39] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:42] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:44] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:46] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:47] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:49] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:54] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 10, 'n_estimators': 100}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:33:59] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 10, 'n_estimators': 200}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:34:01] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 10, 'n_estimators': 300}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:34:07] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Training with: {'learning_rate': 0.3, 'max_depth': 10, 'n_estimators': 400}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:34:12] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ“Š XGBoost Tuning Results:\n","    n_estimators  max_depth  learning_rate  Accuracy       MAE      RMSE  \\\n","23           400         10            0.3  0.992845  0.007155  0.084587   \n","11           400         10            0.1  0.992602  0.007398  0.086009   \n","21           200         10            0.3  0.992481  0.007519  0.086711   \n","22           300         10            0.3  0.992239  0.007761  0.088098   \n","20           100         10            0.3  0.992239  0.007761  0.088098   \n","10           300         10            0.1  0.992117  0.007883  0.088784   \n","19           400          6            0.3  0.991511  0.008489  0.092136   \n","18           300          6            0.3  0.991268  0.008732  0.093443   \n","9            200         10            0.1  0.991147  0.008853  0.094089   \n","17           200          6            0.3  0.988964  0.011036  0.105051   \n","7            400          6            0.1  0.983992  0.016008  0.126522   \n","8            100         10            0.1  0.983628  0.016372  0.127951   \n","16           100          6            0.3  0.979748  0.020252  0.142310   \n","6            300          6            0.1  0.977807  0.022193  0.148972   \n","5            200          6            0.1  0.969682  0.030318  0.174120   \n","15           400          3            0.3  0.968712  0.031288  0.176884   \n","14           300          3            0.3  0.963255  0.036745  0.191690   \n","4            100          6            0.1  0.957555  0.042445  0.206021   \n","13           200          3            0.3  0.955372  0.044628  0.211253   \n","3            400          3            0.1  0.948339  0.051661  0.227291   \n","12           100          3            0.3  0.943124  0.056876  0.238487   \n","2            300          3            0.1  0.942881  0.057119  0.238995   \n","1            200          3            0.1  0.933544  0.066456  0.257792   \n","0            100          3            0.1  0.917293  0.082707  0.287588   \n","\n","        MAPE  \n","23  0.715498  \n","11  0.739753  \n","21  0.751880  \n","22  0.776134  \n","20  0.776134  \n","10  0.788261  \n","19  0.848896  \n","18  0.873151  \n","9   0.885278  \n","17  1.103565  \n","7   1.600776  \n","8   1.637157  \n","16  2.025224  \n","6   2.219258  \n","5   3.031773  \n","15  3.128790  \n","14  3.674509  \n","4   4.244482  \n","13  4.462770  \n","3   5.166141  \n","12  5.687606  \n","2   5.711860  \n","1   6.645646  \n","0   8.270677  \n"]}],"source":["# --- XGBoost ---\n","xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n","xgb.fit(X_train_scaled, y_train)\n","xgb_pred = xgb.predict(X_test_scaled)\n","\n","# XGBoost Hyperparameter grid (24 experiments)\n","param_grid = {\n","    'n_estimators': [100, 200, 300, 400],       # model complexity & performance / default 100\n","    'max_depth': [3, 6, 10],               # shallow, medium, deep trees / default 6\n","    'learning_rate': [0.1, 0.3]      # slow to fast learning / default 0.3\n","}\n","\n","# Run Experiments\n","xgb_results = []\n","grid = list(ParameterGrid(param_grid))\n","\n","for params in grid:\n","    print(f\"\\nğŸ”§ Training with: {params}\")\n","    xgb = XGBClassifier(\n","        n_estimators=params['n_estimators'],\n","        max_depth=params['max_depth'],\n","        learning_rate=params['learning_rate'],\n","        use_label_encoder=False,\n","        eval_metric='logloss',\n","        random_state=42\n","    )\n","\n","    xgb.fit(X_train_scaled, y_train)\n","    xgb_pred = xgb.predict(X_test_scaled)\n","\n","    acc, mae, rmse, mape = evaluate(y_test, xgb_pred)\n","\n","    xgb_results.append({\n","        'n_estimators': params['n_estimators'],\n","        'max_depth': params['max_depth'],\n","        'learning_rate': params['learning_rate'],\n","        'Accuracy': acc,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'MAPE': mape\n","    })\n","\n","# XGBoost Results\n","xgb_results_df = pd.DataFrame(xgb_results)\n","print(\"\\nğŸ“Š XGBoost Tuning Results:\")\n","print(xgb_results_df.sort_values(by='Accuracy', ascending=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gt7uBQDqk9sO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753263278205,"user_tz":-480,"elapsed":18037,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}},"outputId":"862e3f29-27e5-4658-8fac-69772f325ee4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸŒ³ Training DT 1/24 with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 2/24 with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 3/24 with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 4/24 with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 5/24 with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 6/24 with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 7/24 with: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 8/24 with: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 9/24 with: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 10/24 with: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 11/24 with: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 12/24 with: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 13/24 with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 14/24 with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 15/24 with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 16/24 with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 17/24 with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 18/24 with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 19/24 with: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 20/24 with: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 21/24 with: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10}\n","\n","ğŸŒ³ Training DT 22/24 with: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 2}\n","\n","ğŸŒ³ Training DT 23/24 with: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5}\n","\n","ğŸŒ³ Training DT 24/24 with: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10}\n","\n","ğŸ“Š Decision Tree Results:\n","   criterion  max_depth  min_samples_split  Accuracy       MAE      RMSE  \\\n","0       gini        NaN                  2  0.986660  0.013340  0.115498   \n","9       gini       30.0                  2  0.986660  0.013340  0.115498   \n","12   entropy        NaN                  2  0.986418  0.013582  0.116543   \n","21   entropy       30.0                  2  0.986418  0.013582  0.116543   \n","18   entropy       20.0                  2  0.983507  0.016493  0.128424   \n","13   entropy        NaN                  5  0.981567  0.018433  0.135769   \n","22   entropy       30.0                  5  0.981567  0.018433  0.135769   \n","1       gini        NaN                  5  0.979384  0.020616  0.143583   \n","19   entropy       20.0                  5  0.979384  0.020616  0.143583   \n","10      gini       30.0                  5  0.979384  0.020616  0.143583   \n","6       gini       20.0                  2  0.977565  0.022435  0.149784   \n","23   entropy       30.0                 10  0.974048  0.025952  0.161096   \n","14   entropy        NaN                 10  0.974048  0.025952  0.161096   \n","7       gini       20.0                  5  0.973684  0.026316  0.162221   \n","20   entropy       20.0                 10  0.973684  0.026316  0.162221   \n","2       gini        NaN                 10  0.971259  0.028741  0.169532   \n","11      gini       30.0                 10  0.971259  0.028741  0.169532   \n","8       gini       20.0                 10  0.966529  0.033471  0.182950   \n","3       gini       10.0                  2  0.930148  0.069852  0.264295   \n","4       gini       10.0                  5  0.929784  0.070216  0.264983   \n","5       gini       10.0                 10  0.928693  0.071307  0.267034   \n","15   entropy       10.0                  2  0.923842  0.076158  0.275968   \n","16   entropy       10.0                  5  0.923478  0.076522  0.276626   \n","17   entropy       10.0                 10  0.923114  0.076886  0.277283   \n","\n","        MAPE  \n","0   1.333980  \n","9   1.333980  \n","12  1.358234  \n","21  1.358234  \n","18  1.649285  \n","13  1.843318  \n","22  1.843318  \n","1   2.061606  \n","19  2.061606  \n","10  2.061606  \n","6   2.243512  \n","23  2.595198  \n","14  2.595198  \n","7   2.631579  \n","20  2.631579  \n","2   2.874121  \n","11  2.874121  \n","8   3.347077  \n","3   6.985205  \n","4   7.021586  \n","5   7.130730  \n","15  7.615814  \n","16  7.652195  \n","17  7.688576  \n"]}],"source":["# --- Decision Tree ---\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train_scaled, y_train)\n","dt_pred = dt.predict(X_test_scaled)\n","\n","# DT Hyperparameter Grid (24 Experiments)\n","dt_param_grid = {\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'criterion': ['gini', 'entropy']\n","}\n","\n","grid = list(ParameterGrid(dt_param_grid))\n","dt_results = []\n","\n","for i, params in enumerate(grid):\n","    print(f\"\\nğŸŒ³ Training DT {i+1}/{len(grid)} with: {params}\")\n","\n","    dt = DecisionTreeClassifier(**params, random_state=42)\n","    dt.fit(X_train_scaled, y_train)\n","    dt_pred = dt.predict(X_test_scaled)\n","\n","    acc, mae, rmse, mape = evaluate(y_test, dt_pred)\n","\n","    dt_results.append({\n","        **params,\n","        'Accuracy': acc,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'MAPE': mape\n","    })\n","\n","# DT Results\n","dt_results_df = pd.DataFrame(dt_results)\n","print(\"\\nğŸ“Š Decision Tree Results:\")\n","print(dt_results_df.sort_values(by='Accuracy', ascending=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7vq7HJ5lKt-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753266342224,"user_tz":-480,"elapsed":3064015,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"}},"outputId":"31cdc306-d067-4c7b-9d22-0b2121dbf62f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 1/24 with: {'activation': 'relu', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 2/24 with: {'activation': 'relu', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 3/24 with: {'activation': 'relu', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.001}\n","\n","ğŸ§  Training MLP 4/24 with: {'activation': 'relu', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 5/24 with: {'activation': 'relu', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.001}\n","\n","ğŸ§  Training MLP 6/24 with: {'activation': 'relu', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 7/24 with: {'activation': 'relu', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 8/24 with: {'activation': 'relu', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 9/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 10/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 11/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.001}\n","\n","ğŸ§  Training MLP 12/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 13/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 14/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 15/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 16/24 with: {'activation': 'tanh', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 17/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 18/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 19/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 20/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (64, 32), 'learning_rate_init': 0.01}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 21/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 22/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.01}\n","\n","ğŸ§  Training MLP 23/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.001}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Training MLP 24/24 with: {'activation': 'logistic', 'hidden_layer_sizes': (128,), 'learning_rate_init': 0.01}\n","\n","ğŸ“Š MLP Experiment Results:\n","   activation hidden_layer_sizes  learning_rate_init  Accuracy       MAE  \\\n","4        relu      (128, 64, 32)               0.001  0.964468  0.035532   \n","12       tanh      (128, 64, 32)               0.001  0.963983  0.036017   \n","21   logistic      (128, 64, 32)               0.010  0.951734  0.048266   \n","19   logistic           (64, 32)               0.010  0.945307  0.054693   \n","5        relu      (128, 64, 32)               0.010  0.934271  0.065729   \n","10       tanh           (64, 32)               0.001  0.929784  0.070216   \n","2        relu           (64, 32)               0.001  0.927359  0.072641   \n","23   logistic             (128,)               0.010  0.926389  0.073611   \n","6        relu             (128,)               0.001  0.926025  0.073975   \n","11       tanh           (64, 32)               0.010  0.925782  0.074218   \n","15       tanh             (128,)               0.010  0.923721  0.076279   \n","1        relu              (64,)               0.010  0.923357  0.076643   \n","9        tanh              (64,)               0.010  0.922387  0.077613   \n","17   logistic              (64,)               0.010  0.921780  0.078220   \n","0        relu              (64,)               0.001  0.918142  0.081858   \n","13       tanh      (128, 64, 32)               0.010  0.918021  0.081979   \n","3        relu           (64, 32)               0.010  0.914261  0.085739   \n","7        relu             (128,)               0.010  0.904681  0.095319   \n","14       tanh             (128,)               0.001  0.882731  0.117269   \n","8        tanh              (64,)               0.001  0.880427  0.119573   \n","20   logistic      (128, 64, 32)               0.001  0.862843  0.137157   \n","18   logistic           (64, 32)               0.001  0.859690  0.140310   \n","16   logistic              (64,)               0.001  0.853626  0.146374   \n","22   logistic             (128,)               0.001  0.850230  0.149770   \n","\n","        RMSE       MAPE  \n","4   0.188500   3.553238  \n","12  0.189783   3.601746  \n","21  0.219695   4.826583  \n","19  0.233866   5.469318  \n","5   0.256376   6.572884  \n","10  0.264983   7.021586  \n","2   0.269520   7.264128  \n","23  0.271314   7.361145  \n","6   0.271984   7.397526  \n","11  0.272429   7.421780  \n","15  0.276187   7.627941  \n","1   0.276845   7.664322  \n","9   0.278592   7.761339  \n","17  0.279678   7.821974  \n","0   0.286108   8.185787  \n","13  0.286320   8.197914  \n","3   0.292811   8.573854  \n","7   0.308738   9.531894  \n","14  0.342446  11.726898  \n","8   0.345793  11.957313  \n","20  0.370348  13.715741  \n","18  0.374580  14.031045  \n","16  0.382589  14.637400  \n","22  0.387001  14.976959  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}],"source":["# --- MLP Classifier ---\n","mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n","mlp.fit(X_train_scaled, y_train)\n","mlp_pred = mlp.predict(X_test_scaled)\n","\n","# MLP Hyperparameter Grid (24 experiments)\n","mlp_param_grid = {\n","    'hidden_layer_sizes': [(64,), (64, 32), (128, 64, 32), (128,)],\n","    'activation': ['relu', 'tanh', 'logistic'],\n","    'learning_rate_init': [0.001, 0.01]\n","}\n","\n","grid = list(ParameterGrid(mlp_param_grid))\n","mlp_results = []\n","\n","for i, params in enumerate(grid):\n","    print(f\"\\nğŸ§  Training MLP {i+1}/{len(grid)} with: {params}\")\n","\n","    mlp = MLPClassifier(\n","        hidden_layer_sizes=params['hidden_layer_sizes'],\n","        activation=params['activation'],\n","        learning_rate_init=params['learning_rate_init'],\n","        solver='adam',\n","        max_iter=300,\n","        random_state=42\n","    )\n","\n","    mlp.fit(X_train_scaled, y_train)\n","    mlp_pred = mlp.predict(X_test_scaled)\n","\n","    acc, mae, rmse, mape = evaluate(y_test, mlp_pred)\n","\n","    mlp_results.append({\n","        **params,\n","        'Accuracy': acc,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'MAPE': mape\n","    })\n","\n","# MLP Results\n","mlp_results_df = pd.DataFrame(mlp_results)\n","print(\"\\nğŸ“Š MLP Experiment Results:\")\n","print(mlp_results_df.sort_values(by='Accuracy', ascending=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1750767133518,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"JY0vFw7MMQ2U","outputId":"57924d4b-3300-43ec-f32b-cdfa564442e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["                   Model  Accuracy       MAE      RMSE       MAPE\n","0          Random Forest  0.992117  0.007883  0.088784   0.788261\n","1                    SVM  0.826098  0.173902  0.417016  17.390250\n","2                   LSTM  0.869755  0.130245  0.360895  13.024497\n","3                XGBoost  0.979626  0.020374  0.142736   2.037351\n","4          Decision Tree  0.986660  0.013340  0.115498   1.333980\n","5  Multilayer Perceptron  0.925661  0.074339  0.272652   7.433907\n"]}],"source":["# --- Results ---\n","rf_metrics = evaluate(y_test, rf_pred)\n","svm_metrics = evaluate(y_test, svm_pred)\n","lstm_metrics = evaluate(y_test, lstm_pred)\n","xgb_metrics = evaluate(y_test, xgb_pred)\n","dt_metrics = evaluate(y_test, dt_pred)\n","mlp_metrics = evaluate(y_test, mlp_pred)\n","\n","results = pd.DataFrame({\n","    'Model': ['Random Forest', 'SVM', 'LSTM', 'XGBoost', 'Decision Tree', 'Multilayer Perceptron' ],\n","    'Accuracy': [rf_metrics[0], svm_metrics[0], lstm_metrics[0], xgb_metrics[0], dt_metrics[0], mlp_metrics[0]],\n","    'MAE': [rf_metrics[1], svm_metrics[1], lstm_metrics[1], xgb_metrics[1], dt_metrics[1], mlp_metrics[1]],\n","    'RMSE': [rf_metrics[2], svm_metrics[2], lstm_metrics[2], xgb_metrics[2], dt_metrics[2], mlp_metrics[2]],\n","    'MAPE': [rf_metrics[3], svm_metrics[3], lstm_metrics[3], xgb_metrics[3], dt_metrics[3], mlp_metrics[3]]\n","})\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61S0M3PYo2Op"},"outputs":[],"source":["# Define the models dictionary\n","models = {\n","    \"Random Forest\": rf_pred,\n","    \"SVM\": svm_pred,\n","    \"LSTM\": lstm_pred,\n","    \"XGBoost\": xgb_pred,\n","    \"Decision Tree\": dt_pred,\n","    \"Multilayer Perceptron\": mlp_pred\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1750767136649,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"NUjiwsuQ4kyQ","outputId":"02b37a71-22ad-4daf-b0ea-43926a0b90b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 15 Feature Importances:\n","- Turbidity: 0.1712\n","- SSTA_Standard_Deviation: 0.1167\n","- Temperature_Mean: 0.0519\n","- Distance_to_Shore: 0.0359\n","- Cyclone_Frequency: 0.0330\n","- Temperature_Kelvin_Standard_Deviation: 0.0323\n","- TSA_Mean: 0.0321\n","- TSA_Standard_Deviation: 0.0295\n","- TSA_DHW_Standard_Deviation: 0.0258\n","- TSA_Frequency_Standard_Deviation: 0.0238\n","- SSTA_DHWMean: 0.0235\n","- TSA: 0.0230\n","- Temperature_Kelvin: 0.0227\n","- SSTA_Frequency_Standard_Deviation: 0.0224\n","- SSTA_DHW_Standard_Deviation: 0.0220\n"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA2NFJREFUeJzs3XlcTfn/B/DX1XJbbosUlUqpVJIs2Woo25R9FyIpxjqWkW1shWTfZqxDZXwZe/a9kSUMoiw1lkiGjG1EIqnz+8Oj83PconCnmNfz8TiPR+dzPufzeZ9zbvS+n885RyYIggAiIiIiIiIi+uzKlHQARERERERERF8rJt1EREREREREKsKkm4iIiIiIiEhFmHQTERERERERqQiTbiIiIiIiIiIVYdJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdREREREQlzMvLC15eXiUdBhGpAJNuIiKit8hksiItsbGxKo9l6dKl6NKlC6ysrCCTyRAQEFBgvaioqELjvHfv3gf78fLyKnT/P//88zMf1RtLlixBVFSUStr+VF5eXqhWrVpJh/HR7t69i5CQECQkJJR0KKVSSEiI5DOuoaEBa2trDB06FE+ePCnp8EqNd8/T28uyZctKOjwlWVlZCAkJ+Vf+bSYqLvWSDoCIiKg0WbNmjWT9119/xcGDB5XKnZycVB7LzJkz8ezZM9StWxfp6ekfrD9lyhTY2NhIygwNDYvUl4WFBcLDw5XKzc3Ni7R/cS1ZsgTGxsaFfpFAH+/u3bsIDQ2FtbU1atSoUdLhlFpLly6FQqHA8+fPERMTg59++gnnzp3D8ePHSzq0UiX/PL2tXr16JRRN4bKyshAaGgoAnDFApQ6TbiIiorf07NlTsn7q1CkcPHhQqfzfcOTIEXGU+90/egvSokULuLm5fVRfBgYGJXKMn5MgCHj58iW0tbVLOpQS8fr1a+Tl5ZV0GF+Mzp07w9jYGADQv39/dOvWDRs2bMDp06dRt27dEo6u9Hj7PH1Oz58/h66u7mdvl6g04vRyIiKiYnr+/DlGjhwJS0tLyOVyODg4YM6cORAEQVJPJpNhyJAhWLt2LRwcHKClpYXatWvj6NGjReqnUqVKkMlkxYrt2bNnyM3NLdY+RZGdnY3JkyfDzs4OcrkclpaWGD16NLKzsyX1IiMj0aRJE5QvXx5yuRxVq1bF0qVLJXWsra1x+fJlHDlyRJyumj8ylT+l9V35U+hTU1Ml7bRu3Rr79++Hm5sbtLW1sXz5cgDAkydPMHz4cPEa2dnZYebMmR+dlOZfy02bNqFq1arQ1tZGgwYNcPHiRQDA8uXLYWdnBy0tLXh5eUniBP5/ynp8fDzc3d2hra0NGxubAqfp3r9/H0FBQahQoQK0tLTg6uqK1atXS+qkpqZCJpNhzpw5WLBgAWxtbSGXy7FkyRLUqVMHANCnTx/x/OZP5T927Jh4y0L+dRwxYgRevHghaT8gIAAKhQJ37txB+/btoVAoYGJiguDgYKXPV15eHhYuXAgXFxdoaWnBxMQEPj4+OHv2rKTe//73P9SuXRva2towMjJCt27dcPv2bUmda9euoVOnTjA1NYWWlhYsLCzQrVs3ZGRkFO1CfYKGDRsCAFJSUsSyx48fIzg4GC4uLlAoFNDX10eLFi2QmJgo2Tc2NhYymQwbN25EWFgYLCwsoKWlhaZNm+L69etKfa1YsQK2trbQ1tZG3bp1cezYsQJjKu5nYfHixahcuTJ0dHTw7bff4vbt2xAEAVOnToWFhQW0tbXRrl07PH78+FNPl2jTpk3idTU2NkbPnj1x584dSZ38z1NKSgpatmwJPT09+Pn5AXjz+VmwYAGcnZ2hpaWFChUqoH///vjnn38kbZw9exbe3t4wNjYWf38CAwPFc2BiYgIACA0NFT/3ISEhn+04iT4FR7qJiIiKQRAEtG3bFocPH0ZQUBBq1KiB/fv3Y9SoUbhz5w7mz58vqX/kyBFs2LABQ4cOFZMiHx8fnD59+rPfN9y4cWNkZmZCU1MT3t7emDt3Luzt7Yu0b25uLh4+fCgp09LSgkKhQF5eHtq2bYvjx4/ju+++g5OTEy5evIj58+fj6tWr2LZtm7jP0qVL4ezsjLZt20JdXR07d+7EoEGDkJeXh8GDBwMAFixYgO+//x4KhQLjx48HAFSoUOGjjvnKlSvo3r07+vfvj379+sHBwQFZWVnw9PTEnTt30L9/f1hZWeHEiRMYN24c0tPTsWDBgo/q69ixY9ixY4d4HOHh4WjdujVGjx6NJUuWYNCgQfjnn38wa9YsBAYG4vfff5fs/88//6Bly5bo2rUrunfvjo0bN2LgwIHQ1NQUk4cXL17Ay8sL169fx5AhQ2BjY4NNmzYhICAAT548wbBhwyRtRkZG4uXLl/juu+8gl8vRoUMHPHv2DJMmTcJ3330nJpLu7u4A3iRIWVlZGDhwIMqVK4fTp0/jp59+wl9//YVNmzZJ2s7NzYW3tzfq1auHOXPm4NChQ5g7dy5sbW0xcOBAsV5QUBCioqLQokUL9O3bF69fv8axY8dw6tQpceZFWFgYJk6ciK5du6Jv37548OABfvrpJzRq1Ajnz5+HoaEhXr16BW9vb2RnZ+P777+Hqakp7ty5g127duHJkycwMDD4qOtWVPlflJQtW1Ysu3HjBrZt24YuXbrAxsYGf//9N5YvXw5PT08kJSUp3X4xY8YMlClTBsHBwcjIyMCsWbPg5+eHP/74Q6yzatUq9O/fH+7u7hg+fDhu3LiBtm3bwsjICJaWlmK94n4W1q5di1evXuH777/H48ePMWvWLHTt2hVNmjRBbGwsxowZg+vXr+Onn35CcHAwIiIiinRe3k3Q1dTUxHMUFRWFPn36oE6dOggPD8fff/+NhQsXIi4uTryu+V6/fg1vb2988803mDNnDnR0dAC8mWWQ387QoUNx8+ZN/Pzzzzh//jzi4uKgoaGB+/fv49tvv4WJiQnGjh0LQ0NDpKamYuvWrQAAExMTLF26FAMHDkSHDh3QsWNHAED16tWLdIxEKicQERFRoQYPHiy8/d/ltm3bBADCtGnTJPU6d+4syGQy4fr162IZAAGAcPbsWbHs1q1bgpaWltChQ4dixaGrqyv07t27wG0bNmwQAgIChNWrVwvR0dHChAkTBB0dHcHY2FhIS0v7YNuenp5irG8v+f2tWbNGKFOmjHDs2DHJfsuWLRMACHFxcWJZVlaWUvve3t5C5cqVJWXOzs6Cp6enUt3JkycLBf15EhkZKQAQbt68KZZVqlRJACDs27dPUnfq1KmCrq6ucPXqVUn52LFjBTU1tQ+eE09PT8HZ2VlSBkCQy+WS/pcvXy4AEExNTYWnT5+K5ePGjVOKNf8cz507VyzLzs4WatSoIZQvX1549eqVIAiCsGDBAgGA8L///U+s9+rVK6FBgwaCQqEQ+7l586YAQNDX1xfu378vifXMmTMCACEyMlLp2Aq6PuHh4YJMJhNu3bollvXu3VsAIEyZMkVSt2bNmkLt2rXF9d9//10AIAwdOlSp3by8PEEQBCE1NVVQU1MTwsLCJNsvXrwoqKuri+Xnz58XAAibNm1Sautzyv+MXblyRXjw4IGQmpoqRERECNra2oKJiYnw/Plzse7Lly+F3Nxcyf43b94U5HK55NwcPnxYACA4OTkJ2dnZYvnChQsFAMLFixcFQXhzLcuXLy/UqFFDUm/FihUCAMnvRHE/CyYmJsKTJ0/EuvmfQ1dXVyEnJ0cs7969u6CpqSm8fPmySOfp3aVSpUqSY6lWrZrw4sULcb9du3YJAIRJkyaJZfmfp7Fjx0r6OHbsmABAWLt2raR83759kvLo6GgBgHDmzJlC433w4IEAQJg8efJ7j4uoJHB6ORERUTHs2bMHampqGDp0qKR85MiREAQBe/fulZQ3aNAAtWvXFtetrKzQrl077N+//7NNA+/atSsiIyPh7++P9u3bY+rUqdi/fz8ePXqEsLCwIrVhbW2NgwcPSpbRo0cDeDM66uTkBEdHRzx8+FBcmjRpAgA4fPiw2M7b91NnZGTg4cOH8PT0xI0bN1QyRdjGxgbe3t6Ssk2bNqFhw4YoW7asJN5mzZohNze3yNP739W0aVNYW1uL6/kPk+rUqRP09PSUym/cuCHZX11dHf379xfXNTU10b9/f9y/fx/x8fEA3ny+TE1N0b17d7GehoYGhg4diszMTBw5ckTSZqdOncRptUXx9vV5/vw5Hj58CHd3dwiCgPPnzyvVHzBggGS9YcOGkuPasmULZDIZJk+erLRv/m0CW7duRV5eHrp27Sq5HqamprC3txc/P/kj2fv370dWVlaRj+ljOTg4wMTEBNbW1ggMDISdnR327t0rjsACgFwuR5kyb/5czs3NxaNHj6BQKODg4IBz584ptdmnTx9oamqK6/kzDfLP2dmzZ3H//n0MGDBAUi8gIEBpJL+4n4UuXbpI2sj/HPbs2RPq6uqS8levXilNAS/Mli1bJP8urF27VnIsgwYNgpaWlli/VatWcHR0xO7du5XaenuGBPDmd9XAwADNmzeXfDZq164NhUIhfjbyR8x37dqFnJycIsVNVJpwejkREVEx3Lp1C+bm5pIkC/j/p5nfunVLUl7Q9O4qVaogKysLDx48gKmpqUri/Oabb1CvXj0cOnSoSPV1dXXRrFmzArddu3YNycnJhSZ39+/fF3+Oi4vD5MmTcfLkSaXEKSMj47NPEX73ae358V64cKFI8RaHlZWVZD3/WN6eEvx2+bv3pJqbmys9OKpKlSoA3kxtrl+/Pm7dugV7e3sx0ctX2OeroON/n7S0NEyaNAk7duxQiu/dL0Xy789+W9myZSX7paSkwNzcHEZGRoX2ee3aNQiCUOitDhoaGuKx/PDDD5g3bx7Wrl2Lhg0bom3btujZs+d7PzeZmZnIzMwU19XU1Ir0RcSWLVugr6+PBw8eYNGiRbh586bSQ/jy71dfsmQJbt68KfmirFy5ckptvvsZyZ+GnX/O8q/fu+dCQ0MDlStXlpQV97PwqZ/PwjRq1KjAB6nl9+/g4KC0zdHRUekp8Orq6rCwsJCUXbt2DRkZGShfvnyBfef/rnp6eqJTp04IDQ3F/Pnz4eXlhfbt26NHjx6Qy+VFOg6iksSkm4iI6CtlaWmJK1eufHI7eXl5cHFxwbx58wrtB3iTgDVt2hSOjo6YN28eLC0toampiT179mD+/PlFeohZYQ+OK2xWQEFPKs/Ly0Pz5s3Fkfp35Se6xaWmplascuGdB+upQnGe1J6bm4vmzZvj8ePHGDNmDBwdHaGrq4s7d+4gICBA6foUdlzFlZeXB5lMhr179xbY5ttP5p87dy4CAgKwfft2HDhwAEOHDkV4eDhOnTqllLDlmzNnjviqKODNAwjffZBdQd5OJtu0aQMXFxf4+fkhPj5eTHSnT5+OiRMnIjAwEFOnToWRkRHKlCmD4cOHF/h5LsnPQmn8fL7t7VkD+fLy8lC+fHlx9Pxd+V+eyGQybN68GadOncLOnTuxf/9+BAYGYu7cuTh16lSR3u5AVJKYdBMRERVDpUqVcOjQITx79kwy2v3nn3+K29927do1pTauXr0KHR2dYk0L/hg3btz4LH3Y2toiMTERTZs2fe/T1Hfu3Ins7Gzs2LFDMur29vTzfIW1kz8y+OTJE8lDmN4d1ftQvJmZmYWO3JeUu3fvKr0m6erVqwAgTluvVKkSLly4gLy8PEmCUtjnqyCFnduLFy/i6tWrWL16Nfz9/cXygwcPFvtY8tna2mL//v14/PhxoaPdtra2EAQBNjY2RfrCw8XFBS4uLpgwYQJOnDgBDw8PLFu2DNOmTSuwvr+/P7755htx/WNeGadQKDB58mT06dMHGzduRLdu3QAAmzdvRuPGjbFq1SpJ/SdPnnzUa7Tyr9+1a9fE2zMAICcnBzdv3oSrq6uk7qd+FlQpv/8rV65IjiW/rCjx2dra4tChQ/Dw8CjSdatfvz7q16+PsLAwrFu3Dn5+fli/fj369u1b7Dc9EP2beE83ERFRMbRs2RK5ubn4+eefJeXz58+HTCZDixYtJOUnT56U3Pt5+/ZtbN++Hd9+++1nG0l88OCBUtmePXsQHx8PHx+fT26/a9euuHPnDn755RelbS9evMDz588B/P+I2tsjaBkZGYiMjFTaT1dXF0+ePFEqt7W1BQDJfdfPnz9Xek3Sh+I9efIk9u/fr7TtyZMneP36dZHb+pxev34tvtIMAF69eoXly5fDxMREvO+/ZcuWuHfvHjZs2CDZ76effoJCoYCnp+cH+8lP6t89vwVdH0EQsHDhwo8+pk6dOkEQBMlI89ttA0DHjh2hpqaG0NBQpdFVQRDw6NEjAMDTp0+Vro2LiwvKlCmj9Gq6t1WuXBnNmjUTFw8Pj486Fj8/P1hYWGDmzJlimZqamlLMmzZtKvL90O9yc3ODiYkJli1bhlevXonlUVFRStfrc3wWVMnNzQ3ly5fHsmXLJNdn7969SE5ORqtWrT7YRteuXZGbm4upU6cqbXv9+rV4Tv755x+l61CjRg0AEPvOvxe/oH9XiEoaR7qJiIiKoU2bNmjcuDHGjx+P1NRUuLq64sCBA9i+fTuGDx8uJo35qlWrBm9vb8krwwAUmKS8a+fOneL7gHNycnDhwgVxtK9t27bi63Dc3d1Rs2ZNuLm5wcDAAOfOnUNERAQsLS3x448/fvIx9+rVCxs3bsSAAQNw+PBheHh4IDc3F3/++Sc2btwovif722+/haamJtq0aYP+/fsjMzMTv/zyC8qXL4/09HRJm7Vr18bSpUsxbdo02NnZoXz58mjSpAm+/fZbWFlZISgoCKNGjYKamhoiIiJgYmKCtLS0IsU7atQo7NixA61bt0ZAQABq166N58+f4+LFi9i8eTNSU1M/apTyU5mbm2PmzJlITU1FlSpVsGHDBiQkJGDFihXifc3fffcdli9fjoCAAMTHx8Pa2hqbN29GXFwcFixYoPQsgYLY2trC0NAQy5Ytg56eHnR1dVGvXj04OjrC1tYWwcHBuHPnDvT19bFly5Yi39tbkMaNG6NXr15YtGgRrl27Bh8fH+Tl5eHYsWNo3LgxhgwZAltbW0ybNg3jxo1Damoq2rdvDz09Pdy8eRPR0dH47rvvEBwcjN9//x1DhgxBly5dUKVKFbx+/Rpr1qyBmpoaOnXq9NExFpWGhgaGDRuGUaNGYd++ffDx8UHr1q0xZcoU9OnTB+7u7rh48SLWrl2rdP91cfqYNm0a+vfvjyZNmsDX1xc3b95EZGSkUpuf47OgShoaGpg5cyb69OkDT09PdO/eXXxlmLW1NUaMGPHBNjw9PdG/f3+Eh4cjISEB3377LTQ0NHDt2jVs2rQJCxcuROfOnbF69WosWbIEHTp0gK2tLZ49e4ZffvkF+vr6aNmyJYA3MxyqVq2KDRs2oEqVKjAyMkK1atU++6sZiT7Kv//AdCIioi/Hu68MEwRBePbsmTBixAjB3Nxc0NDQEOzt7YXZs2eLr0jKB0AYPHiw8L///U+wt7cX5HK5ULNmTeHw4cNF6jv/NTsFLW+/Dmr8+PFCjRo1BAMDA0FDQ0OwsrISBg4cKNy7d69I/RT0iqx3vXr1Spg5c6bg7OwsyOVyoWzZskLt2rWF0NBQISMjQ6y3Y8cOoXr16oKWlpZgbW0tzJw5U4iIiFB6hda9e/eEVq1aCXp6ekqvSoqPjxfq1asnaGpqClZWVsK8efMKfWVYq1atCoz32bNnwrhx4wQ7OztBU1NTMDY2Ftzd3YU5c+aIr+cqzvnIv5Zvy39V0+zZsyXl+a+PevvVV/ltnj17VmjQoIGgpaUlVKpUSfj555+V+v/777+FPn36CMbGxoKmpqbg4uKi9PqvwvrOt337dqFq1aqCurq65POSlJQkNGvWTFAoFIKxsbHQr18/ITExUekz1bt3b0FXV1ep3YJe6fb69Wth9uzZgqOjo6CpqSmYmJgILVq0EOLj4yX1tmzZInzzzTeCrq6uoKurKzg6OgqDBw8Wrly5IgiCINy4cUMIDAwUbG1tBS0tLcHIyEho3LixcOjQoQKP8WPlH8ODBw+UtmVkZAgGBgbi5/Hly5fCyJEjBTMzM0FbW1vw8PAQTp48KXh6eko+swVdc0H4/+v07vVbsmSJYGNjI8jlcsHNzU04evSoUpuC8GmfhcJiyv9det/rtz50nt62YcMGoWbNmoJcLheMjIwEPz8/4a+//pLUKezzlG/FihVC7dq1BW1tbUFPT09wcXERRo8eLdy9e1cQBEE4d+6c0L17d8HKykqQy+VC+fLlhdatW0texygIgnDixAmhdu3agqamJl8fRqWKTBD+5acoEBER/UfIZDIMHjxYaSo6/fd4eXnh4cOHuHTpUkmHQkRE/zLe001ERERERESkIky6iYiIiIiIiFSESTcRERERERGRivCebiIiIiIiIiIV4Ug3ERERERERkYow6SYiIiIiIiJSEfWSDoCIqCjy8vJw9+5d6OnpQSaTlXQ4RERERPQfJwgCnj17BnNzc5QpU/h4NpNuIvoi3L17F5aWliUdBhERERGRxO3bt2FhYVHodibdRPRF0NPTA/DmHzV9ff0SjoaIiIiI/uuePn0KS0tL8e/UwjDpJqIvQv6Ucn19fSbdRERERFRqfOjWRz5IjYiIiIiIiEhFONJNRF+URhN+g5pcu6TDICIiIqJSIH62f0mH8EEc6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVIRJN9F/jLW1NRYsWPDeOjKZDNu2bSt0e2pqKmQyGRISEgAAsbGxkMlkePLkyWeLk4iIiIjoa8Ckm6iUkslk711CQkJU1nd6ejpatGhR5Pru7u5IT0+HgYEBACAqKgqGhoYqio6IiIiI6MuhXtIBEFHB0tPTxZ83bNiASZMm4cqVK2KZQqEoVnuvXr2CpqZmkeqampoWq21NTc1i70NERERE9F/AkW6iUsrU1FRcDAwMIJPJxPVly5bhm2++kdRfsGABrK2txfWAgAC0b98eYWFhMDc3h4ODg7jt2bNn6N69O3R1dVGxYkUsXrxY0ta708tPnz6NmjVrQktLC25ubjh//ryk/tvTy2NjY9GnTx9kZGRIRuWnTJmCatWqKR1njRo1MHHixE84U0REREREpReTbqKvWExMDK5cuYKDBw9i165dYvns2bPh6uqK8+fPY+zYsRg2bBgOHjxYYBuZmZlo3bo1qlativj4eISEhCA4OLjQPt3d3bFgwQLo6+sjPT0d6enpCA4ORmBgIJKTk3HmzBmx7vnz53HhwgX06dNHqZ3s7Gw8ffpUshARERERfWk4vZzoK6arq4uVK1cqTSv38PDA2LFjAQBVqlRBXFwc5s+fj+bNmyu1sW7dOuTl5WHVqlXQ0tKCs7Mz/vrrLwwcOLDAPjU1NSUj8/kUCgW8vb0RGRmJOnXqAAAiIyPh6emJypUrK7UTHh6O0NDQjz52IiIiIqLSgCPdRF8xFxeXAu/jbtCggdJ6cnJygW0kJyejevXq0NLSKnT/ourXrx9+++03vHz5Eq9evcK6desQGBhYYN1x48YhIyNDXG7fvv1RfRIRERERlSSOdBN9gcqUKQNBECRlOTk5SvV0dXX/rZCKpE2bNpDL5YiOjoampiZycnLQuXPnAuvK5XLI5fJ/OUIiIiIios+LSTfRF8jExAT37t2DIAiQyWQAIL4zuyhOnTqltO7k5FRgXScnJ6xZswYvX74UR7vf3f9dmpqayM3NVSpXV1dH7969ERkZCU1NTXTr1g3a2tpFjpuIiIiI6EvD6eVEXyAvLy88ePAAs2bNQkpKChYvXoy9e/cWef+4uDjMmjULV69exeLFi7Fp0yYMGzaswLo9evSATCZDv379kJSUhD179mDOnDnvbd/a2hqZmZmIiYnBw4cPkZWVJW7r27cvfv/9d+zbt6/QqeVERERERF8LJt1EXyAnJycsWbIEixcvhqurK06fPv3eJ4q/a+TIkTh79ixq1qyJadOmYd68efD29i6wrkKhwM6dO3Hx4kXUrFkT48ePx8yZM9/bvru7OwYMGABfX1+YmJhg1qxZ4jZ7e3u4u7vD0dER9erVK3LMRERERERfIpnw7o2hREQqJAgC7O3tMWjQIPzwww9F3u/p06cwMDCA6/fLoCbnlHQiIiIiAuJn+5dY3/l/n2ZkZEBfX7/Qerynm4j+NQ8ePMD69etx7969At/NTURERET0tWHSTUT/mvLly8PY2BgrVqxA2bJlSzocIiIiIiKVY9JNRP8a3s1CRERERP81fJAaERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRfggNSL6ohyd1v2970EkIiIiIipNONJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCp5cT0Rel0YTfoCbXLukwiIiIPrv42f4lHQIRqQBHuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTf9ZXl5eGD58+BfX9qeIioqCoaFhqWmHiIiIiOhrx6T7K/HgwQMMHDgQVlZWkMvlMDU1hbe3N+Li4gAAiYmJaNu2LcqXLw8tLS1YW1vD19cX9+/fR0hICGQy2XuXfL/99hvU1NQwePDgYsf4yy+/wNXVFQqFAoaGhqhZsybCw8PF7QEBAWjfvv0nn4svjZeXl3ie5XI5KlasiDZt2mDr1q2fvS9fX19cvXq1WPtYW1tjwYIFn9wOEREREdF/EZPur0SnTp1w/vx5rF69GlevXsWOHTvg5eWFR48e4cGDB2jatCmMjIywf/9+JCcnIzIyEubm5nj+/DmCg4ORnp4uLhYWFpgyZYqkLN+qVaswevRo/Pbbb3j58mWR44uIiMDw4cMxdOhQJCQkIC4uDqNHj0ZmZqYqTse/QhAEvH79+rO01a9fP6SnpyMlJQVbtmxB1apV0a1bN3z33Xefpf182traKF++fKlph4iIiIjoa8ek+yvw5MkTHDt2DDNnzkTjxo1RqVIl1K1bF+PGjUPbtm0RFxeHjIwMrFy5EjVr1oSNjQ0aN26M+fPnw8bGBgqFAqampuKipqYGPT09SRkA3Lx5EydOnMDYsWNRpUqVYo3E7tixA127dkVQUBDs7Ozg7OyM7t27IywsDAAQEhKC1atXY/v27eKob2xsLABgzJgxqFKlCnR0dFC5cmVMnDgROTk5YtshISGoUaMG1qxZA2traxgYGKBbt2549uyZWOf58+fw9/eHQqGAmZkZ5s6dqxTjmjVr4ObmJh57jx49cP/+fXF7bGwsZDIZ9u7di9q1a0Mul+P48eNFavtDdHR0YGpqCgsLC9SvXx8zZ87E8uXL8csvv+DQoUNivdu3b6Nr164wNDSEkZER2rVrh9TUVADAgQMHoKWlhSdPnkjaHjZsGJo0aQJAeVp4SkoK2rVrhwoVKkChUKBOnTqS/ry8vHDr1i2MGDFCMuuhoOnlS5cuha2tLTQ1NeHg4IA1a9ZItstkMqxcuRIdOnSAjo4O7O3tsWPHjmKfKyIiIiKiLwmT7q+AQqGAQqHAtm3bkJ2drbTd1NQUr1+/RnR0NARB+Oh+IiMj0apVKxgYGKBnz55YtWpVkfc1NTXFqVOncOvWrQK3BwcHo2vXrvDx8RFH193d3QEAenp6iIqKQlJSEhYuXIhffvkF8+fPl+yfkpKCbdu2YdeuXdi1axeOHDmCGTNmiNtHjRqFI0eOYPv27Thw4ABiY2Nx7tw5SRs5OTmYOnUqEhMTsW3bNqSmpiIgIEAp1rFjx2LGjBlITk5G9erVi9T2x+jduzfKli0rfrmRk5MDb29v6Onp4dixY4iLi4NCoYCPjw9evXqFpk2bwtDQEFu2bBHbyM3NxYYNG+Dn51dgH5mZmWjZsiViYmJw/vx5+Pj4oE2bNkhLSwMAbN26VWnmQ0Gio6MxbNgwjBw5EpcuXUL//v3Rp08fHD58WFIvNDQUXbt2xYULF9CyZUv4+fnh8ePHn3yuiIiIiIhKKybdXwF1dXVERUVh9erVMDQ0hIeHB3788UdcuHABAFC/fn38+OOP6NGjB4yNjdGiRQvMnj0bf//9d5H7yMvLQ1RUFHr27AkA6NatG44fP46bN28Waf/JkyfD0NAQ1tbWcHBwQEBAADZu3Ii8vDwAb7440NbWFu9HNzU1haamJgBgwoQJcHd3h7W1Ndq0aYPg4GBs3LixwPiqVauGhg0bolevXoiJiQHwJrFctWoV5syZg6ZNm8LFxQWrV69WmhoeGBiIFi1aoHLlyqhfvz4WLVqEvXv3Kk2BnzJlCpo3by6O6hal7Y9RpkwZVKlSRRzJ3rBhA/Ly8rBy5Uq4uLjAyckJkZGRSEtLQ2xsLNTU1NCtWzesW7dObCMmJgZPnjxBp06dCuzD1dUV/fv3R7Vq1WBvb4+pU6fC1tZWHIE2MjJSmvlQkDlz5iAgIACDBg1ClSpV8MMPP6Bjx46YM2eOpF5AQAC6d+8OOzs7TJ8+HZmZmTh9+nSBbWZnZ+Pp06eShYiIiIjoS8Ok+yvRqVMn3L17Fzt27ICPjw9iY2NRq1YtREVFAQDCwsJw7949LFu2DM7Ozli2bBkcHR1x8eLFIrV/8OBBPH/+HC1btgQAGBsbo3nz5oiIiCjS/mZmZjh58iQuXryIYcOG4fXr1+jduzd8fHzExLswGzZsgIeHB0xNTaFQKDBhwgRxJDaftbU19PT0JP3lTw1PSUnBq1evUK9ePXG7kZERHBwcJG3Ex8ejTZs2sLKygp6eHjw9PQFAqS83Nzfx56K2/bEEQRCndCcmJuL69evQ09MTZzcYGRnh5cuXSElJAQD4+fkhNjYWd+/eBQCsXbsWrVq1KvRJ45mZmQgODoaTkxMMDQ2hUCiQnJysdMwfkpycDA8PD0mZh4cHkpOTJWXVq1cXf9bV1YW+vr5kCv/bwsPDYWBgIC6WlpbFiomIiIiIqDRg0v0V0dLSQvPmzTFx4kScOHECAQEBmDx5sri9XLly6NKlC+bMmYPk5GSYm5srjUQWZtWqVXj8+DG0tbWhrq4OdXV17NmzB6tXr/5g0vy2atWqYdCgQfjf//6HgwcP4uDBgzhy5Eih9U+ePAk/Pz+0bNkSu3btwvnz5zF+/Hi8evVKUk9DQ0OyLpPJihXX8+fP4e3tDX19faxduxZnzpxBdHQ0ACj1paurW+R2P0Vubi6uXbsGGxsbAG8S5Nq1ayMhIUGyXL16FT169AAA1KlTB7a2tli/fj1evHiB6OjoQqeWA2+m9UdHR2P69Ok4duwYEhIS4OLionTMn0txrtO4ceOQkZEhLrdv31ZJTEREREREqqRe0gGQ6lStWhXbtm0rcJumpiZsbW3x/PnzD7bz6NEjbN++HevXr4ezs7NYnpubi2+++QYHDhyAj4/PR8UHQIxBU1MTubm5kjonTpxApUqVMH78eLGssPvCC2NrawsNDQ388ccfsLKyAgD8888/uHr1qjia/eeff+LRo0eYMWOGOKJ69uzZz9L2x1q9ejX++ecfcWp4rVq1sGHDBpQvXx76+vqF7ufn54e1a9fCwsICZcqUQatWrQqtGxcXh4CAAHTo0AHAm8Q+fzp7voKuy7ucnJwQFxeH3r17S9rOv8YfQy6XQy6Xf/T+RERERESlAZPur8CjR4/QpUsXBAYGonr16tDT08PZs2cxa9YstGvXDrt27cL69evRrVs3VKlSBYIgYOfOndizZw8iIyM/2P6aNWtQrlw5dO3aVfLObgBo2bIlVq1a9cGke+DAgTA3N0eTJk1gYWGB9PR0TJs2DSYmJmjQoAGAN1PE9+/fjytXrqBcuXIwMDCAvb090tLSsH79etSpUwe7d+8WR6CLSqFQICgoCKNGjUK5cuVQvnx5jB8/HmXK/P9EDysrK2hqauKnn37CgAEDcOnSJUydOvWztF0UWVlZuHfvHl6/fo2//voL0dHRmD9/PgYOHIjGjRsDeJNMz549G+3atcOUKVNgYWGBW7duYevWrRg9ejQsLCzEeiEhIQgLC0Pnzp3fm7ja29tj69ataNOmDWQyGSZOnKg08mxtbY2jR4+iW7dukMvlMDY2Vmpn1KhR6Nq1K2rWrIlmzZph586d2Lp1q+RJ6ERERERE/0VMur8CCoUC9erVw/z585GSkoKcnBxYWlqiX79++PHHH5Geng4dHR2MHDkSt2/fhlwuh729PVauXIlevXp9sP2IiAh06NBBKeEG3txL3qtXLzx8+LDAZCxfs2bNEBERgaVLl+LRo0cwNjZGgwYNEBMTg3LlygF4867q2NhYuLm5ITMzE4cPH0bbtm0xYsQIDBkyBNnZ2WjVqhUmTpyIkJCQYp2j2bNnIzMzE23atIGenh5GjhyJjIwMcbuJiQmioqLw448/YtGiRahVqxbmzJmDtm3bfnLbRfHLL7/gl19+gaamJsqVK4fatWtjw4YN4gg08Oa1YkePHsWYMWPQsWNHPHv2DBUrVkTTpk0lI992dnaoW7cuTp8+jQULFry333nz5iEwMBDu7u4wNjbGmDFjlB5YNmXKFPTv3x+2trbIzs4u8An47du3x8KFCzFnzhwMGzYMNjY2iIyMhJeXV7HOAxERERHR10YmfMo7pIiI/iVPnz6FgYEBXL9fBjW5dkmHQ0RE9NnFz/Yv6RCIqBjy/z7NyMh47+2ffJAaERERERERkYow6abPokWLFuJrrN5dpk+fXtLhlZhjx44Vel4UCkVJh0dERERERCrGe7rps1i5ciVevHhR4DYjI6N/OZrSw83NDQkJCSUdBhERERERlRAm3fRZVKxYsaRDKJW0tbVhZ2dX0mEQEREREVEJ4fRyIiIiIiIiIhVh0k1ERERERESkIky6iYiIiIiIiFSE93QT0Rfl6LTu730PIhERERFRacKRbiIiIiIiIiIVYdJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCV4YR0Rel0YTfoCbXLukwiEqF+Nn+JR0CERERfQBHuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqpRMhksvcuISEhJR3iZ2dtbY0FCxaUdBgAgJCQEMhkMvj4+Chtmz17NmQyGby8vP79wIiIiIiIvjLqJR0A/Telp6eLP2/YsAGTJk3ClStXxDKFQlESYRWbIAjIzc2Fuvq/96v06tUraGpqfnI7ZmZmOHz4MP766y9YWFiI5REREbCysvrk9omIiIiIiCPdVEJMTU3FxcDAADKZTFK2fv16ODk5QUtLC46OjliyZIm4b2pqKmQyGTZu3IiGDRtCW1sbderUwdWrV3HmzBm4ublBoVCgRYsWePDggbhfQEAA2rdvj9DQUJiYmEBfXx8DBgzAq1evxDp5eXkIDw+HjY0NtLW14erqis2bN4vbY2NjIZPJsHfvXtSuXRtyuRzHjx9HSkoK2rVrhwoVKkChUKBOnTo4dOiQuJ+Xlxdu3bqFESNGiKP5wJsR5xo1akjOzYIFC2Btba0Ud1hYGMzNzeHg4AAAuH37Nrp27QpDQ0MYGRmhXbt2SE1NLfI1KF++PL799lusXr1aLDtx4gQePnyIVq1aKdVfuXJlodcEAMaMGYMqVapAR0cHlStXxsSJE5GTkyNuzz/WNWvWwNraGgYGBujWrRuePXtW5JiJiIiIiL40TLqp1Fm7di0mTZqEsLAwJCcnY/r06Zg4caIkOQSAyZMnY8KECTh37hzU1dXRo0cPjB49GgsXLsSxY8dw/fp1TJo0SbJPTEwMkpOTERsbi99++w1bt25FaGiouD08PBy//vorli1bhsuXL2PEiBHo2bMnjhw5Imln7NixmDFjBpKTk1G9enVkZmaiZcuWiImJwfnz5+Hj44M2bdogLS0NALB161ZYWFhgypQpSE9Pl4z0F0VMTAyuXLmCgwcPYteuXcjJyYG3tzf09PRw7NgxxMXFQaFQwMfHR/IlwocEBgYiKipKXI+IiICfn5/SSHpRromenh6ioqKQlJSEhQsX4pdffsH8+fMl7aSkpGDbtm3YtWsXdu3ahSNHjmDGjBkFxpadnY2nT59KFiIiIiKiLw2nl1OpM3nyZMydOxcdO3YEANjY2CApKQnLly9H7969xXrBwcHw9vYGAAwbNgzdu3dHTEwMPDw8AABBQUGShBIANDU1ERERAR0dHTg7O2PKlCkYNWoUpk6dipycHEyfPh2HDh1CgwYNAACVK1fG8ePHsXz5cnh6eortTJkyBc2bNxfXjYyM4OrqKq5PnToV0dHR2LFjB4YMGQIjIyOoqalBT08PpqamxT4nurq6WLlypZgM/+9//0NeXh5WrlwpjppHRkbC0NAQsbGx+Pbbb4vUbuvWrTFgwAAcPXoUtWvXxsaNG3H8+HFERERI6hXlmkyYMEGsb21tjeDgYKxfvx6jR48Wy/Py8hAVFQU9PT0AQK9evRATE4OwsDCl2MLDwyVfiBARERERfYmYdFOp8vz5c6SkpCAoKAj9+vUTy1+/fg0DAwNJ3erVq4s/V6hQAQDg4uIiKbt//75kH1dXV+jo6IjrDRo0QGZmJm7fvo3MzExkZWVJkmngzT3UNWvWlJS5ublJ1jMzMxESEoLdu3cjPT0dr1+/xosXL8SR7k/l4uIiGX1OTEzE9evXxeQ138uXL5GSklLkdjU0NNCzZ09ERkbixo0bqFKliuS8AkW/Jhs2bMCiRYuQkpKCzMxMvH79Gvr6+pK2rK2tJTGbmZkpXaN848aNww8//CCuP336FJaWlkU+NiIiIiKi0oBJN5UqmZmZAIBffvkF9erVk2xTU1OTrGtoaIg/54/2vluWl5dX7L53796NihUrSrbJ5XLJuq6urmQ9ODgYBw8exJw5c2BnZwdtbW107tz5g1O9y5QpA0EQJGVv3wddWH+ZmZmoXbs21q5dq1TXxMTkvX2+KzAwEPXq1cOlS5cQGBiotL0o1+TkyZPw8/NDaGgovL29YWBggPXr12Pu3LmS+m9fH+D910gulyuddyIiIiKiLw2TbipVKlSoAHNzc9y4cQN+fn6fvf3ExES8ePEC2traAIBTp05BoVDA0tISRkZGkMvlSEtLk0wlL4q4uDgEBASgQ4cOAN4kqu8+1ExTUxO5ubmSMhMTE9y7dw+CIIhfHCQkJHywv1q1amHDhg0oX7680mhycTk7O8PZ2RkXLlxAjx49lLYX5ZqcOHEClSpVwvjx48WyW7dufVJcRERERERfAybdVOqEhoZi6NChMDAwgI+PD7Kzs3H27Fn8888/kunGH+PVq1cICgrChAkTkJqaismTJ2PIkCEoU6YM9PT0EBwcjBEjRiAvLw/ffPMNMjIyEBcXB319fcn95O+yt7fH1q1b0aZNG8hkMkycOFFpBNfa2hpHjx5Ft27dIJfLYWxsDC8vLzx48ACzZs1C586dsW/fPuzdu/eDibSfnx9mz56Ndu3aYcqUKbCwsMCtW7ewdetWjB49WvIKsKL4/fffkZOTA0NDwwK3f+ia2NvbIy0tDevXr0edOnWwe/duREdHFysGIiIiIqKvEZ9eTqVO3759sXLlSkRGRsLFxQWenp6IioqCjY3NJ7fdtGlT2Nvbo1GjRvD19UXbtm0REhIibp86dSomTpyI8PBwODk5wcfHB7t37/5g3/PmzUPZsmXh7u6ONm3awNvbG7Vq1ZLUmTJlClJTU2FraytOAXdycsKSJUuwePFiuLq64vTp0wgODv7gcejo6ODo0aOwsrJCx44d4eTkhKCgILx8+fKjRr51dXULTbiBD1+Ttm3bYsSIERgyZAhq1KiBEydOYOLEicWOg4iIiIjoayMT3r2hlOgrFRAQgCdPnmDbtm0lHQp9hKdPn8LAwACu3y+Dmly7pMMhKhXiZ/uXdAhERET/Wfl/n2ZkZLx34Isj3UREREREREQqwqSb6CukUCgKXY4dO1bS4RERERER/WfwQWr0nxEVFVXSIfxr3vcE9Hdfh0ZERERERKrDpJvoK2RnZ1fSIRARERERETi9nIiIiIiIiEhlmHQTERERERERqQiTbiIiIiIiIiIV4T3dRPRFOTqt+3vfg0hEREREVJpwpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVIRJNxEREREREZGK8JVhRPRFaTThN6jJtUs6DCqF4mf7l3QIREREREo40k1ERERERESkIky6iYiIiIiIiFSESTcRERERERGRijDpJiIiIiIiIlIRJt1EREREREREKsKkm4iIiIiIiEhFmHQTERERERERqQiTbvriyGQybNu2raTD+OrExsZCJpPhyZMnJR0KEREREdFXg0k3lRoBAQGQyWSQyWTQ0NBAhQoV0Lx5c0RERCAvL0+sl56ejhYtWhSpzS81QVdF3ImJiWjbti3Kly8PLS0tWFtbw9fXF/fv3/+s/RARERER0f9j0k2lio+PD9LT05Gamoq9e/eicePGGDZsGFq3bo3Xr18DAExNTSGXy0s40i/LgwcP0LRpUxgZGWH//v1ITk5GZGQkzM3N8fz5c5X2nZOTo9L2iYiIiIhKMybdVKrI5XKYmpqiYsWKqFWrFn788Uds374de/fuRVRUFADpKPCrV68wZMgQmJmZQUtLC5UqVUJ4eDgAwNraGgDQoUMHyGQycT0lJQXt2rVDhQoVoFAoUKdOHRw6dEgSh7W1NaZPn47AwEDo6enBysoKK1askNT566+/0L17dxgZGUFXVxdubm74448/xO3bt29HrVq1oKWlhcqVKyM0NFT84uB9CosbAJYuXQpbW1toamrCwcEBa9asKdJ5jYuLQ0ZGBlauXImaNWvCxsYGjRs3xvz582FjYyOpGx8fDzc3N+jo6MDd3R1XrlyRbP9QDDKZDEuXLkXbtm2hq6uLsLCwTzofRERERERfMibdVOo1adIErq6u2Lp1q9K2RYsWYceOHdi4cSOuXLmCtWvXiknqmTNnAACRkZFIT08X1zMzM9GyZUvExMTg/Pnz8PHxQZs2bZCWliZpe+7cuXBzc8P58+cxaNAgDBw4UExAMzMz4enpiTt37mDHjh1ITEzE6NGjxWnwx44dg7+/P4YNG4akpCQsX74cUVFRYgL6PoXFHR0djWHDhmHkyJG4dOkS+vfvjz59+uDw4cMfbNPU1BSvX79GdHQ0BEF4b93x48dj7ty5OHv2LNTV1REYGChuK2oMISEh6NChAy5evIjAwMCPOh/Z2dl4+vSpZCEiIiIi+tLIhA/9BU70LwkICMCTJ08KvJe5W7duuHDhApKSkiCTyRAdHY327dtj6NChuHz5Mg4dOgSZTKa039t136datWoYMGAAhgwZAuDNaHPDhg3FUVxBEGBqaorQ0FAMGDAAK1asQHBwMFJTU2FkZKTUXrNmzdC0aVOMGzdOLPvf//6H0aNH4+7dux88FwXF7eHhAWdnZ8mIe9euXfH8+XPs3r37g22OHz8es2bNgr6+PurWrYsmTZrA398fFSpUAPDmQWqNGzfGoUOH0LRpUwDAnj170KpVK7x48QJaWlpFikEmk2H48OGYP3/+J52PkJAQhIaGKpW7fr8ManLtDx4v/ffEz/Yv6RCIiIjoP+Tp06cwMDBARkYG9PX1C63HkW76IgiCUGBSHRAQgISEBDg4OGDo0KE4cODAB9vKzMxEcHAwnJycYGhoCIVCgeTkZKWR7urVq4s/y2QymJqaig8dS0hIQM2aNQtMuIE3Dy2bMmUKFAqFuPTr1w/p6enIysoqzqGLkpOT4eHhISnz8PBAcnJykfYPCwvDvXv3sGzZMjg7O2PZsmVwdHTExYsXJfXePm4zMzMAEI+7qDG4ublJ1j/mfIwbNw4ZGRnicvv27SIdJxERERFRaaJe0gEQFUVycrLSvccAUKtWLdy8eRN79+7FoUOH0LVrVzRr1gybN28utK3g4GAcPHgQc+bMgZ2dHbS1tdG5c2e8evVKUk9DQ0OyLpPJxOnj2trvH2nNzMxEaGgoOnbsqLRNS0vrvfuqUrly5dClSxd06dIF06dPR82aNTFnzhysXr1arPP2ced/0fH20+OLQldXV7L+MedDLpfzgXlERERE9MVj0k2l3u+//46LFy9ixIgRBW7X19eHr68vfH190blzZ/j4+ODx48cwMjKChoYGcnNzJfXj4uIQEBCADh06AHiTEKamphYrpurVq2PlypViP++qVasWrly5Ajs7u2K1m6+guJ2cnBAXF4fevXuLZXFxcahatepH9aGpqQlbW9tiPb38Y2P41PNBRERERPSlYtJNpUp2djbu3buH3Nxc/P3339i3bx/Cw8PRunVr+Psr3685b948mJmZoWbNmihTpgw2bdoEU1NTGBoaAnhzb3ZMTAw8PDwgl8tRtmxZ2NvbY+vWrWjTpg1kMhkmTpxY7JHc7t27Y/r06Wjfvj3Cw8NhZmaG8+fPw9zcHA0aNMCkSZPQunVrWFlZoXPnzihTpgwSExNx6dIlTJs27YPtFxT3qFGj0LVrV9SsWRPNmjXDzp07sXXrVqUnrxdk165dWL9+Pbp164YqVapAEATs3LkTe/bsQWRkZJGP+2Nj+NTzQURERET0peI93VSq7Nu3D2ZmZrC2toaPjw8OHz6MRYsWYfv27VBTU1Oqr6enh1mzZsHNzQ116tRBamoq9uzZgzJl3ny0586di4MHD8LS0hI1a9YE8CZRL1u2LNzd3dGmTRt4e3ujVq1axYpTU1MTBw4cQPny5dGyZUu4uLhgxowZYoze3t7YtWsXDhw4gDp16qB+/fqYP38+KlWqVKT2C4q7ffv2WLhwIebMmQNnZ2csX74ckZGR8PLy+mB7VatWhY6ODkaOHIkaNWqgfv362LhxI1auXIlevXoV+bg/NoZPPR9ERERERF8qPr2ciL4I+U+H5NPLqTB8ejkRERH9m/j0ciIiIiIiIqISxqSb6F+2du1ayauz3l6cnZ1LTZtERERERPTp+CA1on9Z27ZtUa9evQK3vfuaspJsk4iIiIiIPh2TbqJ/mZ6eHvT09Ep9m0RERERE9Ok4vZyIiIiIiIhIRZh0ExEREREREakIp5cT0Rfl6LTu730lAxERERFRacKRbiIiIiIiIiIVYdJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCV4YR0Rel0YTfoCbXLukw6F8SP9u/pEMgIiIi+iQc6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVIRJN321UlNTIZPJkJCQUNKhEBERERHRfxSTbipR9+7dw/fff4/KlStDLpfD0tISbdq0QUxMTEmHphJRUVGQyWRKy8qVK0s6NCIiIiIiUgH1kg6A/rtSU1Ph4eEBQ0NDzJ49Gy4uLsjJycH+/fsxePBg/PnnnyUdokro6+vjypUrkjIDAwOleq9evYKmpua/FRYREREREakAR7qpxAwaNAgymQynT59Gp06dUKVKFTg7O+OHH37AqVOnEBgYiNatW0v2ycnJQfny5bFq1SoAQF5eHmbNmgU7OzvI5XJYWVkhLCys0D6PHDmCunXrQi6Xw8zMDGPHjsXr16/F7V5eXhg6dChGjx4NIyMjmJqaIiQkRNLGkydP0LdvX5iYmEBfXx9NmjRBYmJikY9bJpPB1NRUsmhrayMkJAQ1atTAypUrYWNjAy0trSL3N2PGDFSoUAF6enoICgrC2LFjUaNGDclxDR8+XLJP+/btERAQIK5nZ2cjODgYFStWhK6uLurVq4fY2Fhxe1RUFAwNDbF//344OTlBoVDAx8cH6enpknYjIiLg7OwsnuMhQ4YAQJGuJxERERHR14ZJN5WIx48fY9++fRg8eDB0dXWVthsaGqJv377Yt2+fJKnbtWsXsrKy4OvrCwAYN24cZsyYgYkTJyIpKQnr1q1DhQoVCuzzzp07aNmyJerUqYPExEQsXboUq1atwrRp0yT1Vq9eDV1dXfzxxx+YNWsWpkyZgoMHD4rbu3Tpgvv372Pv3r2Ij49HrVq10LRpUzx+/PiTz8v169exZcsWbN26VbwX/UP9bdy4ESEhIZg+fTrOnj0LMzMzLFmypNh9DxkyBCdPnsT69etx4cIFdOnSBT4+Prh27ZpYJysrC3PmzMGaNWtw9OhRpKWlITg4WNy+dOlSDB48GN999x0uXryIHTt2wM7ODgCKdD2JiIiIiL42nF5OJeL69esQBAGOjo6F1nF3d4eDgwPWrFmD0aNHAwAiIyPRpUsXKBQKPHv2DAsXLsTPP/+M3r17AwBsbW3xzTffFNjekiVLYGlpiZ9//hkymQyOjo64e/cuxowZg0mTJqFMmTffQVWvXh2TJ08GANjb2+Pnn39GTEwMmjdvjuPHj+P06dO4f/8+5HI5AGDOnDnYtm0bNm/ejO++++6Dx56RkQGFQiGuKxQK3Lt3D8CbKeW//vorTExMAKBI/S1YsABBQUEICgoCAEybNg2HDh3Cy5cvPxhLvrS0NERGRiItLQ3m5uYAgODgYOzbtw+RkZGYPn06gDcj08uWLYOtrS2AN4n6lClTxHamTZuGkSNHYtiwYWJZnTp1AHz4er4rOzsb2dnZ4vrTp0+LfDxERERERKUFR7qpRAiCUKR6ffv2RWRkJADg77//xt69exEYGAgASE5ORnZ2Npo2bVqktpKTk9GgQQPIZDKxzMPDA5mZmfjrr7/EsurVq0v2MzMzw/379wEAiYmJyMzMRLly5aBQKMTl5s2bSElJKVIcenp6SEhIEJcTJ06I2ypVqiQm3EXtLzk5GfXq1ZP00aBBgyLFku/ixYvIzc1FlSpVJP0cOXJEclw6Ojpiwv3uubl//z7u3r373uvxvuv5rvDwcBgYGIiLpaVlsY6JiIiIiKg04Eg3lQh7e3vIZLIPPizN398fY8eOxcmTJ3HixAnY2NigYcOGAABtbW2VxKahoSFZl8lkyMvLAwBkZmbCzMxMcq9zPkNDwyK1X6ZMGXHK9bvenWr/OfrL7/PdLzpycnIk/aipqSE+Ph5qamqSem+PQhd0bvLbLcr1eN/1fNe4cePwww8/iOtPnz5l4k1EREREXxwm3VQijIyM4O3tjcWLF2Po0KFKyeaTJ09gaGiIcuXKoX379oiMjMTJkyfRp08fsY69vT20tbURExODvn37frBPJycnbNmyBYIgiKPdcXFx0NPTg4WFRZHirlWrFu7duwd1dXVYW1sX/YA/UlH6c3Jywh9//AF/f3+x7NSpU5I6JiYmknupc3NzcenSJTRu3BgAULNmTeTm5uL+/fuFJsEfoqenB2tra8TExIjtvut91/NdcrlcnFJPRERERPSl4vRyKjGLFy9Gbm4u6tatiy1btuDatWtITk7GokWLJNOj+/bti9WrVyM5OVm8dxsAtLS0MGbMGIwePRq//vorUlJScOrUqUKfhD1o0CDcvn0b33//Pf78809s374dkydPxg8//CDez/0hzZo1Q4MGDdC+fXscOHAAqampOHHiBMaPH4+zZ89+2gn5yP6GDRuGiIgIREZG4urVq5g8eTIuX74saadJkybYvXs3du/ejT///BMDBw7EkydPxO1VqlSBn58f/P39sXXrVty8eROnT59GeHg4du/eXeR4Q0JCMHfuXCxatAjXrl3DuXPn8NNPP0nqFHY9iYiIiIi+RhzpphJTuXJlnDt3DmFhYRg5ciTS09NhYmKC2rVrY+nSpWK9Zs2awczMDM7OzuJDvvJNnDgR6urqmDRpEu7evQszMzMMGDCgwP4qVqyIPXv2YNSoUXB1dYWRkRGCgoIwYcKEIscsk8mwZ88ejB8/Hn369MGDBw9gamqKRo0aFfrU9E9RlP58fX2RkpKC0aNH4+XLl+jUqRMGDhyI/fv3i+0EBgYiMTER/v7+UFdXx4gRI5RGoyMjI8UHod25cwfGxsaoX7++0mu+3qd37954+fIl5s+fj+DgYBgbG6Nz586SOu+7nkREREREXxuZUNQnWhGVkMzMTFSsWBGRkZHo2LFjSYfzRQgJCcG2bdvE146VJh97PZ8+fQoDAwO4fr8ManLV3M9PpU/8bP8PVyIiIiIqAfl/n2ZkZEBfX7/QehzpplIrLy8PDx8+xNy5c2FoaIi2bduWdEj0CXg9iYiIiOi/iEk3lVppaWmwsbGBhYUFoqKioK5e+j+uzs7OuHXrVoHbli9fDj8/v385otLjS7yeRERERESfitPLiT6jW7duSV7F9bYKFSpAT0/vX47o68Hp5f9NnF5OREREpRWnlxOVgEqVKpV0CEREREREVIrwlWFEREREREREKsKkm4iIiIiIiEhFmHQTERERERERqQjv6SaiL8rRad3f+6AKIiIiIqLShCPdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVISvDCOiL0qjCb9BTa5d0mGQisXP9i/pEIiIiIg+C450ExEREREREakIk24iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVIRJNxEREREREZGKMOkmIiIiIiIiUhEm3UREREREREQqwqT7P8Ta2hoLFiwocn0vLy8MHz5cZfGUtJCQENSoUeOLa/tTpKamQiaTISEhoVS0Q0RERET0tSty0i2Tyd67hISEqDDMklHcJFWVCkrijh07BkNDQwwfPhyCIHz2Prdu3YqpU6d+tvaOHDmCJk2awMjICDo6OrC3t0fv3r3x6tUrAEBUVBQMDQ0/W39fipCQEPH3SF1dHcbGxmjUqBEWLFiA7Ozsz9qXpaUl0tPTUa1atSLvExAQgPbt239yO0RERERE/0VFTrrT09PFZcGCBdDX15eUBQcHqzLOz0YQBLx+/fpf7TM/qfycdu/eDW9vb/zwww9YsGABZDLZZ+/DyMgIenp6n6WtpKQk+Pj4wM3NDUePHsXFixfx008/QVNTE7m5uZ+lj5KQk5PzWdpxdnZGeno60tLScPjwYXTp0gXh4eFwd3fHs2fPPksfAKCmpgZTU1Ooq6uXinaIiIiIiL52RU66TU1NxcXAwAAymUxStn79ejg5OUFLSwuOjo5YsmSJuG/+VNSNGzeiYcOG0NbWRp06dXD16lWcOXMGbm5uUCgUaNGiBR48eCDulz/CFhoaChMTE+jr62PAgAGSJDYvLw/h4eGwsbGBtrY2XF1dsXnzZnF7bGwsZDIZ9u7di9q1a0Mul+P48eNISUlBu3btUKFCBSgUCtSpUweHDh0S9/Py8sKtW7cwYsQIcRQSKHjEecGCBbC2tlaKOywsDObm5nBwcAAA3L59G127doWhoSGMjIzQrl07pKamFvUSiNatW4eOHTti1qxZmDRpklh+/Phx8fxaWlpi6NCheP78eYFt9OjRA76+vpKynJwcGBsb49dffxXPwdvTy62trTF9+nQEBgZCT08PVlZWWLFiRZFiPnDgAExNTTFr1ixUq1YNtra28PHxwS+//AJtbW3ExsaiT58+yMjIUJo9sWbNGri5uUFPTw+mpqbo0aMH7t+/L7adf41jYmLg5uYGHR0duLu748qVK5IYZsyYgQoVKkBPTw9BQUF4+fKlZPuZM2fQvHlzGBsbw8DAAJ6enjh37pykjkwmw9KlS9G2bVvo6uoiLCysSG1/iLq6OkxNTWFubg4XFxd8//33OHLkCC5duoSZM2eK9bKzsxEcHIyKFStCV1cX9erVQ2xsLADg6dOn0NbWxt69eyVtR0dHQ09PD1lZWUrTwnNzcxEUFCT+/jg4OGDhwoXiviEhIVi9ejW2b98uXpfY2NgCp5cfOXIEdevWhVwuh5mZGcaOHSv5gsvLywtDhw7F6NGjYWRkBFNT069yhgwRERER0ds+yz3da9euxaRJkxAWFobk5GRMnz4dEydOxOrVqyX1Jk+ejAkTJuDcuXNQV1dHjx49MHr0aCxcuBDHjh3D9evXJUkkAMTExCA5ORmxsbH47bffsHXrVoSGhorbw8PD8euvv2LZsmW4fPkyRowYgZ49e+LIkSOSdsaOHYsZM2YgOTkZ1atXR2ZmJlq2bImYmBicP38ePj4+aNOmDdLS0gC8mVptYWGBKVOmiKP5xRETE4MrV67g4MGD2LVrF3JycuDt7Q09PT0cO3YMcXFxUCgU8PHxKdZI+OLFi9GnTx9ERERgyJAhYnlKSgp8fHzQqVMnXLhwARs2bMDx48cldd7m5+eHnTt3IjMzUyzbv38/srKy0KFDh0L7nzt3Ltzc3HD+/HkMGjQIAwcOVEpuC2Jqaor09HQcPXq0wO3u7u5KMyjyZ0/k5ORg6tSpSExMxLZt25CamoqAgAClNsaPH4+5c+fi7NmzUFdXR2BgoLht48aNCAkJwfTp03H27FmYmZlJvhgCgGfPnqF37944fvw4Tp06BXt7e7Rs2VJppDkkJAQdOnTAxYsXERgYWKS2P4ajoyNatGiBrVu3imVDhgzByZMnsX79ely4cAFdunSBj48Prl27Bn19fbRu3Rrr1q2TtLN27Vq0b98eOjo6Sn3k5eXBwsICmzZtQlJSEiZNmoQff/wRGzduBAAEBweja9eu8PHxEa+Lu7u7Ujt37txBy5YtUadOHSQmJmLp0qVYtWoVpk2bJqm3evVq6Orq4o8//sCsWbMwZcoUHDx4sMDjz87OxtOnTyULEREREdGX5rPMDZ08eTLmzp2Ljh07AgBsbGyQlJSE5cuXo3fv3mK94OBgeHt7AwCGDRuG7t27IyYmBh4eHgCAoKAgREVFSdrW1NREREQEdHR04OzsjClTpmDUqFGYOnUqcnJyMH36dBw6dAgNGjQAAFSuXBnHjx/H8uXL4enpKbYzZcoUNG/eXFw3MjKCq6uruD516lRER0djx44dGDJkCIyMjKCmpiaOrhaXrq4uVq5cCU1NTQDA//73P+Tl5WHlypXiqHlkZCQMDQ0RGxuLb7/99oNtJicnY8iQIVi1ahX8/Pwk28LDw+Hn5yeOTNvb22PRokXw9PTE0qVLoaWlJanv7e0NXV1dREdHo1evXgDejKC3bdv2vVPKW7ZsiUGDBgEAxowZg/nz5+Pw4cPiaH5hunTpgv3798PT0xOmpqaoX78+mjZtCn9/f+jr60NTU1Myg+JtbyfPlStXxqJFi1CnTh1kZmZCoVCI28LCwsRrPnbsWLRq1QovX76ElpYWFixYgKCgIAQFBQEApk2bhkOHDklGpJs0aSLpd8WKFTA0NMSRI0fQunVrsbxHjx7o06ePuN6tW7cPtv2xHB0dceDAAQBAWloaIiMjkZaWBnNzcwBvfqf27duHyMhITJ8+HX5+fujVqxeysrKgo6ODp0+fYvfu3YiOji6wfQ0NDcmXWDY2Njh58iQ2btyIrl27QqFQQFtbG9nZ2e/9PViyZAksLS3x888/QyaTwdHREXfv3sWYMWMwadIklCnz5vu96tWrY/LkyQDefEZ//vlnxMTESH4384WHh0tiIyIiIiL6En3ySPfz58+RkpKCoKAgKBQKcZk2bRpSUlIkdatXry7+XKFCBQCAi4uLpOztacMA4OrqKhmha9CgATIzM3H79m1cv34dWVlZaN68uaTvX3/9ValvNzc3yXpmZiaCg4Ph5OQEQ0NDKBQKJCcniyPdn8rFxUVMuAEgMTER169fh56enhinkZERXr58qRRrYSwsLFCrVi3Mnj1baeQ9MTERUVFRkvPg7e2NvLw83Lx5U6ktdXV1dO3aFWvXrgXw5jpu375dKZl/19vXMD9BfveaFURNTQ2RkZH466+/MGvWLFSsWBHTp08X72V+n/j4eLRp0wZWVlbQ09MTE+t3r9XbsZmZmQGAGFtycjLq1asnqZ//RU2+v//+G/369YO9vT0MDAygr6+PzMxMpX7e/SwVpe2PJQiC+CXNxYsXkZubiypVqkiu85EjR8TPUMuWLaGhoYEdO3YAALZs2QJ9fX00a9as0D4WL16M2rVrw8TEBAqFAitWrCj270FycjIaNGggebaAh4cHMjMz8ddff4llb18j4M11KuzzM27cOGRkZIjL7du3ixUTEREREVFp8Mkj3fnTk3/55RelxENNTU2yrqGhIf6c/8f5u2V5eXnF7nv37t2oWLGiZJtcLpes6+rqStaDg4Nx8OBBzJkzB3Z2dtDW1kbnzp0/ONW7TJkySk8KL+hhWu/2l5mZidq1a4tJ7ttMTEze22c+PT09HDp0CM2bN0fjxo1x+PBhMbnMzMxE//79MXToUKX9rKysCmzPz88Pnp6euH//Pg4ePAhtbW34+Pi8N4a3rxdQ/GtWsWJF9OrVC7169cLUqVNRpUoVLFu2rNARzefPn8Pb2xve3t5Yu3YtTExMkJaWBm9vb6VrVdDnqzix9e7dG48ePcLChQtRqVIlyOVyNGjQQKmfd6+tKiUnJ8PGxgbAm2uspqaG+Ph4pd+t/BF/TU1NdO7cGevWrUO3bt2wbt06+Pr6FvrAs/Xr1yM4OBhz585FgwYNoKenh9mzZ+OPP/5QyfEU5/Mjl8uVfo+JiIiIiL40n5x0V6hQAebm5rhx48YHR0k/RmJiIl68eAFtbW0AwKlTp6BQKGBpaQkjIyPI5XKkpaVJppIXRVxcHAICAsT7lzMzM5UealbQk7VNTExw7949yQhkUd5VXKtWLWzYsAHly5eHvr5+sWJ9W9myZXHo0CF8++238PLywuHDh2Fubo5atWohKSkJdnZ2RW7L3d0dlpaW2LBhA/bu3YsuXbooJUWqVLZsWZiZmYkPeyvofP/555949OgRZsyYAUtLSwDA2bNni92Xk5MT/vjjD/j7+4tlp06dktSJi4vDkiVL0LJlSwBvHnz38OHDz9L2x/jzzz+xb98+jBs3DgBQs2ZN5Obm4v79+2jYsGGh+/n5+aF58+a4fPkyfv/9d6X7qt8WFxcHd3d38ZYBAEozL4ryhHknJyds2bJF8nsRFxcHPT09WFhYfPBYiYiIiIi+Vp/lQWqhoaEIDw/HokWLcPXqVVy8eBGRkZGYN2/eJ7f96tUrBAUFISkpCXv27MHkyZMxZMgQlClTBnp6eggODsaIESOwevVqpKSk4Ny5c/jpp5+UHuL2Lnt7e2zduhUJCQlITExEjx49lEbcrK2tcfToUdy5c0dMvry8vPDgwQPMmjULKSkpWLx4sdLTogvi5+cHY2NjtGvXDseOHcPNmzcRGxuLoUOHSqbfFoWhoSEOHjyIsmXLwsvLS7x39sSJExgyZAgSEhJw7do1bN++vdAHqeXr0aMHli1bhoMHD6rkS5N8y5cvx8CBA3HgwAGkpKTg8uXLGDNmDC5fvow2bdoAeHO+MzMzERMTg4cPHyIrKwtWVlbQ1NTETz/9hBs3bmDHjh0f9e7wYcOGISIiApGRkbh69SomT56My5cvS+rY29tjzZo1SE5Oxh9//AE/Pz/xy55PbftDXr9+jXv37uHu3bvi69Q8PT1Ro0YNjBo1CgBQpUoV+Pn5wd/fH1u3bsXNmzdx+vRphIeHY/fu3WJbjRo1gqmpKfz8/GBjY6M0A+XdYz579iz279+Pq1evYuLEiThz5oykjrW1NS5cuIArV67g4cOHBc7sGDRoEG7fvo3vv/8ef/75J7Zv347Jkyfjhx9+EO/nJiIiIiL6L/osfw337dsXK1euRGRkJFxcXODp6YmoqChxWuynaNq0Kezt7dGoUSP4+vqibdu2ktcMTZ06FRMnTkR4eDicnJzg4+OD3bt3f7DvefPmoWzZsnB3d0ebNm3g7e2NWrVqSepMmTIFqampsLW1FaeAOzk5YcmSJVi8eDFcXV1x+vTpIr2jXEdHB0ePHoWVlRU6duwIJycn8dVSHzPybWBggAMHDsDY2Bienp4oV64cjhw5gqtXr6Jhw4aoWbMmJk2aJD5wqzB+fn5ISkpCxYoVxQfaqULdunWRmZmJAQMGwNnZGZ6enjh16hS2bdsmzlJwd3fHgAED4OvrCxMTE8yaNQsmJiaIiorCpk2bULVqVcyYMQNz5swpdv++vr6YOHEiRo8ejdq1a+PWrVsYOHCgpM6qVavwzz//oFatWujVqxeGDh2K8uXLf5a2P+Ty5cswMzODlZUVvLy8sHHjRowbNw7Hjh2TPCwuMjIS/v7+GDlyJBwcHNC+fXucOXNGcguBTCZD9+7dkZiY+MEvUvr374+OHTvC19cX9erVw6NHjySj3gDQr18/ODg4wM3NDSYmJoiLi1Nqp2LFitizZw9Onz4NV1dXDBgwAEFBQZgwYUKxzgMRERER0ddGJrx7g3IpEhAQgCdPnmDbtm0lHQoRlbCnT5/CwMAArt8vg5r8wzMQ6MsWP9v/w5WIiIiISlD+36cZGRnvHUjlvE8iIiIiIiIiFWHSXUq8/Qqod5djx46VdHgfNH369ELjb9GiRUmHV6K+9GtLREREREQf75OfXq5KUVFRJR3Cv+Z9T0B/93VopdGAAQPQtWvXArcV5WFkX7Mv/doSEREREdHHK9VJ939JcV71VRoZGRnByMiopMMolb70a0tERERERB+P08uJiIiIiIiIVIRJNxEREREREZGKMOkmIiIiIiIiUhHe001EX5Sj07q/9z2IRERERESlCUe6iYiIiIiIiFSESTcRERERERGRijDpJiIiIiIiIlIRJt1EREREREREKsKkm4iIiIiIiEhF+PRyIvqiNJrwG9Tk2iUdBqlQ/Gz/kg6BiIiI6LPhSDcRERERERGRijDpJiIiIiIiIlIRJt1EREREREREKsKkm4iIiIiIiEhFmHQTERERERERqQiTbiIiIiIiIiIVYdJNREREREREpCJMuomIiIiIiIhUhEk30b9EJpO9dwkJCQEAREdHo379+jAwMICenh6cnZ0xfPjwAtv09vaGmpoazpw5U6xYAgICIJPJMGDAAKVtgwcPhkwmQ0BAQDGPkIiIiIiI3sWkm+hfkp6eLi4LFiyAvr6+pCw4OBgxMTHw9fVFp06dcPr0acTHxyMsLAw5OTlK7aWlpeHEiRMYMmQIIiIiih2PpaUl1q9fjxcvXohlL1++xLp162BlZfVJx0pERERERG8w6Sb6l5iamoqLgYEBZDKZpEyhUGDnzp3w8PDAqFGj4ODggCpVqqB9+/ZYvHixUnuRkZFo3bo1Bg4ciN9++02SPBdFrVq1YGlpia1bt4plW7duhZWVFWrWrCmpm5eXh/DwcNjY2EBbWxuurq7YvHmzuD03NxdBQUHidgcHByxcuFDSRkBAANq3b485c+bAzMwM5cqVw+DBgwv8QoGIiIiI6GvBpJuoFDE1NcXly5dx6dKl99YTBAGRkZHo2bMnHB0dYWdnJ0mCiyowMBCRkZHiekREBPr06aNULzw8HL/++iuWLVuGy5cvY8SIEejZsyeOHDkC4E1SbmFhgU2bNiEpKQmTJk3Cjz/+iI0bN0raOXz4MFJSUnD48GGsXr0aUVFRiIqKKnbcRERERERfCibdRKXI999/jzp16sDFxQXW1tbo1q0bIiIikJ2dLal36NAhZGVlwdvbGwDQs2dPrFq1qtj99ezZE8ePH8etW7dw69YtxMXFoWfPnpI62dnZmD59OiIiIuDt7Y3KlSsjICAAPXv2xPLlywEAGhoaCA0NhZubG2xsbODn54c+ffooJd1ly5bFzz//DEdHR7Ru3RqtWrVCTExMgbFlZ2fj6dOnkoWIiIiI6EvDpJuoFNHV1cXu3btx/fp1TJgwAQqFAiNHjkTdunWRlZUl1ouIiICvry/U1dUBAN27d0dcXBxSUlKK1Z+JiQlatWqFqKgoREZGolWrVjA2NpbUuX79OrKystC8eXMoFApx+fXXXyX9LV68GLVr14aJiQkUCgVWrFiBtLQ0SVvOzs5QU1MT183MzHD//v0CYwsPD4eBgYG4WFpaFuvYiIiIiIhKAybdRKWQra0t+vbti5UrV+LcuXNISkrChg0bAACPHz9GdHQ0lixZAnV1dairq6NixYp4/fr1Rz1QLTAwEFFRUVi9ejUCAwOVtmdmZgIAdu/ejYSEBHFJSkoSp7SvX78ewcHBCAoKwoEDB5CQkIA+ffrg1atXkrY0NDQk6zKZDHl5eQXGNW7cOGRkZIjL7du3i31sREREREQlTb2kAyCi97O2toaOjg6eP38OAFi7di0sLCywbds2Sb0DBw5g7ty5mDJlimQ0+UN8fHzw6tUryGQycbr626pWrQq5XI60tDR4enoW2EZcXBzc3d0xaNAgsay4o+7vksvlkMvln9QGEREREVFJY9JNVIqEhIQgKysLLVu2RKVKlfDkyRMsWrQIOTk5aN68OQBg1apV6Ny5M6pVqybZ19LSEuPGjcO+ffvQqlWrIveppqaG5ORk8ed36enpITg4GCNGjEBeXh6++eYbZGRkIC4uDvr6+ujduzfs7e3x66+/Yv/+/bCxscGaNWtw5swZ2NjYfMLZICIiIiL68nF6OVEp4unpiRs3bsDf3x+Ojo5o0aIF7t27hwMHDsDBwQHx8fFITExEp06dlPY1MDBA06ZNP+qBavr6+tDX1y90+9SpUzFx4kSEh4fDyckJPj4+2L17t5hU9+/fHx07doSvry/q1auHR48eSUa9iYiIiIj+q2SCIAglHQQR0Yc8ffoUBgYGcP1+GdTk2iUdDqlQ/Gz/kg6BiIiI6IPy/z7NyMh47wAWR7qJiIiIiIiIVIRJN9FXJi0tTfJqr3eXd1/jRUREREREqsMHqRF9ZczNzZGQkPDe7URERERE9O9g0k30lVFXV4ednV1Jh0FEREREROD0ciIiIiIiIiKVYdJNREREREREpCJMuomIiIiIiIhUhPd0E9EX5ei07u99DyIRERERUWnCkW4iIiIiIiIiFWHSTURERERERKQiTLqJiIiIiIiIVIRJNxEREREREZGKMOkmIiIiIiIiUhEm3UREREREREQqwleGEdEXpdGE36Am1y7pMOgjxM/2L+kQiIiIiP51HOkmIiIiIiIiUhEm3UREREREREQqwqSbiIiIiIiISEWYdBMRERERERGpCJNuIiIiIiIiIhVh0k1ERERERESkIky6iYiIiIiIiFSESTf9Z6WmpkImkyEhIeGLavtTeXl5Yfjw4aWmHSIiIiKirxmT7q+cTCZ77xISEgIAiI6ORv369WFgYAA9PT04OzsXmlB5e3tDTU0NZ86cKVYsDx48wMCBA2FlZQW5XA5TU1N4e3sjLi5OEu+2bds+8mi/TPkJev6Sf/4HDx6Ma9euffb+tm7diqlTpxa5fmxsLGQyGZ48efJJ7RARERER/Repl3QApFrp6enizxs2bMCkSZNw5coVsUyhUCAmJga+vr4ICwtD27ZtIZPJkJSUhIMHDyq1l5aWhhMnTmDIkCGIiIhAnTp1ihxLp06d8OrVK6xevRqVK1fG33//jZiYGDx69OjTDrIEvXr1Cpqamp+lrUOHDsHZ2RlZWVm4ePEiFi5cCFdXV+zcuRNNmzb9LH0AgJGRUalqh4iIiIjoa8aR7q+cqampuBgYGEAmk0nKFAoFdu7cCQ8PD4waNQoODg6oUqUK2rdvj8WLFyu1FxkZidatW2PgwIH47bff8OLFiyLF8eTJExw7dgwzZ85E48aNUalSJdStWxfjxo1D27ZtAQDW1tYAgA4dOkAmk4nrKSkpaNeuHSpUqACFQoE6derg0KFDkvatra0xffp0BAYGQk9PD1ZWVlixYoWkzunTp1GzZk1oaWnBzc0N58+fl2zPzc1FUFAQbGxsoK2tDQcHByxcuFBSJyAgAO3bt0dYWBjMzc3h4OBQpLaLoly5cjA1NUXlypXRrl07HDp0CPXq1UNQUBByc3PFetu3b0etWrWgpaWFypUrIzQ0FK9fvwYA9OjRA76+vpJ2c3JyYGxsjF9//RWA8rTwNWvWwM3NDXp6ejA1NUWPHj1w//59AG9G4Rs3bgwAKFu2LGQyGQICAgps559//oG/vz/Kli0LHR0dtGjRQjJSHxUVBUNDQ+zfvx9OTk5QKBTw8fGRfDFERERERPS1YdJNMDU1xeXLl3Hp0qX31hMEAZGRkejZsyccHR1hZ2eHzZs3F6kPhUIBhUKBbdu2ITs7u8A6+dPVIyMjkZ6eLq5nZmaiZcuWiImJwfnz5+Hj44M2bdogLS1Nsv/cuXPFhHfQoEEYOHCgOKqfmZmJ1q1bo2rVqoiPj0dISAiCg4Ml++fl5cHCwgKbNm1CUlISJk2ahB9//BEbN26U1IuJicGVK1dw8OBB7Nq1q0htf4wyZcpg2LBhuHXrFuLj4wEAx44dg7+/P4YNG4akpCQsX74cUVFRCAsLAwD4+flh586dyMzMFNvZv38/srKy0KFDhwL7ycnJwdSpU5GYmIht27YhNTVVTKwtLS2xZcsWAMCVK1eQnp6u9EVEvoCAAJw9exY7duzAyZMnIQgCWrZsiZycHLFOVlYW5syZgzVr1uDo0aNIS0v7LOeKiIiIiKi0YtJN+P7771GnTh24uLjA2toa3bp1Q0REhFJyfOjQIWRlZcHb2xsA0LNnT6xatapIfairqyMqKgqrV6+GoaEhPDw88OOPP+LChQtiHRMTEwCAoaEhTE1NxXVXV1f0798f1apVg729PaZOnQpbW1vs2LFD0kfLli0xaNAg2NnZYcyYMTA2Nsbhw4cBAOvWrUNeXh5WrVoFZ2dntG7dGqNGjZLsr6GhgdDQULi5ucHGxgZ+fn7o06ePUtKtq6uLlStXwtnZGc7OzkVq+2M5OjoCeDPiDAChoaEYO3YsevfujcqVK6N58+aYOnUqli9fDuDN/fa6urqIjo4W21i3bh3atm0LPT29AvsIDAxEixYtULlyZdSvXx+LFi3C3r17kZmZCTU1NXEaefny5cUZE++6du0aduzYgZUrV6Jhw4ZwdXXF2rVrcefOHck9+jk5OVi2bBnc3NxQq1YtDBkyBDExMQXGlZ2djadPn0oWIiIiIqIvDZNugq6uLnbv3o3r169jwoQJUCgUGDlyJOrWrYusrCyxXkREBHx9faGu/uZRAN27d0dcXBxSUlKK1E+nTp1w9+5d7NixAz4+PoiNjUWtWrUQFRX13v0yMzMRHBwMJycnGBoaQqFQIDk5WWmku3r16uLP+dPo86dJJycno3r16tDS0hLrNGjQQKmvxYsXo3bt2jAxMYFCocCKFSuU+nFxcZHcx13Utj+GIAji8QBAYmIipkyZIs4cUCgU6NevH9LT05GVlQV1dXV07doVa9euBQA8f/4c27dvh5+fX6F9xMfHo02bNrCysoKenh48PT0BQOm43yc5ORnq6uqoV6+eWFauXDk4ODggOTlZLNPR0YGtra24bmZmJl6jd4WHh8PAwEBcLC0tixwPEREREVFpwaSbRLa2tujbty9WrlyJc+fOISkpCRs2bAAAPH78GNHR0ViyZAnU1dWhrq6OihUr4vXr14iIiChyH1paWmjevDkmTpyIEydOICAgAJMnT37vPsHBwYiOjsb06dNx7NgxJCQkwMXFBa9evZLU09DQkKzLZDLk5eUVObb169cjODgYQUFBOHDgABISEtCnTx+lfnR1dYvc5qfKT1htbGwAvPkCIjQ0FAkJCeJy8eJFXLt2TUz6/fz8EBMTg/v372Pbtm3Q1taGj49Pge0/f/4c3t7e0NfXx9q1a3HmzBlxlPzd4/4cCrpG+V8svGvcuHHIyMgQl9u3b3/2eIiIiIiIVI1PL6cCWVtbQ0dHB8+fPwcArF27FhYWFkqv8zpw4ADmzp2LKVOmQE1Nrdj9VK1aVdKmhoaG5KFhABAXF4eAgADxnuTMzExxunVROTk5Yc2aNXj58qWYnJ46dUqpH3d3dwwaNEgsK8ooflHa/hh5eXlYtGgRbGxsULNmTQBArVq1cOXKFdjZ2RW6n7u7OywtLbFhwwbs3bsXXbp0UUp28/3555949OgRZsyYIY4knz17VlInf1T/3evyNicnJ7x+/Rp//PEH3N3dAQCPHj3ClStXULVq1aIf9FvkcjnkcvlH7UtEREREVFpwpJsQEhKC0aNHIzY2Fjdv3sT58+cRGBiInJwcNG/eHACwatUqdO7cGdWqVZMsQUFBePjwIfbt2/fePh49eoQmTZrgf//7Hy5cuICbN29i06ZNmDVrFtq1ayfWs7a2RkxMDO7du4d//vkHAGBvb4+tW7ciISEBiYmJ6NGjR7FGsIE3T/WWyWTo168fkpKSsGfPHsyZM0dSx97eHmfPnsX+/ftx9epVTJw4sUjvIi9K20Xx6NEj3Lt3Dzdu3MCOHTvQrFkznD59GqtWrRK/0Jg0aRJ+/fVXhIaG4vLly0hOTsb69esxYcIEpZiWLVuGgwcPvndquZWVFTQ1NfHTTz+J/b777u1KlSpBJpNh165dePDggeQhbfns7e3Rrl079OvXD8ePH0diYiJ69uyJihUrSq4vEREREdF/DZNugqenJ27cuAF/f384OjqiRYsWuHfvHg4cOAAHBwfEx8cjMTERnTp1UtrXwMAATZs2/eAD1RQKBerVq4f58+ejUaNGqFatGiZOnIh+/frh559/FuvNnTsXBw8ehKWlpTi6O2/ePJQtWxbu7u5o06YNvL29UatWrWIdY/6r0S5evIiaNWti/PjxmDlzpqRO//790bFjR/j6+qJevXp49OiRZNT7U9ouimbNmsHMzAwuLi4YO3YsnJyccOHCBfGVXcCbB6Xt2rULBw4cQJ06dVC/fn3Mnz8flSpVkrTl5+eHpKQkVKxYER4eHoX2aWJigqioKGzatAlVq1bFjBkzlL4wqFixovgAtwoVKmDIkCEFthUZGYnatWujdevWaNCgAQRBwJ49ewodZSciIiIi+i+QCYXdUElEVIo8ffoUBgYGcP1+GdTk2iUdDn2E+Nn+JR0CERER0WeT//dpRkYG9PX1C63HkW4iIiIiIiIiFWHSTZ9FWlqa5DVW7y7Fef3U12bAgAGFnpcBAwaUdHhERERERKRCfHo5fRbm5uZISEh47/b/qilTpiA4OLjAbe+bhkJERERERF8+Jt30Wairq7/3NVb/ZeXLl0f58uVLOgwiIiIiIioBnF5OREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYrwnm4i+qIcndadD6AjIiIioi8GR7qJiIiIiIiIVIRJNxEREREREZGKMOkmIiIiIiIiUhEm3UREREREREQqwqSbiIiIiIiISEWYdBMRERERERGpCF8ZRkRflEYTfoOaXLukw/hPi5/tX9IhEBEREX0xONJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbdRERERERERCrCpJuIiIiIiIhIRZh0ExEREREREakIk276z4qNjYVMJsOTJ0++qLY/lbW1NRYsWFBq2iEiIiIi+pox6f5IMpnsvUtISAgAIDo6GvXr14eBgQH09PTg7OyM4cOHF9imt7c31NTUcObMmWLFEhAQIParoaGBChUqoHnz5oiIiEBeXp6kbmGJUkhICGrUqAEA2LdvH2QyGe7duyepY2ZmBmtra0lZamoqZDIZYmJiPhjnzZs30aNHD5ibm0NLSwsWFhZo164d/vzzT0lbCQkJRT72r0F+gi6TyVCmTBkYGBigZs2aGD16NNLT0z97f2fOnMF3331X5PpRUVEwNDT85HaIiIiIiP6LmHR/pPT0dHFZsGAB9PX1JWXBwcGIiYmBr68vOnXqhNOnTyM+Ph5hYWHIyclRai8tLQ0nTpzAkCFDEBERUex4fHx8kJ6ejtTUVOzduxeNGzfGsGHD0Lp1a7x+/bpYbX3zzTdQV1dHbGysWJacnIwXL17gn3/+QWpqqlh++PBhyOVyeHh4vLfNnJwcNG/eHBkZGdi6dSuuXLmCDRs2wMXFpVSOBhfVq1evPltbV65cwd27d3HmzBmMGTMGhw4dQrVq1XDx4sXP1gcAmJiYQEdHp9S0Q0RERET0NWPS/ZFMTU3FxcDAADKZTFKmUCiwc+dOeHh4YNSoUXBwcECVKlXQvn17LF68WKm9yMhItG7dGgMHDsRvv/2GFy9eFCseuVwOU1NTVKxYEbVq1cKPP/6I7du3Y+/evYiKiipWWwqFAnXq1JEk3bGxsfjmm2/g4eGhVF6/fn1oaWm9t83Lly8jJSUFS5YsQf369VGpUiV4eHhg2rRpqF+/PgDAxsYGAFCzZk3IZDJ4eXkBeDOi2rx5cxgbG8PAwACenp44d+6cpH2ZTIaVK1eiQ4cO0NHRgb29PXbs2CGps2fPHlSpUgXa2tpo3Lix5MsDAHj06BG6d++OihUrQkdHBy4uLvjtt98kdby8vDBkyBAMHz4cxsbG8Pb2LlLbRVG+fHmYmpqiSpUq6NatG+Li4mBiYoKBAwdK6q1cuRJOTk7Q0tKCo6MjlixZIm5zd3fHmDFjJPUfPHgADQ0NHD16FIDybId58+bBxcUFurq6sLS0xKBBg5CZmQngzfXt06cPMjIylGZxvNtOWloa2rVrB4VCAX19fXTt2hV///23uD1/NsWaNWtgbW0NAwMDdOvWDc+ePSv2uSIiIiIi+lIw6VYhU1NTXL58GZcuXXpvPUEQEBkZiZ49e8LR0RF2dnbYvHnzJ/ffpEkTuLq6YuvWrcXet3Hjxjh8+LC4fvjwYXh5ecHT01NSHhsbi8aNG3+wPRMTE5QpUwabN29Gbm5ugXVOnz4NADh06BDS09PFuJ89e4bevXvj+PHjOHXqFOzt7dGyZUulZC00NBRdu3bFhQsX0LJlS/j5+eHx48cAgNu3b6Njx45o06YNEhIS0LdvX4wdO1ay/8uXL1G7dm3s3r0bly5dwnfffYdevXqJceVbvXo1NDU1ERcXh2XLlhWp7Y+hra2NAQMGIC4uDvfv3wcArF27FpMmTUJYWBiSk5Mxffp0TJw4EatXrwYA+Pn5Yf369RAEQWxnw4YNMDc3R8OGDQvsp0yZMli0aBEuX76M1atX4/fff8fo0aMBvEni353JERwcrNRGXl4e2rVrh8ePH+PIkSM4ePAgbty4AV9fX0m9lJQUbNu2Dbt27cKuXbtw5MgRzJgxo8C4srOz8fTpU8lCRERERPSlYdKtQt9//z3q1KkDFxcXWFtbo1u3boiIiEB2drak3qFDh5CVlSWOmvbs2ROrVq36LDE4OjoqjbqOGTMGCoVCskyfPl1Sp3Hjxrh69ap4T/GRI0fg6emJRo0a4ciRIwCAGzduIC0trUhJd8WKFbFo0SJMmjQJZcuWRZMmTTB16lTcuHFDrGNiYgIAKFeuHExNTWFkZATgzZcH+V9IODk5YcWKFcjKyhLjyBcQEIDu3bvDzs4O06dPR2ZmppgwL126FLa2tpg7dy4cHBzg5+eHgIAApRiDg4NRo0YNVK5cGd9//z18fHywceNGST17e3vMmjULDg4OcHBwKFLbH8vR0REAxGs4efJkzJ07Fx07doSNjQ06duyIESNGYPny5QCArl274u7duzh+/LjYxrp169C9e3fIZLIC+xg+fDgaN24Ma2trNGnSBNOmTROPWVNTU2kmh0KhUGojJiYGFy9exLp161C7dm3Uq1cPv/76K44cOSJ5RkFeXh6ioqJQrVo1NGzYEL169Sr0eQDh4eEwMDAQF0tLy+KfQCIiIiKiEsakW4V0dXWxe/duXL9+HRMmTIBCocDIkSNRt25dZGVlifUiIiLg6+sLdXV1AED37t0RFxeHlJSUT45BEASlZGvUqFFISEiQLAMGDJDUcXd3h6amJmJjY5GUlIQXL16gVq1acHNzw4MHD3Dz5k3ExsZCW1tbnB7+IYMHD8a9e/ewdu1aNGjQAJs2bYKzszMOHjz43v3+/vtv9OvXD/b29jAwMIC+vj4yMzORlpYmqVe9enXxZ11dXejr64sjxMnJyahXr56kfoMGDSTrubm5mDp1KlxcXGBkZASFQoH9+/cr9VO7dm3JelHa/lj5I9YymQzPnz9HSkoKgoKCJF+YTJs2TfysmJiY4Ntvv8XatWsBvHl43cmTJ+Hn51doH4cOHULTpk1RsWJF6OnpoVevXnj06JHkM/ohycnJsLS0lCTGVatWhaGhIZKTk8Uya2tr6OnpietmZmbiNXrXuHHjkJGRIS63b98ucjxERERERKUFk+5/ga2tLfr27YuVK1fi3LlzSEpKwoYNGwAAjx8/RnR0NJYsWQJ1dXWoq6ujYsWKeP369Uc9UO1dycnJ4r3S+YyNjWFnZydZ8keV8+no6KBu3bo4fPgwDh8+jG+++QZqamrQ0NCAu7u7WO7h4QFNTc0ix6Onp4c2bdogLCwMiYmJaNiwIaZNm/befXr37o2EhAQsXLgQJ06cQEJCAsqVK6f0EDMNDQ3JukwmU3p6+/vMnj0bCxcuxJgxY3D48GEkJCTA29tbqR9dXd0it/mp8hNWa2tr8T7rX375RfKFyaVLl3Dq1ClxHz8/P2zevBk5OTlYt24dXFxc4OLiUmD7qampaN26NapXr44tW7YgPj5efObA53xIXL7iXCO5XA59fX3JQkRERET0pWHS/S+ztraGjo4Onj9/DuDNPboWFhZITEyUJFJz585FVFRUofc/F8Xvv/+OixcvolOnTh+1f+PGjREbG4vY2FjxoWYA0KhRI8TGxuLIkSNFmlpeGJlMBkdHR/Fc5Cfv7x5zXFwchg4dipYtW8LZ2RlyuRwPHz4sVl9OTk5K92a/najm99OuXTv07NkTrq6uqFy5Mq5evfpZ2v4YL168wIoVK9CoUSOYmJigQoUKMDc3x40bN5S+NHn7i5V27drh5cuX2LdvH9atW/feUe74+Hjk5eVh7ty5qF+/PqpUqYK7d+9K6mhqan7wc+jk5ITbt29LRqOTkpLw5MkTVK1a9SPPABERERHRl49JtwqFhIRg9OjRiI2Nxc2bN3H+/HkEBgaKr88CgFWrVqFz586oVq2aZAkKCsLDhw+xb9++IvWVnZ2Ne/fu4c6dOzh37hymT5+Odu3aoXXr1vD39/+o+Bs3boxr165h//798PT0FMs9PT2xbds23L59u8hJd0JCAtq1a4fNmzcjKSkJ169fx6pVqxAREYF27doBePP0bm1tbezbtw9///03MjIyALy5h3rNmjVITk7GH3/8AT8/P2hraxfrWAYMGIBr165h1KhRuHLlCtatW6f0VHd7e3scPHgQJ06cQHJyMvr37y95+vantF0U9+/fx71793Dt2jWsX78eHh4eePjwIZYuXSrWCQ0NRXh4OBYtWoSrV6/i4sWLiIyMxLx588Q6urq6aN++PSZOnIjk5GR079690D7t7OyQk5ODn376CTdu3MCaNWuwbNkySZ38UfaYmBg8fPiwwGnnzZo1g4uLC/z8/HDu3DmcPn0a/v7+8PT0hJubW7HPBRERERHR14JJtwp5enrixo0b8Pf3h6OjI1q0aIF79+7hwIEDcHBwQHx8PBITEwsciTYwMEDTpk2L/EC1ffv2wczMDNbW1vDx8cHhw4exaNEibN++HWpqah8Vf4MGDSCXyyEIguQ+5nr16iEnJ0d8tVhRWFhYwNraGqGhoahXrx5q1aqFhQsXIjQ0FOPHjwcAqKurY9GiRVi+fDnMzc3FZHzVqlX4559/UKtWLfTq1QtDhw5F+fLli3UsVlZW2LJlC7Zt2wZXV1csW7ZM6eFxEyZMQK1ateDt7Q0vLy+Ympqiffv2n6XtonBwcIC5uTlq166NGTNmoFmzZrh06ZJkpDj/NoXIyEi4uLjA09MTUVFRSrcQ+Pn5idP3raysCu3T1dUV8+bNw8yZM1GtWjWsXbsW4eHhkjru7u4YMGAAfH19YWJiglmzZim1I5PJsH37dpQtWxaNGjVCs2bNULlyZfE2CiIiIiKi/yqZ8Pa7hYiISqmnT5/CwMAArt8vg5q8eDMd6POKn/1xs2eIiIiIvib5f59mZGS89/lDHOkmIiIiIiIiUhEm3aVcWlqa0ju1317efZ1VSTl27Nh74/wva9GiRaHn5WOmoRMRERER0ZdDvaQDoPczNzdHQkLCe7eXBm5ubu+N879s5cqVePHiRYHb3n1VGxERERERfV2YdJdy6urqsLOzK+kwPkhbW/uLiLMkVKxYsaRDICIiIiKiEsLp5UREREREREQqwqSbiIiIiIiISEWYdBMRERERERGpCO/pJqIvytFp3d/7HkQiIiIiotKEI91EREREREREKsKkm4iIiIiIiEhFmHQTERERERERqQiTbiIiIiIiIiIVYdJNREREREREpCJ8ejkRfVEaTfgNanLtkg7jqxY/27+kQyAiIiL6anCkm4iIiIiIiEhFmHQTERERERERqQiTbiIiIiIiIiIVYdJNREREREREpCJMuomIiIiIiIhUhEk3ERERERERkYow6SYiIiIiIiJSESbd9J8VFRUFQ0PDL67tTyWTybBt27ZS0w4RERER0dfsi0i6ZTLZe5eQkBAAQHR0NOrXrw8DAwPo6enB2dkZw4cPL7BNb29vqKmp4cyZM8WKJSAgoMAYrl+//olH+fVLTExE27ZtUb58eWhpacHa2hq+vr64f/8+ACA2NhYymQxPnjwp2UD/ZVFRUeLnSE1NDWXLlkW9evUwZcoUZGRkfPb+0tPT0aJFiyLXDwkJQY0aNT65HSIiIiKi/yL1kg6gKNLT08WfN2zYgEmTJuHKlStimUKhQExMDHx9fREWFoa2bdtCJpMhKSkJBw8eVGovLS0NJ06cwJAhQxAREYE6deoUKx4fHx9ERkZKykxMTJTqvXr1CpqamsVq+2v14MEDNG3aFK1bt8b+/fthaGiI1NRU7NixA8+fPy/p8D7a57rG+vr6uHLlCgRBwJMnT3DixAmEh4cjMjIScXFxMDc3/wzRvmFqalqq2iEiIiIi+pp9ESPdpqam4mJgYACZTCYpUygU2LlzJzw8PDBq1Cg4ODigSpUqaN++PRYvXqzUXmRkJFq3bo2BAwfit99+w4sXL4oVj1wul/RvamoKNTU1eHl5YciQIRg+fDiMjY3h7e0NALh06RJatGgBhUKBChUqoFevXnj48KHY3vPnz+Hv7w+FQgEzMzPMnTsXXl5eklH6gqbyGhoaIioqSly/ffs2unbtCkNDQxgZGaFdu3ZITU0VtwcEBKB9+/aYM2cOzMzMUK5cOQwePBg5OTlinezsbIwZMwaWlpaQy+Wws7PDqv9r777Dorjet4HfC0hdWARFRIFFBSkKgl2jgA3ssRdEiSXRiEYj1ogFC3ZjiUGlmqhojAVLbAQMghXBys9CNJhvsGGUoiLCvH94Ma8rRVCWlvtzXXOFnTlz5jlnV8Kz58yZoCAIgoBGjRph1apVCjEkJiaWaKQ/NjYWz58/R2BgIBwdHWFhYQFXV1esXbsWFhYWuHfvHlxdXQEANWvWhEQigZeXFwDg6NGj+Oyzz6Cvrw9DQ0P06tULycnJYt337t2DRCLB3r174erqCm1tbTg4OODMmTMKMYSGhsLMzAza2tro168f0tLSFI4nJyejb9++qFOnDqRSKVq2bImTJ08qlJHL5Vi0aBFGjhwJPT09fPnllyWq+0PyP9N169aFjY0NxowZg7i4OGRmZmLGjBliuby8PPj7+8PCwgJaWlpwcHDAnj17xGP169fHjz/+qFB3QkICVFRU8Ndff4nXevezNHPmTFhZWUFbWxsNGjSAr6+v+JkIDQ3FwoULcfnyZXE0Pv8z9349V69eRadOnaClpQVDQ0N8+eWXyMzMFI+X5PNHRERERFTdVImkuySMjY1x/fp1XLt2rdhygiAgJCQEI0aMgLW1NRo1aiQmLWUhLCwM6urqiI2NRUBAAJ49e4ZOnTrB0dERFy9exNGjR/Hw4UMMHjxYPGf69Ok4deoUDhw4gOPHjyM6OhqXLl0q1XVzcnLg5uYGXV1dxMTEIDY2FlKpFO7u7nj9+rVYLioqCsnJyYiKikJYWBhCQ0MVEveRI0di586dWL9+PZKSkrB582ZIpVJIJBKMHj26wAh/SEgIOnbsiEaNGhUbn7GxMd68eYN9+/ZBEIQCx01NTfHrr78CAG7evInU1FSsW7cOwNsvJb799ltcvHgRkZGRUFFRQb9+/ZCXl6dQx3fffQcfHx8kJibCysoKw4YNw5s3bwAA586dw5gxY+Dt7Y3ExES4urpi8eLFCudnZmaiR48eiIyMREJCAtzd3dG7d2+kpKQolFu1ahUcHByQkJAAX1/fEtX9MYyMjODh4YGIiAjk5uYCAPz9/bFt2zYEBATg+vXrmDp1KkaMGIFTp05BRUUFw4YNw44dOxTq2b59O9q3bw9zc/NCr6Orq4vQ0FDcuHED69atw9atW7F27VoAwJAhQzBt2jTY2dkhNTUVqampGDJkSIE6srKy4Obmhpo1a+LChQv45ZdfcPLkSXh7eyuU+9Dnj4iIiIiouqkS08tLYtKkSYiJiUHTpk1hbm6ONm3aoFu3bvDw8ICGhoZY7uTJk3jx4oU4Cj1ixAgEBQXB09OzxNc6dOgQpFKp+Lp79+745ZdfAACWlpZYsWKFeGzx4sVwdHTE0qVLxX3BwcEwNTXFrVu3YGJigqCgIPz888/o3LkzgLeJe/369UvV/l27diEvLw+BgYGQSCQA3ibE+vr6iI6ORrdu3QC8HUXeuHEjVFVVYW1tjZ49eyIyMhLjxo3DrVu3sHv3bpw4cQJdunQBADRo0EC8hpeXF+bNm4fz58+jVatWyMnJwY4dOwqMfhemTZs2mDNnDoYPH47x48ejVatW6NSpE0aOHIk6depAVVUVBgYGAN4mm+8uQjZgwACFuoKDg1G7dm3cuHEDTZo0Eff7+PigZ8+eAICFCxfCzs4Od+7cgbW1NdatWwd3d3dx1NjKygpxcXE4evSoeL6DgwMcHBzE14sWLcK+ffsQERGhkDx26tQJ06ZNE1/7+vp+sO6PZW1tjYyMDKSlpUEmk2Hp0qU4efIk2rZtC+Dt+3P69Gls3rwZzs7O8PDwwOrVq5GSkgIzMzPk5eUhPDwcc+fOLfIa7x6Ty+Xw8fFBeHg4ZsyYAS0tLUilUqipqRU7nXzHjh149eoVtm3bBh0dHQDAxo0b0bt3byxfvhx16tQBUPzn733Z2dnIzs4WX6enp5eu84iIiIiIKoFqM9Kto6ODw4cP486dO5g7dy6kUimmTZuGVq1a4cWLF2K54OBgDBkyBGpqb79vGDZsGGJjYxWmK3+Iq6srEhMTxW39+vXisebNmyuUvXz5MqKioiCVSsXN2toawNvpzMnJyXj9+jVat24tnmNgYIDGjRuXqv2XL1/GnTt3oKurK17HwMAAr169UmibnZ0dVFVVxdd169YVFzJLTEyEqqoqnJ2dC72GiYkJevbsieDgYADAwYMHkZ2djUGDBpUoxiVLluDBgwcICAiAnZ0dAgICYG1tjatXrxZ73u3btzFs2DA0aNAAenp6kMvlAFBgBNre3l6hXQDEtiUlJSn0MQAxcc2XmZkJHx8f2NjYQF9fH1KpFElJSQWu06JFC4XXJan7Y+XPCsifwv/ixQt07dpV4fO0bds28T1u1qwZbGxsxNHuU6dO4dGjR8W+R7t27UL79u3FWzXmzp1boM0fkpSUBAcHBzHhBoD27dsjLy9PYf2F4j5/7/P394dMJhM3U1PTUsVERERERFQZVJukO1/Dhg0xduxYBAYG4tKlS7hx4wZ27doFAHj69Cn27duHTZs2QU1NDWpqaqhXrx7evHkjJpIloaOjg0aNGolbfoKXf+xdmZmZ6N27t0KSnpiYiNu3b6Njx44lvqZEIikwLfvde2EzMzPRvHnzAte5desWhg8fLparUaNGgXrzp2lraWl9MI6xY8ciPDwcL1++REhICIYMGQJtbe0St8PQ0BCDBg3CqlWrkJSUBBMTkw+OlPfu3RtPnz7F1q1bce7cOZw7dw4AFKbNv9+2/NH+96egF8fHxwf79u3D0qVLERMTg8TERDRt2rTAdd5/j5UpKSkJenp6MDQ0FO+PPnz4sMJ7fOPGDYVbJDw8PMSke8eOHXB3d4ehoWGh9Z85cwYeHh7o0aMHDh06hISEBHz33XcF2lxWivv8vW/27Nl4/vy5uN2/f18pMRERERERKVO1mV5eGLlcDm1tbXF17O3bt6N+/foFFiQ7fvw4Vq9eDT8/P4VRuLLg5OSEX3/9FXK5XBxdf1fDhg1Ro0YNnDt3DmZmZgCAf//9F7du3VIYca5du7bCKu63b99WGMF3cnLCrl27YGRkBD09vY+KtWnTpsjLy8OpU6fE6eXv69GjB3R0dPDjjz/i6NGj+OOPPz7qWgCgrq6Ohg0biu9P/irg+fcvA0BaWhpu3ryJrVu3okOHDgCA06dPl/paNjY2YrKe7+zZswqvY2Nj4eXlhX79+gF4+0XGuwvRfUrdH+PRo0fYsWMHPv/8c6ioqMDW1hYaGhpISUkpcjYCAAwfPhxz585FfHw89uzZg4CAgCLLxsXFwdzcHN999524L3/BtXzq6uoK70lhbGxsEBoaiqysLPFLidjYWKioqJR61kY+DQ0NhVtDiIiIiIiqomoz0r1gwQLMmDED0dHRuHv3LhISEjB69Gjk5OSga9euAICgoCAMHDgQTZo0UdjGjBmDJ0+elMk9uO+bOHEinj59imHDhuHChQtITk7GsWPH8MUXXyA3NxdSqRRjxozB9OnT8fvvv+PatWvw8vKCioriW9OpUyds3LgRCQkJuHjxIsaPH68waujh4YFatWqhb9++iImJwd27dxEdHY3Jkyfj77//LlGscrkco0aNwujRo7F//36xjt27d4tlVFVV4eXlhdmzZ8PS0rLE06gPHTqEESNG4NChQ7h16xZu3ryJVatW4ciRI+jbty8AwNzcHBKJBIcOHcLjx4+RmZmJmjVrwtDQEFu2bMGdO3fw+++/49tvvy3RNd81efJkHD16FKtWrcLt27excePGAu+3paUl9u7di8TERFy+fBnDhw8v0Uh5Ser+EEEQ8ODBA6SmpiIpKQnBwcFo164dZDIZli1bBuDtgmc+Pj6YOnUqwsLCkJycjEuXLmHDhg0ICwsT65LL5WjXrh3GjBmD3Nxc9OnTp8jrWlpaIiUlBeHh4UhOTsb69euxb98+hTJyuRx3795FYmIinjx5onCfdT4PDw9oampi1KhRuHbtGqKiojBp0iR4enqK93MTEREREf0XVZuk29nZGX/++SdGjhwJa2trdO/eHQ8ePMDx48fRuHFjxMfH4/LlywUW5QIAmUyGzp07IygoqMzjMjExQWxsLHJzc9GtWzc0bdoUU6ZMgb6+vphYr1y5Eh06dEDv3r3RpUsXfPbZZwXuDV+9ejVMTU3RoUMHDB8+HD4+PgrTurW1tfHHH3/AzMwM/fv3Fx879erVq1KNfP/4448YOHAgvv76a1hbW2PcuHEFnqM9ZswYvH79Gl988UWJ67W1tYW2tjamTZuGZs2aoU2bNti9ezcCAwPFRezq1auHhQsXYtasWahTpw68vb2hoqKC8PBwxMfHo0mTJpg6dSpWrlxZ4uvma9OmDbZu3Yp169bBwcEBx48fL7C42Jo1a1CzZk20a9cOvXv3hpubG5ycnMqk7g9JT09H3bp1Ua9ePbRt2xabN2/GqFGjkJCQoHD7wqJFi+Dr6wt/f3/Y2NjA3d0dhw8fhoWFhUJ9Hh4euHz5Mvr161fsbQN9+vTB1KlT4e3tjWbNmiEuLg6+vr4KZQYMGAB3d3e4urqidu3a2LlzZ4F6tLW1cezYMTx9+hQtW7bEwIED0blzZ2zcuLFU/UBEREREVN1IhMKe30QVzsXFBc2aNcP3339f0aEUEBMTg86dO+P+/fscxaRyk56eDplMBodJAVDV+PD6A/Tx4leOrOgQiIiIiCq9/L9Pnz9/XuxAZ7W+p5vKVnZ2Nh4/fowFCxZg0KBBTLiJiIiIiIg+oNpMLy8LKSkpCo9ien8r7WOUqpudO3fC3Nwcz549U3gWOfB2kbqi+s3Ozq6CIq4c7Ozsiuyb7du3V3R4RERERESkRJxe/o43b94Uu1p1USuQE5CRkYGHDx8WeqxGjRowNzcv54gqj7/++kvh8W7vqlOnDnR1dcs5oqqJ08vLD6eXExEREX0Yp5d/BDU1NTRq1Kiiw6iSdHV1mTwW4b/8hQMRERER0X8dp5cTERERERERKQmTbiIiIiIiIiIlYdJNREREREREpCS8p5uIqpQ/Fg8rdqEKIiIiIqLKhCPdRERERERERErCpJuIiIiIiIhISZh0ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlISPDCOiKqXj3J1Q1dCq6DCqrfiVIys6BCIiIqJqhSPdRERERERERErCpJuIiIiIiIhISZh0ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhIm3VRtPH78GBMmTICZmRk0NDRgbGwMNzc3xMbGAgAuX76MPn36wMjICJqampDL5RgyZAgePXqEBQsWQCKRFLvl27lzJ1RVVTFx4sRSxRcdHS3WpaKiAplMBkdHR8yYMQOpqakKZRcsWIBmzZoVqOPevXuQSCRITEwEANStWxfLli1TKDNr1ixIJBJER0cr7HdxcYGnpycAIDQ0FBKJBDY2NgWu8csvv0AikUAul5eqfUREREREVBCTbqo2BgwYgISEBISFheHWrVuIiIiAi4sL0tLS8PjxY3Tu3BkGBgY4duwYkpKSEBISAhMTE2RlZcHHxwepqaniVr9+ffj5+SnsyxcUFIQZM2Zg586dePXqVanjvHnzJv755x9cuHABM2fOxMmTJ9GkSRNcvXq11HW5uLgUSK6joqJgamqqsP/Vq1c4e/YsOnXqJO7T0dHBo0ePcObMGYXzg4KCYGZmVupYiIiIiIioICbdVC08e/YMMTExWL58OVxdXWFubo5WrVph9uzZ6NOnD2JjY/H8+XMEBgbC0dERFhYWcHV1xdq1a2FhYQGpVApjY2NxU1VVha6ursI+ALh79y7i4uIwa9YsWFlZYe/evaWO1cjICMbGxrCyssLQoUMRGxuL2rVrY8KECaWuy9XVFbGxsXjz5g0AICMjAwkJCZg5c6ZC0n3mzBlkZ2fD1dVV3Kempobhw4cjODhY3Pf3338jOjoaw4cPL3CtAwcOwMnJCZqammjQoAEWLlwoXhcA1qxZg6ZNm0JHRwempqb4+uuvkZmZKR4PDQ2Fvr4+jh07BhsbG0ilUri7uxcY5SciIiIiqk6YdFO1IJVKIZVKsX//fmRnZxc4bmxsjDdv3mDfvn0QBOGjrxMSEoKePXtCJpNhxIgRCAoK+pSwAQBaWloYP348YmNj8ejRo1Kd6+rqiszMTFy4cAEAEBMTAysrKwwYMADnzp0TR+KjoqIgl8sLTBkfPXo0du/ejRcvXgB4mxi7u7ujTp06CuViYmIwcuRIfPPNN7hx4wY2b96M0NBQLFmyRCyjoqKC9evX4/r16wgLC8Pvv/+OGTNmKNTz4sULrFq1Cj/99BP++OMPpKSkwMfHp9C2ZWdnIz09XWEjIiIiIqpqmHRTtaCmpobQ0FCEhYVBX18f7du3x5w5c3DlyhUAQJs2bTBnzhwMHz4ctWrVQvfu3bFy5Uo8fPiwxNfIy8tDaGgoRowYAQAYOnQoTp8+jbt3735y/NbW1gDe3rOd7+rVq+KXCfmbnZ2dwnmWlpaoV6+eOKodHR0NZ2dnGBsbw8zMTJw6Hh0drTDKnc/R0RENGjTAnj17IAgCQkNDMXr06ALlFi5ciFmzZmHUqFFo0KABunbtikWLFmHz5s1imSlTpsDV1RVyuRydOnXC4sWLsXv3boV6cnJyEBAQgBYtWsDJyQne3t6IjIwstE/8/f0hk8nEzdTU9MMdSURERERUyTDppmpjwIAB+OeffxAREQF3d3dER0fDyckJoaGhAIAlS5bgwYMHCAgIgJ2dHQICAmBtbV3ie6lPnDiBrKws9OjRAwBQq1YtdO3aVWF69sfKH31/d8G2xo0bIzExUWE7cuRIgXPfva87OjoaLi4uAABnZ2dER0fj5cuXOHfuXKFJN/B2tDskJASnTp1SaN+7Ll++DD8/P4UvAMaNG4fU1FRxlPzkyZPo3Lkz6tWrB11dXXh6eiItLU08DgDa2tpo2LCh+Lpu3bpFju7Pnj0bz58/F7f79+8X04NERERERJUTk26qVjQ1NdG1a1f4+voiLi4OXl5emD9/vnjc0NAQgwYNwqpVq5CUlAQTExOsWrWqRHUHBQXh6dOn0NLSgpqaGtTU1HDkyBGEhYUhLy/vk+JOSkoCAIXp3+rq6mjUqJHCZm5uXuDc/Pu609LSkJCQAGdnZwBvk+6oqCjExcXh9evXCouovcvDwwNnz57FggUL4OnpCTU1tQJlMjMzsXDhQoUvAK5evYrbt29DU1MT9+7dQ69evWBvb49ff/0V8fHx+OGHHwAAr1+/FuupUaOGQr0SiaTI6f4aGhrQ09NT2IiIiIiIqpqCf10TVSO2trbYv39/ocfU1dXRsGFDZGVlfbCetLQ0HDhwAOHh4QpTvHNzc/HZZ5/h+PHjcHd3/6gYX758iS1btqBjx46oXbt2qc93dXVFVlYW1qxZA0tLSxgZGQEAOnbsiDFjxuC3334Tp6EXxsDAAH369MHu3bsREBBQaBknJyfcvHkTjRo1KvR4fHw88vLysHr1aqiovP0u7/2p5URERERE/0VMuqlaSEtLw6BBgzB69GjY29tDV1cXFy9exIoVK9C3b18cOnQI4eHhGDp0KKysrCAIAg4ePIgjR44gJCTkg/X/9NNPMDQ0xODBgxWmgANAjx49EBQUVOKk+9GjR3j16hUyMjIQHx+PFStW4MmTJx+1EjoANGjQAGZmZtiwYQM8PDzE/aampjAxMcGWLVswbNiwYusIDQ3Fpk2bYGhoWOjxefPmoVevXjAzM8PAgQOhoqKCy5cv49q1a1i8eDEaNWqEnJwcbNiwAb1790ZsbGyRCTwRERER0X8Jp5dTtSCVStG6dWusXbsWHTt2RJMmTeDr64tx48Zh48aNsLW1hba2NqZNm4ZmzZqhTZs22L17NwIDA+Hp6fnB+oODg9GvX78CCTfw9l7yiIgIPHnypESxNm7cGCYmJmjevDmWLVuGLl264Nq1a7C1tS11u/O5uroiIyNDvJ87n7OzMzIyMoq8nzuflpZWkQk3ALi5ueHQoUM4fvw4WrZsiTZt2mDt2rXidHcHBwesWbMGy5cvR5MmTbB9+3b4+/t/dHuIiIiIiKoLifApz08iIion6enpkMlkcJgUAFUNrYoOp9qKXzmyokMgIiIiqhLy/z59/vx5sesPcaSbiIiIiIiISEmYdBOVke7duxd4rnb+tnTp0ooOj4iIiIiIKgAXUiMqI4GBgXj58mWhxwwMDMo5GiIiIiIiqgyYdBOVkaIeyUVERERERP9dnF5OREREREREpCRMuomIiIiIiIiUhEk3ERERERERkZLwnm4iqlL+WDys2OcgEhERERFVJhzpJiIiIiIiIlISJt1ERERERERESsKkm4iIiIiIiEhJmHQTERERERERKQmTbiIiIiIiIiIlYdJNREREREREpCR8ZBgRVSkd5+6EqoZWRYdR7cSvHFnRIRARERFVSxzpJiIiIiIiIlISJt1ERERERERESsKkm4iIiIiIiEhJmHQTERERERERKQmTbiIiIiIiIiIlYdJNREREREREpCRMuomIiIiIiIiUhEk3ERERERERkZIw6SaiIkkkkmK3BQsWAAD27duHNm3aQCaTQVdXF3Z2dpgyZUqhdbq5uUFVVRUXLlwov4YQEREREVUQtYoOgIgqr9TUVPHnXbt2Yd68ebh586a4TyqVIjIyEkOGDMGSJUvQp08fSCQS3LhxAydOnChQX0pKCuLi4uDt7Y3g4GC0bNmyXNpBRERERFRRmHQTUZGMjY3Fn2UyGSQSicI+ADh48CDat2+P6dOni/usrKzw+eefF6gvJCQEvXr1woQJE9CmTRusWbMGWlpaSoufiIiIiKiicXo5EX0SY2NjXL9+HdeuXSu2nCAICAkJwYgRI2BtbY1GjRphz549RZbPzs5Genq6wkZEREREVNUw6SaiTzJp0iS0bNkSTZs2hVwux9ChQxEcHIzs7GyFcidPnsSLFy/g5uYGABgxYgSCgoKKrNff3x8ymUzcTE1NldoOIiIiIiJlYNJNRJ9ER0cHhw8fxp07dzB37lxIpVJMmzYNrVq1wosXL8RywcHBGDJkCNTU3t7VMmzYMMTGxiI5ObnQemfPno3nz5+L2/3798ulPUREREREZYlJNxGViYYNG2Ls2LEIDAzEpUuXcOPGDezatQsA8PTpU+zbtw+bNm2Cmpoa1NTUUK9ePbx58wbBwcGF1qehoQE9PT2FjYiIiIioquFCakRU5uRyObS1tZGVlQUA2L59O+rXr4/9+/crlDt+/DhWr14NPz8/qKqqVkCkRERERETKxaSbiD7JggUL8OLFC/To0QPm5uZ49uwZ1q9fj5ycHHTt2hUAEBQUhIEDB6JJkyYK55qammL27Nk4evQoevbsWRHhExEREREpFaeXE9EncXZ2xp9//omRI0fC2toa3bt3x4MHD3D8+HE0btwY8fHxuHz5MgYMGFDgXJlMhs6dOxe7oBoRERERUVUmEQRBqOggiIg+JD09HTKZDA6TAqCqwWd7l7X4lSMrOgQiIiKiKiX/79Pnz58Xu/4QR7qJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhIm3URERERERERKwqSbiIiIiIiISEmYdBMREREREREpiVpFB0BEVBp/LB5W7CMZiIiIiIgqE450ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhI+MoyIqpSOc3dCVUOrosOoVuJXjqzoEIiIiIiqLY50ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhIm3URERERERERKwqSbiEpELpfj+++/L3F5FxcXTJkyRWnxEBERERFVBUy6qVxIJJJitwULFlR0iGWutEmqMi1YsADNmjVT2BcTEwN9fX1MmTIFgiCU+TX37t2LRYsWlXm9RERERERViVpFB0D/DampqeLPu3btwrx583Dz5k1xn1QqrYiwSk0QBOTm5kJNrfz+6bx+/Rrq6uplWufhw4cxaNAgzJo1C/PmzSvTuvMZGBgopV4iIiIioqqEI91ULoyNjcVNJpNBIpEo7AsPD4eNjQ00NTVhbW2NTZs2iefeu3cPEokEu3fvRocOHaClpYWWLVvi1q1buHDhAlq0aAGpVIru3bvj8ePH4nleXl74/PPPsXDhQtSuXRt6enoYP348Xr9+LZbJy8uDv78/LCwsoKWlBQcHB+zZs0c8Hh0dDYlEgt9++w3NmzeHhoYGTp8+jeTkZPTt2xd16tSBVCpFy5YtcfLkSfE8FxcX/PXXX5g6dao4mg8UPuL8/fffQy6XF4h7yZIlMDExQePGjQEA9+/fx+DBg6Gvrw8DAwP07dsX9+7dK/V7sWPHDvTv3x8rVqxQSLhPnz4t9q+pqSkmT56MrKysQusYPnw4hgwZorAvJycHtWrVwrZt28Q+eHd6uVwux9KlSzF69Gjo6urCzMwMW7ZsKXX8RERERERVCZNuqnDbt2/HvHnzsGTJEiQlJWHp0qXw9fVFWFiYQrn58+dj7ty5uHTpEtTU1DB8+HDMmDED69atQ0xMDO7cuVNg1DYyMhJJSUmIjo7Gzp07sXfvXixcuFA87u/vj23btiEgIADXr1/H1KlTMWLECJw6dUqhnlmzZmHZsmVISkqCvb09MjMz0aNHD0RGRiIhIQHu7u7o3bs3UlJSALydWl2/fn34+fkhNTVVYaS/JCIjI3Hz5k2cOHEChw4dQk5ODtzc3KCrq4uYmBjExsZCKpXC3d1d4UuED/nhhx/wxRdfIDg4GN7e3uL+5ORkuLu7Y8CAAbhy5Qp27dqF06dPK5R5l4eHBw4ePIjMzExx37Fjx/DixQv069evyOuvXr0aLVq0QEJCAr7++mtMmDBBYcYDEREREVF1w+nlVOHmz5+P1atXo3///gAACwsL3LhxA5s3b8aoUaPEcj4+PnBzcwMAfPPNNxg2bBgiIyPRvn17AMCYMWMQGhqqULe6ujqCg4Ohra0NOzs7+Pn5Yfr06Vi0aBFycnKwdOlSnDx5Em3btgUANGjQAKdPn8bmzZvh7Ows1uPn54euXbuKrw0MDODg4CC+XrRoEfbt24eIiAh4e3vDwMAAqqqq0NXVhbGxcan7REdHB4GBgeK08p9//hl5eXkIDAwUR81DQkKgr6+P6OhodOvW7YN1JiUlwdvbG0FBQfDw8FA45u/vDw8PD3Fk2tLSEuvXr4ezszN+/PFHaGpqKpR3c3ODjo4O9u3bB09PTwBvR9D79OkDXV3dImPo0aMHvv76awDAzJkzsXbtWkRFRYmj+e/Kzs5Gdna2+Do9Pf2DbSQiIiIiqmyYdFOFysrKQnJyMsaMGYNx48aJ+9+8eQOZTKZQ1t7eXvy5Tp06AICmTZsq7Hv06JHCOQ4ODtDW1hZft23bFpmZmbh//z4yMzPx4sULhWQaeHsPtaOjo8K+Fi1aKLzOzMzEggULcPjwYaSmpuLNmzd4+fKlONL9qZo2bapwH/fly5dx586dAgntq1evkJycXKI669evD319faxcuRLdu3dH3bp1Feq/cuUKtm/fLu4TBAF5eXm4e/cubGxsFOpSU1PD4MGDsX37dnh6eiIrKwsHDhxAeHh4sTG8+x7m32Lw/nuWz9/fX2FWAhERERFRVcSkmypU/vTkrVu3onXr1grHVFVVFV7XqFFD/Dl/tPf9fXl5eaW+9uHDh1GvXj2FYxoaGgqvdXR0FF77+PjgxIkTWLVqFRo1agQtLS0MHDjwg1O9VVRUCqwUnpOTU6Dc+9fLzMxE8+bNFZLifLVr1y72mvl0dXVx8uRJdO3aFa6uroiKihIT78zMTHz11VeYPHlygfPMzMwKrc/DwwPOzs549OgRTpw4AS0tLbi7uxcbw7vvF1D8ezZ79mx8++234uv09HSYmpoWWz8RERERUWXDpJsqVJ06dWBiYoI///yzwJTnsnD58mW8fPkSWlpaAICzZ89CKpXC1NQUBgYG0NDQQEpKisJU8pKIjY2Fl5eXeP9yZmZmgUXN1NXVkZubq7Cvdu3aePDgAQRBEL84SExM/OD1nJycsGvXLhgZGUFPT69Usb6rZs2aOHnyJLp16wYXFxdERUXBxMQETk5OuHHjBho1alTiutq1awdTU1Ps2rULv/32GwYNGlQgqf4UGhoaBb78ICIiIiKqariQGlW4hQsXwt/fH+vXr8etW7dw9epVhISEYM2aNZ9c9+vXrzFmzBjcuHEDR44cwfz58+Ht7Q0VFRXo6urCx8cHU6dORVhYGJKTk3Hp0iVs2LChwCJu77O0tMTevXuRmJiIy5cvY/jw4QVGbOVyOf744w/873//w5MnTwC8XdH78ePHWLFiBZKTk/HDDz/gt99++2A7PDw8UKtWLfTt2xcxMTG4e/cuoqOjMXnyZPz999+l6hN9fX2cOHECNWvWhIuLC/755x/MnDkTcXFx8Pb2RmJiIm7fvo0DBw4UuZBavuHDhyMgIAAnTpxQypcmRERERERVHZNuqnBjx45FYGAgQkJC0LRpUzg7OyM0NBQWFhafXHfnzp1haWmJjh07YsiQIejTpw8WLFggHl+0aBF8fX3h7+8PGxsbuLu74/Dhwx+89po1a1CzZk20a9cOvXv3hpubG5ycnBTK+Pn54d69e2jYsKE4BdzGxgabNm3CDz/8AAcHB5w/fx4+Pj4fbIe2tjb++OMPmJmZoX///rCxscGYMWPw6tWrjxr5lslkOH78OGrVqgVnZ2cYGhri1KlTuHXrFjp06ABHR0fMmzcPJiYmxdbj4eGBGzduoF69euKCdkRERERE9P9JhPdvMCWqJry8vPDs2TPs37+/okOhMpCeng6ZTAaHSQFQ1dCq6HCqlfiVIys6BCIiIqIqJ//v0+fPnxc7EMaRbiIiIiIiIiIlYdJNVA1IpdIit5iYmIoOj4iIiIjoP4url1O1FRoaWtEhlJviVkB//3FoRERERERUfph0E1UDpXnUFxERERERlR9OLyciIiIiIiJSEibdRERERERERErCpJuIiIiIiIhISXhPNxFVKX8sHlbscxCJiIiIiCoTjnQTERERERERKQmTbiIiIiIiIiIlYdJNREREREREpCRMuomIiIiIiIiUhEk3ERERERERkZIw6SYiIiIiIiJSEj4yjIiqlI5zd0JVQ6uiw6g24leOrOgQiIiIiKo1jnQTERERERERKQmTbiIiIiIiIiIlYdJNREREREREpCRMuomIiIiIiIiUhEk3ERERERERkZIw6SYiIiIiIiJSEibdRERERERERErCpJv+syQSCfbv31/l6v4UXl5e+PzzzytNPURERERE1V2lSrofP36MCRMmwMzMDBoaGjA2NoabmxtiY2MBAJcvX0afPn1gZGQETU1NyOVyDBkyBI8ePcKCBQsgkUiK3fLt3LkTqqqqmDhxYqnii46OLrTeuXPnlmk/VEcvXrzA7Nmz0bBhQ2hqaqJ27dpwdnbGgQMHxDJyuRzff/99xQVZQd79LOno6MDS0hJeXl6Ij48v82utW7cOoaGhJS5/7949SCQSJCYmflI9RERERET/VWoVHcC7BgwYgNevXyMsLAwNGjTAw4cPERkZibS0NDx+/BidO3dGr169cOzYMejr6+PevXuIiIhAVlYWfHx8MH78eLGuli1b4ssvv8S4ceMKXCcoKAgzZszA5s2bsXr1amhqapYqzps3b0JPT098LZVKC5TJzc2FRCKBikql+l6jwowfPx7nzp3Dhg0bYGtri7S0NMTFxSEtLa2iQ/tor1+/hrq6epnUFRISAnd3d7x69Qq3bt3Cli1b0Lp1awQHB2PkyJFlcg0AkMlklaoeIiIiIqLqrtJkhM+ePUNMTAyWL18OV1dXmJubo1WrVpg9ezb69OmD2NhYPH/+HIGBgXB0dISFhQVcXV2xdu1aWFhYQCqVwtjYWNxUVVWhq6ursA8A7t69i7i4OMyaNQtWVlbYu3dvqWM1MjJSqFcqlSI0NBT6+vqIiIiAra0tNDQ0kJKSguzsbPj4+KBevXrQ0dFB69atER0drVBfaGgozMzMoK2tjX79+mH16tXQ19cXjxc2lXfKlClwcXERX+fl5cHf3x8WFhbQ0tKCg4MD9uzZIx7PH6WPjIxEixYtoK2tjXbt2uHmzZsK9R48eBAtW7aEpqYmatWqhX79+gEA/Pz80KRJkwJ90axZM/j6+n6wzyIiIjBnzhz06NEDcrkczZs3x6RJkzB69GgAgIuLC/766y9MnTpVYWZCWloahg0bhnr16kFbWxtNmzbFzp07Fep2cXHB5MmTMWPGDBgYGMDY2BgLFixQKHP79m107NgRmpqasLW1xYkTJwrEOHPmTFhZWUFbWxsNGjSAr68vcnJyxOMLFixAs2bNEBgYCAsLC/HLmpLU/SH6+vowNjaGXC5Ht27dsGfPHnh4eMDb2xv//vuvWO706dPo0KEDtLS0YGpqismTJyMrKwsAMGfOHLRu3bpA3Q4ODvDz8wNQ8LN09OhRfPbZZ9DX14ehoSF69eqF5ORk8biFhQUAwNHRERKJRPzMvV9PdnY2Jk+eLM5C+eyzz3DhwgXxeEk/f0RERERE1U2lSbqlUimkUin279+P7OzsAseNjY3x5s0b7Nu3D4IgfPR1QkJC0LNnT8hkMowYMQJBQUGfEraCFy9eYPny5QgMDMT169dhZGQEb29vnDlzBuHh4bhy5QoGDRoEd3d33L59GwBw7tw5jBkzBt7e3khMTISrqysWL15c6mv7+/tj27ZtCAgIwPXr1zF16lSMGDECp06dUij33XffYfXq1bh48SLU1NTEpBcADh8+jH79+qFHjx5ISEhAZGQkWrVqBQAYPXo0kpKSFBKphIQEXLlyBV988cUH4zM2NsaRI0eQkZFR6PG9e/eifv368PPzQ2pqKlJTUwEAr169QvPmzXH48GFcu3YNX375JTw9PXH+/HmF88PCwqCjo4Nz585hxYoV8PPzE5PfvLw89O/fH+rq6jh37hwCAgIwc+bMAjHo6uoiNDQUN27cwLp167B161asXbtWocydO3fw66+/Yu/evUhMTCxx3R9j6tSpyMjIENuRnJwMd3d3DBgwAFeuXMGuXbtw+vRpeHt7AwA8PDxw/vx5haT5+vXruHLlCoYPH17oNbKysvDtt9/i4sWLiIyMhIqKCvr164e8vDwAEPv55MmTSE1NLfJLqhkzZuDXX39FWFgYLl26hEaNGsHNzQ1Pnz5VKFfc5+992dnZSE9PV9iIiIiIiKqaSjO9XE1NDaGhoRg3bhwCAgLg5OQEZ2dnDB06FPb29mjTpg3mzJmD4cOHY/z48WjVqhU6deqEkSNHok6dOiW6Rl5eHkJDQ7FhwwYAwNChQzFt2jTcvXtXHNErifr16yu8/uuvvwAAOTk52LRpExwcHAAAKSkpCAkJQUpKCkxMTAAAPj4+OHr0KEJCQrB06VKsW7cO7u7umDFjBgDAysoKcXFxOHr0aInjyc7OxtKlS3Hy5Em0bdsWANCgQQOcPn0amzdvhrOzs1h2yZIl4utZs2ahZ8+eePXqFTQ1NbFkyRIMHToUCxcuFMvnt6V+/fpwc3NDSEgIWrZsCeDtFxjOzs5o0KDBB2PcsmULPDw8YGhoCAcHB3z22WcYOHAg2rdvDwAwMDBQmJ2Qr169evDx8RFfT5o0CceOHcPu3bvFLwQAwN7eHvPnzwcAWFpaYuPGjYiMjETXrl1x8uRJ/N///R+OHTsmvg9Lly5F9+7dFWJ89958uVwOHx8fhIeHi+8N8HZK+bZt21C7dm0AwPHjx0tU98ewtrYG8Pa+auDtFyseHh6YMmWK2M7169fD2dkZP/74I+zs7ODg4IAdO3aIsw+2b9+O1q1bo1GjRoVeY8CAAQqvg4ODUbt2bdy4cQNNmjQR22loaKjwvrwrKysLP/74I0JDQ8V2b926FSdOnEBQUBCmT58uli3u8/c+f39/hc8iEREREVFVVGlGuoG3CcA///yDiIgIuLu7Izo6Gk5OTuKCTUuWLMGDBw8QEBAAOzs7BAQEwNraGlevXi1R/SdOnEBWVhZ69OgBAKhVqxa6du2K4ODgUsUZExODxMREcatZsyYAQF1dHfb29mK5q1evIjc3F1ZWVuJIvlQqxalTp8TRyKSkpAJTgvMT55K6c+cOXrx4ga5duypcZ9u2bQqjngAU4qtbty4A4NGjRwCAxMREdO7cucjrjBs3Djt37sSrV6/w+vVr7Nixo9iRynd17NgRf/75JyIjIzFw4EBcv34dHTp0wKJFi4o9Lzc3F4sWLULTpk1hYGAAqVSKY8eOISUlpch25bctv11JSUkwNTUVk2Kg8D7etWsX2rdvL94yMHfu3ALXMTc3FxPR0tT9MfJndORPtb98+TJCQ0MV3mM3Nzfk5eXh7t27AN6Odu/YsUM8f+fOnfDw8CjyGrdv38awYcPQoEED6OnpQS6XA0CBdhcnOTkZOTk54hcoAFCjRg20atUKSUlJCmWL+/y9b/bs2Xj+/Lm43b9/v8QxERERERFVFpVmpDufpqYmunbtiq5du8LX1xdjx47F/Pnz4eXlBeDtiNugQYMwaNAgLF26FI6Ojli1ahXCwsI+WHdQUBCePn0KLS0tcV9eXh6uXLmChQsXlnjRMwsLC4V7rvNpaWkprJKemZkJVVVVxMfHQ1VVVaFsYYuvFUVFRaXAlPp37zXOzMwE8HZ6eL169RTKaWhoKLyuUaOG+HN+rPlTid/tl8L07t0bGhoa2LdvH9TV1ZGTk4OBAweWuB01atRAhw4d0KFDB8ycOROLFy+Gn58fZs6cWeSCZCtXrsS6devw/fffo2nTptDR0cGUKVPw+vXrItuV37b8dpXEmTNn4OHhgYULF8LNzQ0ymQzh4eFYvXq1QjkdHZ0S1/mp8hPW/FkYmZmZ+OqrrzB58uQCZc3MzAAAw4YNw8yZM3Hp0iW8fPkS9+/fx5AhQ4q8Ru/evWFubo6tW7fCxMQEeXl5aNKkSYH+LSvFff7ep6GhUeDzS0RERERU1VS6pPt9tra2RT7vWF1dHQ0bNhQXkipOWloaDhw4gPDwcNjZ2Yn7c3Nz8dlnn+H48eNwd3cvq7ABvF18Kjc3F48ePUKHDh0KLWNjY4Nz584p7Dt79qzC69q1a+PatWsK+xITE8UE5t2F296dSl5a9vb2iIyMLPIebTU1NYwaNQohISFQV1fH0KFDP5ioF8fW1hZv3rzBq1evoK6uDnV1deTm5iqUiY2NRd++fTFixAgAbxO0W7duwdbWtsTXsbGxwf3795GamiqOrr7fx3FxcTA3N8d3330n7su/beBT6/5Y33//PfT09NClSxcAgJOTE27cuFHkVHHg7W0Azs7O2L59O16+fImuXbvCyMio0LJpaWm4efMmtm7dKn4+T58+rVAm/8uQ99+XdzVs2BDq6uqIjY2Fubk5gLdfCl24cEGcCk9ERERE9F9VaZLutLQ0DBo0CKNHj4a9vT10dXVx8eJFrFixAn379sWhQ4cQHh6OoUOHwsrKCoIg4ODBgzhy5AhCQkI+WP9PP/0EQ0NDDB48WGE0GgB69OiBoKCgMk+6rays4OHhgZEjR2L16tVwdHTE48ePERkZCXt7e/Ts2ROTJ09G+/btsWrVKvTt2xfHjh0rcD93p06dsHLlSmzbtg1t27bFzz//jGvXrsHR0RHA2wXAfHx8MHXqVOTl5eGzzz7D8+fPERsbCz09PYwaNapE8c6fPx+dO3dGw4YNMXToULx58wZHjhxRWBhs7NixsLGxAQDx+ekl4eLigmHDhqFFixYwNDTEjRs3MGfOHLi6uoqPX5PL5fjjjz8wdOhQaGhooFatWrC0tMSePXsQFxeHmjVrYs2aNXj48GGpku4uXbrAysoKo0aNwsqVK5Genq6QXANv749OSUlBeHg4WrZsicOHD2Pfvn1lUndJPHv2DA8ePEB2djZu3bqFzZs3Y//+/di2bZs4q2LmzJlo06YNvL29MXbsWOjo6ODGjRs4ceIENm7cKNbl4eGB+fPn4/Xr1wUWgntXzZo1YWhoiC1btqBu3bpISUnBrFmzFMoYGRlBS0sLR48eRf369aGpqVngcWE6OjqYMGECpk+fDgMDA5iZmWHFihV48eIFxowZU+q+ICIiIiKqTirNPd1SqRStW7fG2rVr0bFjRzRp0gS+vr4YN24cNm7cCFtbW2hra2PatGlo1qwZ2rRpg927dyMwMBCenp4frD84OBj9+vUrkHADb+8lj4iIwJMnT8q8XSEhIRg5ciSmTZuGxo0b4/PPP8eFCxfE6cBt2rTB1q1bsW7dOjg4OOD48eMKC3oBgJubG3x9fTFjxgy0bNkSGRkZBZ7dvGjRIvj6+sLf3x82NjZwd3fH4cOHS7VAnIuLC3755RdERESgWbNm6NSpU4FVwi0tLdGuXTtYW1sX+niqori5uSEsLAzdunWDjY0NJk2aBDc3N+zevVss4+fnh3v37qFhw4bifdNz586Fk5MT3Nzc4OLiAmNj4wKPT/sQFRUV7Nu3Dy9fvkSrVq0wduxYLFmyRKFMnz59MHXqVHh7e6NZs2aIi4sr0aPQSlJ3SXzxxReoW7curK2tMWHCBEilUpw/f15h1XF7e3ucOnUKt27dQocOHeDo6Ih58+Yp3E8OAAMHDkRaWhpevHhRbF+pqKggPDwc8fHxaNKkCaZOnYqVK1cqlFFTU8P69euxefNmmJiYoG/fvoXWtWzZMgwYMACenp5wcnLCnTt3cOzYMXG9AyIiIiKi/yqJ8CnP3yKlCA0NxZQpU/Ds2bOKDqUAQRBgaWmJr7/+Gt9++21Fh0P/Ienp6ZDJZHCYFABVjY+/rYEUxa8c+eFCRERERFRA/t+nz58/F2fvFqbSTC+nyu/x48cIDw/HgwcPSvRsbiIiIiIiov+6SjO9vDLo3r27wuOY3t2WLl1a0eFVOCMjI/j5+WHLli0Fpg0X1W9SqRQxMTEVFHHFW7p0aZH9UhbP8iYiIiIiosqN08vf8b///Q8vX74s9JiBgQEMDAzKOaKq486dO0Ueq1ev3ietcl6VPX36FE+fPi30mJaWVoFHvFHROL1cOTi9nIiIiOjjcHr5R2AC9PGKe4zVfxm/rCEiIiIi+m/j9HIiIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlIT3dBNRlfLH4mHFLlRBRERERFSZcKSbiIiIiIiISEmYdBMREREREREpCZNuIiIiIiIiIiVh0k1ERERERESkJEy6iYiIiIiIiJSEq5cTUZXSce5OqGpoVXQYVV78ypEVHQIRERHRfwJHuomIiIiIiIiUhEk3ERERERERkZIw6SYiIiIiIiJSEibdRERERERERErCpJuIiIiIiIhISZh0ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLrpP0sul+P777+vcnV/igULFqBZs2aVph4iIiIiouqOSfcHPH78GBMmTICZmRk0NDRgbGwMNzc3xMbGAgAuX76MPn36wMjICJqampDL5RgyZAgePXqEBQsWQCKRFLvl27lzJ1RVVTFx4sRSxRcdHS3WpaKiAplMBkdHR8yYMQOpqakKZYtKlO7duweJRILExEQAQN26dbFs2TKFMrNmzYJEIkF0dLTCfhcXF3h6en4wztzcXCxbtgzW1tbQ0tKCgYEBWrdujcDAQIW6pkyZUqJ2VydyuVx8D7W0tCCXyzF48GD8/vvvZX4tHx8fREZGluociUSC/fv3f3I9RERERET/RUy6P2DAgAFISEhAWFgYbt26hYiICLi4uCAtLQ2PHz9G586dYWBggGPHjiEpKQkhISEwMTFBVlYWfHx8kJqaKm7169eHn5+fwr58QUFBmDFjBnbu3IlXr16VOs6bN2/in3/+wYULFzBz5kycPHkSTZo0wdWrV0tdl4uLS4HkOioqCqampgr7X716hbNnz6JTp04frHPhwoVYu3YtFi1ahBs3biAqKgpffvklnj17Vur4Kovc3Fzk5eWVSV35n4ubN29i27Zt0NfXR5cuXbBkyZIyqT+fVCqFoaFhpamHiIiIiKi6Y9JdjGfPniEmJgbLly+Hq6srzM3N0apVK8yePRt9+vRBbGwsnj9/jsDAQDg6OsLCwgKurq5Yu3YtLCwsIJVKYWxsLG6qqqrQ1dVV2AcAd+/eRVxcHGbNmgUrKyvs3bu31LEaGRnB2NgYVlZWGDp0KGJjY1G7dm1MmDCh1HW5uroiNjYWb968AQBkZGQgISEBM2fOVEi6z5w5g+zsbLi6un6wzoiICHz99dcYNGgQLCws4ODggDFjxsDHxwcA4OXlhVOnTmHdunXiqO+9e/eQm5uLMWPGwMLCAlpaWmjcuDHWrVunULeXlxc+//xzrFq1CnXr1oWhoSEmTpyInJwcscyjR4/Qu3dvaGlpwcLCAtu3by8Q45o1a9C0aVPo6OjA1NQUX3/9NTIzM8XjoaGh0NfXR0REBGxtbaGhoYGUlJQS1f0h+Z8LMzMzdOzYEVu2bIGvry/mzZuHmzdviuWuXbuG7t27QyqVok6dOvD09MSTJ08AAFu2bIGJiUmBLwL69u2L0aNHAyg42+HChQvo2rUratWqBZlMBmdnZ1y6dEk8LpfLAQD9+vWDRCIRX79fT15eHvz8/FC/fn1oaGigWbNmOHr0qHg8fzbF3r174erqCm1tbTg4OODMmTOl7isiIiIioqqESXcxpFIppFIp9u/fj+zs7ALHjY2N8ebNG+zbtw+CIHz0dUJCQtCzZ0/IZDKMGDECQUFBnxI2AEBLSwvjx49HbGwsHj16VKpzXV1dkZmZiQsXLgAAYmJiYGVlhQEDBuDcuXPiSHxUVBTkcrmYiBXH2NgYv//+Ox4/flzo8XXr1qFt27YYN26cOAvA1NQUeXl5qF+/Pn755RfcuHED8+bNw5w5c7B7926F86OiopCcnIyoqCiEhYUhNDQUoaGh4nEvLy/cv38fUVFR2LNnDzZt2lSgX1RUVLB+/Xpcv34dYWFh+P333zFjxgyFMi9evMDy5csRGBiI69evw8jIqER1f4xvvvkGgiDgwIEDAN5+CdSpUyc4Ojri4sWLOHr0KB4+fIjBgwcDAAYNGoS0tDRERUWJdTx9+hRHjx6Fh4dHodfIyMjAqFGjcPr0aZw9exaWlpbo0aMHMjIyAED8DISEhCA1NVV8/b5169Zh9erVWLVqFa5cuQI3Nzf06dMHt2/fVij33XffwcfHB4mJibCyssKwYcPEL3fel52djfT0dIWNiIiIiKiqYdJdDDU1NYSGhiIsLAz6+vpo37495syZgytXrgAA2rRpgzlz5mD48OGoVasWunfvjpUrV+Lhw4clvkZeXh5CQ0MxYsQIAMDQoUNx+vRp3L1795Pjt7a2BvB2lDHf1atXxS8T8jc7OzuF8ywtLVGvXj1xVDs6OhrOzs7iSGz+6GR0dHSJRrmBt6PIjx8/hrGxMezt7TF+/Hj89ttv4nGZTAZ1dXVoa2srzAyoUaMGFi5ciBYtWsDCwgIeHh744osvCiTdNWvWxMaNG2FtbY1evXqhZ8+e4j3Ht27dwm+//YatW7eiTZs2aN68OYKCgvDy5UuFOqZMmQJXV1fI5XJ06tQJixcvLnCdnJwcbNq0Ce3atUPjxo3x999/l6juj2FgYAAjIyPx/du4cSMcHR2xdOlSWFtbw9HREcHBwYiKisKtW7dQs2ZNdO/eHTt27BDr2LNnD2rVqlXk+9SpUyeMGDEC1tbWsLGxwZYtW/DixQucOnUKAFC7dm0AgL6+PoyNjcXX71u1ahVmzpyJoUOHonHjxli+fDmaNWtWYDE5Hx8f9OzZE1ZWVli4cCH++usv3Llzp9A6/f39IZPJxM3U1LQ03UdEREREVCkw6f6AAQMG4J9//kFERATc3d0RHR0NJycncRR1yZIlePDgAQICAmBnZ4eAgABYW1uX+F7qEydOICsrCz169AAA1KpVC127dkVwcPAnx54/+v7ugm2NGzdGYmKiwnbkyJEC5757X3d0dDRcXFwAAM7OzoiOjsbLly9x7ty5Eifdtra2uHbtGs6ePYvRo0eLU7LHjh37wXN/+OEHNG/eHLVr14ZUKsWWLVuQkpKiUMbOzg6qqqri67p164qjzUlJSVBTU0Pz5s3F49bW1tDX11eo4+TJk+jcuTPq1asHXV1deHp6Ii0tDS9evBDLqKurw97eXnxd0ro/liAI4vt3+fJlREVFKXxhkv/FSnJyMgDAw8MDv/76qzgzY/v27Rg6dChUVAr/p/7w4UOMGzcOlpaWkMlk0NPTQ2ZmZoH+LU56ejr++ecftG/fXmF/+/btkZSUpLDv3b6rW7cuABQ5K2D27Nl4/vy5uN2/f7/EMRERERERVRZMuktAU1MTXbt2ha+vL+Li4uDl5YX58+eLxw0NDTFo0CCsWrUKSUlJMDExwapVq0pUd1BQEJ4+fQotLS2oqalBTU0NR44cQVhY2Ccv0pWf8Lw7/VtdXR2NGjVS2MzNzQucm39fd1paGhISEuDs7AzgbdIdFRWFuLg4vH79ukSLqOVTUVFBy5YtMWXKFOzduxehoaEICgoqdlQ/PDwcPj4+GDNmDI4fP47ExER88cUXeP36tUK5GjVqKLyWSCSl6r979+6hV69esLe3x6+//or4+Hj88MMPAKBwLS0tLYUvMZQpf7E+CwsLAEBmZiZ69+5d4EuT27dvo2PHjgCA3r17QxAEHD58GPfv30dMTEyRU8sBYNSoUUhMTMS6desQFxeHxMREGBoaFujfsvLu+5Tfj0W9TxoaGtDT01PYiIiIiIiqGrWKDqAqsrW1LfAIpXzq6upo2LAhsrKyPlhPWloaDhw4gPDwcIUp3rm5ufjss89w/PhxuLu7f1SML1++xJYtW9CxY8cipwQXx9XVFVlZWVizZg0sLS1hZGQEAOjYsSPGjBmD3377TZyG/rFsbW0BQOwrdXV15ObmKpSJjY1Fu3bt8PXXX4v78kd1S8ra2hpv3rxBfHw8WrZsCeDtau/vrpweHx+PvLw8rF69WhwVfn9q+cfW/bHWrVsHFRUVfP755wAAJycn/Prrr5DL5VBTK/yfrqamJvr374/t27fjzp07aNy4MZycnIq8RmxsLDZt2iTOtLh//764MFu+GjVqFHhf3qWnpwcTExPExsaKX87k192qVauSNpeIiIiIqFpi0l2MtLQ0DBo0CKNHj4a9vT10dXVx8eJFrFixAn379sWhQ4cQHh6OoUOHwsrKCoIg4ODBgzhy5AhCQkI+WP9PP/0EQ0NDDB48uMDoaY8ePRAUFFTipPvRo0d49eoVMjIyEB8fjxUrVuDJkycftRI6ADRo0ABmZmbYsGGDwkipqakpTExMsGXLFgwbNqzE9Q0cOBDt27dHu3btYGxsjLt372L27NmwsrISp0jL5XKcO3cO9+7dg1QqhYGBASwtLbFt2zYcO3YMFhYW+Omnn3DhwgVx9LckGjduDHd3d3z11Vf48ccfoaamhilTpkBLS0ss06hRI+Tk5GDDhg3o3bs3YmNjERAQUCZ1l0RGRgYePHiAnJwc3L17Fz///DMCAwPh7++PRo0aAQAmTpyIrVu3YtiwYZgxYwYMDAxw584dhIeHIzAwUJxe7+HhgV69euH69eviWgFFsbS0xE8//YQWLVogPT0d06dPLxC7XC5HZGQk2rdvDw0NDdSsWbNAPdOnT8f8+fPRsGFDNGvWDCEhIUhMTPyoldyJiIiIiKoTTi8vhlQqRevWrbF27Vp07NgRTZo0ga+vL8aNG4eNGzfC1tYW2tramDZtGpo1a4Y2bdpg9+7dCAwMhKen5wfrDw4OFh/F9L4BAwYgIiKiwKhjURo3bgwTExM0b94cy5YtQ5cuXXDt2jVxNPljuLq6IiMjQ7yfO5+zszMyMjJKfD83ALi5ueHgwYPo3bs3rKysMGrUKFhbW+P48ePiqK2Pjw9UVVVha2uL2rVrIyUlBV999RX69++PIUOGoHXr1khLS1MY9S6p/OenOzs7o3///vjyyy/F0XsAcHBwwJo1a7B8+XI0adIE27dvh7+/f5nUXRLz5s1D3bp10ahRI3h6euL58+eIjIzEzJkzxTL5o8m5ubno1q0bmjZtiilTpkBfX1/hnu1OnTrBwMAAN2/exPDhw4u9blBQEP799184OTnB09MTkydPLhD76tWrceLECZiamsLR0bHQeiZPnoxvv/0W06ZNQ9OmTXH06FFERETA0tKyVP1ARERERFTdSIRPedYVEVE5SU9Ph0wmg8OkAKhqlG4mARUUv3JkRYdAREREVKXl/336/PnzYtcf4kg3ERERERERkZIw6a7kunfvXuC52vnb0qVLKzo8kZ2dXZFx/pfv692+fXuR/fL+89GJiIiIiKj64UJqlVxgYCBevnxZ6DEDA4NyjqZoR44cQU5OTqHH6tSpU87RVB59+vRB69atCz32/mPOiIiIiIio+mHSXcl9yiO5ylNhz/omQFdXF7q6uhUdBhERERERVRBOLyciIiIiIiJSEibdRERERERERErC6eVEVKX8sXhYsY9kICIiIiKqTDjSTURERERERKQkTLqJiIiIiIiIlITTy4moShAEAQCQnp5ewZEQEREREf3/v0vz/04tCpNuIqoS0tLSAACmpqYVHAkRERER0f+XkZEBmUxW5HEm3URUJRgYGAAAUlJSiv2lRm+lp6fD1NQU9+/f58JzJcD+Kjn2Vemwv0qH/VVy7KvSYX+VHPuq5ARBQEZGBkxMTIotx6SbiKoEFZW3S1DIZDL+D6AU9PT02F+lwP4qOfZV6bC/Sof9VXLsq9Jhf5Uc+6pkSjIYxIXUiIiIiIiIiJSESTcRERERERGRkjDpJqIqQUNDA/Pnz4eGhkZFh1IlsL9Kh/1Vcuyr0mF/lQ77q+TYV6XD/io59lXZkwgfWt+ciIiIiIiIiD4KR7qJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhIm3URUYX744QfI5XJoamqidevWOH/+fLHlf/nlF1hbW0NTUxNNmzbFkSNHFI4LgoB58+ahbt260NLSQpcuXXD79m1lNqFclWV/5eTkYObMmWjatCl0dHRgYmKCkSNH4p9//lF2M8pFWX+23jV+/HhIJBJ8//33ZRx1xVFGfyUlJaFPnz6QyWTQ0dFBy5YtkZKSoqwmlJuy7qvMzEx4e3ujfv360NLSgq2tLQICApTZhHJVmv66fv06BgwYALlcXuy/sdK+B1VJWfeXv78/WrZsCV1dXRgZGeHzzz/HzZs3ldiC8qOMz1a+ZcuWQSKRYMqUKWUbdAVSRn/973//w4gRI2BoaAgtLS00bdoUFy9eVFILqjiBiKgChIeHC+rq6kJwcLBw/fp1Ydy4cYK+vr7w8OHDQsvHxsYKqqqqwooVK4QbN24Ic+fOFWrUqCFcvXpVLLNs2TJBJpMJ+/fvFy5fviz06dNHsLCwEF6+fFlezVKasu6vZ8+eCV26dBF27dol/N///Z9w5swZoVWrVkLz5s3Ls1lKoYzPVr69e/cKDg4OgomJibB27Volt6R8KKO/7ty5IxgYGAjTp08XLl26JNy5c0c4cOBAkXVWFcroq3HjxgkNGzYUoqKihLt37wqbN28WVFVVhQMHDpRXs5SmtP11/vx5wcfHR9i5c6dgbGxc6L+x0tZZlSijv9zc3ISQkBDh2rVrQmJiotCjRw/BzMxMyMzMVHJrlEsZffVuWblcLtjb2wvffPONchpQzpTRX0+fPhXMzc0FLy8v4dy5c8Kff/4pHDt2TLhz546SW1M1MekmogrRqlUrYeLEieLr3NxcwcTERPD39y+0/ODBg4WePXsq7GvdurXw1VdfCYIgCHl5eYKxsbGwcuVK8fizZ88EDQ0NYefOnUpoQfkq6/4qzPnz5wUAwl9//VU2QVcQZfXV33//LdSrV0+4du2aYG5uXm2SbmX015AhQ4QRI0YoJ+AKpIy+srOzE/z8/BTKODk5Cd99910ZRl4xSttf7yrq39in1FnZKaO/3vfo0SMBgHDq1KlPCbXCKauvMjIyBEtLS+HEiROCs7NztUm6ldFfM2fOFD777LOyDLNa4/RyIip3r1+/Rnx8PLp06SLuU1FRQZcuXXDmzJlCzzlz5oxCeQBwc3MTy9+9excPHjxQKCOTydC6desi66wqlNFfhXn+/DkkEgn09fXLJO6KoKy+ysvLg6enJ6ZPnw47OzvlBF8BlNFfeXl5OHz4MKysrODm5gYjIyO0bt0a+/fvV1o7yoOyPlvt2rVDREQE/ve//0EQBERFReHWrVvo1q2bchpSTj6mvyqizsqivNr2/PlzAICBgUGZ1VnelNlXEydORM+ePQv8u63KlNVfERERaNGiBQYNGgQjIyM4Ojpi69atZRFytcSkm4jK3ZMnT5Cbm4s6deoo7K9Tpw4ePHhQ6DkPHjwotnz+f0tTZ1WhjP5636tXrzBz5kwMGzYMenp6ZRN4BVBWXy1fvhxqamqYPHly2QddgZTRX48ePUJmZiaWLVsGd3d3HD9+HP369UP//v1x6tQp5TSkHCjrs7VhwwbY2tqifv36UFdXh7u7O3744Qd07Nix7BtRjj6mvyqizsqiPNqWl5eHKVOmoH379mjSpEmZ1FkRlNVX4eHhuHTpEvz9/T81xEpFWf31559/4scff4SlpSWOHTuGCRMmYPLkyQgLC/vUkKsltYoOgIiIKlZOTg4GDx4MQRDw448/VnQ4lU58fDzWrVuHS5cuQSKRVHQ4lV5eXh4AoG/fvpg6dSoAoFmzZoiLi0NAQACcnZ0rMrxKZ8OGDTh79iwiIiJgbm6OP/74AxMnToSJiUm1Gm2jijdx4kRcu3YNp0+fruhQKp379+/jm2++wYkTJ6CpqVnR4VQJeXl5aNGiBZYuXQoAcHR0xLVr1xAQEIBRo0ZVcHSVD0e6iajc1apVC6qqqnj48KHC/ocPH8LY2LjQc4yNjYstn//f0tRZVSijv/LlJ9x//fUXTpw4UaVHuQHl9FVMTAwePXoEMzMzqKmpQU1NDX/99RemTZsGuVyulHaUF2X0V61ataCmpgZbW1uFMjY2NlV69XJl9NXLly8xZ84crFmzBr1794a9vT28vb0xZMgQrFq1SjkNKScf018VUWdloey2eXt749ChQ4iKikL9+vU/ub6KpIy+io+Px6NHj+Dk5CT+nj916hTWr18PNTU15ObmlkXoFUJZn626detWu9/zysSkm4jKnbq6Opo3b47IyEhxX15eHiIjI9G2bdtCz2nbtq1CeQA4ceKEWN7CwgLGxsYKZdLT03Hu3Lki66wqlNFfwP9PuG/fvo2TJ0/C0NBQOQ0oR8roK09PT1y5cgWJiYniZmJigunTp+PYsWPKa0w5UEZ/qauro2XLlgUeS3Tr1i2Ym5uXcQvKjzL6KicnBzk5OVBRUfxzTFVVVZwxUFV9TH9VRJ2VhbLaJggCvL29sW/fPvz++++wsLAoi3ArlDL6qnPnzrh69arC7/kWLVrAw8MDiYmJUFVVLavwy52yPlvt27evdr/nlaqCF3Ijov+o8PBwQUNDQwgNDRVu3LghfPnll4K+vr7w4MEDQRAEwdPTU5g1a5ZYPjY2VlBTUxNWrVolJCUlCfPnzy/0kWH6+vrCgQMHhCtXrgh9+/atVo8MK8v+ev36tdCnTx+hfv36QmJiopCamipu2dnZFdLGsqKMz9b7qtPq5cror7179wo1atQQtmzZIty+fVvYsGGDoKqqKsTExJR7+8qSMvrK2dlZsLOzE6KiooQ///xTCAkJETQ1NYVNmzaVe/vKWmn7Kzs7W0hISBASEhKEunXrCj4+PkJCQoJw+/btEtdZlSmjvyZMmCDIZDIhOjpa4ff8ixcvyr19ZUkZffW+6rR6uTL66/z584KampqwZMkS4fbt28L27dsFbW1t4eeffy739lUFTLqJqMJs2LBBMDMzE9TV1YVWrVoJZ8+eFY85OzsLo0aNUii/e/duwcrKSlBXVxfs7OyEw4cPKxzPy8sTfH19hTp16ggaGhpC586dhZs3b5ZHU8pFWfbX3bt3BQCFblFRUeXUIuUp68/W+6pT0i0IyumvoKAgoVGjRoKmpqbg4OAg7N+/X9nNKBdl3VepqamCl5eXYGJiImhqagqNGzcWVq9eLeTl5ZVHc5SuNP1V1O8lZ2fnEtdZ1ZV1fxX1ez4kJKT8GqUkyvhsvas6Jd2CoJz+OnjwoNCkSRNBQ0NDsLa2FrZs2VJOral6JIIgCMofTyciIiIiIiL67+E93URERERERERKwqSbiIiIiIiISEmYdBMREREREREpCZNuIiIiIiIiIiVh0k1ERERERESkJEy6iYiIiIiIiJSESTcRERERERGRkjDpJiIiIiIiIlISJt1ERERERERESsKkm4iIiEhJvLy88Pnnn1d0GIW6d+8eJBIJEhMTKzoUIqJqjUk3ERER0X/M69evKzoEIqL/DCbdREREROXAxcUFkyZNwpQpU1CzZk3UqVMHW7duRVZWFr744gvo6uqiUaNG+O2338RzoqOjIZFIcPjwYdjb20NTUxNt2rTBtWvXFOr+9ddfYWdnBw0NDcjlcqxevVrhuFwux6JFizBy5Ejo6enhyy+/hIWFBQDA0dEREokELi4uAIALFy6ga9euqFWrFmQyGZydnXHp0iWF+iQSCQIDA9GvXz9oa2vD0tISERERCmWuX7+OXr16QU9PD7q6uujQoQOSk5PF44GBgbCxsYGmpiasra2xadOmT+5jIqLKiEk3ERERUTkJCwtDrVq1cP78eUyaNAkTJkzAoEGD0K5dO1y6dAndunWDp6cnXrx4oXDe9OnTsXr1aly4cAG1a9dG7969kZOTAwCIj4/H4MGDMXToUFy9ehULFiyAr68vQkNDFepYtWoVHBwckJCQAF9fX5w/fx4AcPLkSaSmpmLv3r0AgIyMDIwaNQqnT5/G2bNnYWlpiR49eiAjI0OhvoULF2Lw4MG4cuUKevToAQ8PDzx9+hQA8L///Q8dO3aEhoYGfv/9d8THx2P06NF48+YNAGD79u2YN28elixZgqSkJCxduhS+vr4ICwsr8z4nIqpoEkEQhIoOgoiIiKg68vLywrNnz7B//364uLggNzcXMTExAIDc3FzIZDL0798f27ZtAwA8ePAAdevWxZkzZ9CmTRtER0fD1dUV4eHhGDJkCADg6dOnqF+/PkJDQzF48GB4eHjg8ePHOH78uHjdGTNm4PDhw7h+/TqAtyPdjo6O2Ldvn1jm3r17sLCwQEJCApo1a1ZkG/Ly8qCvr48dO3agV69eAN6OdM+dOxeLFi0CAGRlZUEqleK3336Du7s75syZg/DwcNy8eRM1atQoUGejRo2waNEiDBs2TNy3ePFiHDlyBHFxcR/T1URElRZHuomIiIjKib29vfizqqoqDA0N0bRpU3FfnTp1AACPHj1SOK9t27bizwYGBmjcuDGSkpIAAElJSWjfvr1C+fbt2+P27dvIzc0V97Vo0aJEMT58+BDjxo2DpaUlZDIZ9PT0kJmZiZSUlCLboqOjAz09PTHuxMREdOjQodCEOysrC8nJyRgzZgykUqm4LV68WGH6ORFRdaFW0QEQERER/Ve8n4RKJBKFfRKJBMDb0eWypqOjU6Jyo0aNQlpaGtatWwdzc3NoaGigbdu2BRZfK6wt+XFraWkVWX9mZiYAYOvWrWjdurXCMVVV1RLFSERUlTDpJiIiIqrkzp49CzMzMwDAv//+i1u3bsHGxgYAYGNjg9jYWIXysbGxsLKyKjaJVVdXBwCF0fD8czdt2oQePXoAAO7fv48nT56UKl57e3uEhYUhJyenQHJep04dmJiY4M8//4SHh0ep6iUiqoqYdBMRERFVcn5+fjA0NESdOnXw3XffoVatWuLzv6dNm4aWLVti0aJFGDJkCM6cOYONGzd+cDVwIyMjaGlp4ejRo6hfvz40NTUhk8lgaWmJn376CS1atEB6ejqmT59e7Mh1Yby9vbFhwwYMHToUs2fPhkwmw9mzZ9GqVSs0btwYCxcuxOTJkyGTyeDu7o7s7GxcvHgR//77L7799tuP7SYiokqJ93QTERERVXLLli3DN998g+bNm+PBgwc4ePCgOFLt5OSE3bt3Izw8HE2aNMG8efPg5+cHLy+vYutUU1PD+vXrsXnzZpiYmKBv374AgKCgIPz7779wcnKCp6cnJk+eDCMjo1LFa2hoiN9//x2ZmZlwdnZG8+bNsXXrVnHUe+zYsQgMDERISAiaNm0KZ2dnhIaGio8xIyKqTrh6OREREVEllb96+b///gt9ff2KDoeIiD4CR7qJiIiIiIiIlIRJNxEREREREZGScHo5ERERERERkZJwpJuIiIiIiIhISZh0ExERERERESkJk24iIiIiIiIiJWHSTURERERERKQkTLqJiIiIiIiIlIRJNxEREREREZGSMOkmIiIiIiIiUhIm3URERERERERKwqSbiIiIiIiISEn+Hwc5Ze5Tp+UqAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# --- Plot Feature Importance (Using RF) ---\n","importances = rf.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","top_features = np.array(X.columns)[indices][:15]\n","\n","top_15_features_with_importance = [(X.columns[i], importances[i]) for i in indices[:15]]\n","\n","# Print the list\n","top_15_features_with_importance = [(X.columns[i], importances[i]) for i in indices[:15]]\n","print(\"Top 15 Feature Importances:\")\n","for feature, importance in top_15_features_with_importance:\n","    print(f\"- {feature}: {importance:.4f}\")\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=importances[indices][:15], y=top_features)\n","plt.title(\"Top 15 Feature Importances - Random Forest\")\n","plt.xlabel(\"Importance\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1750767133534,"user":{"displayName":"DELIENA TASHA ABDUL RAHIM","userId":"01342415897221284478"},"user_tz":-480},"id":"z5XuSmVCUhBY","outputId":"03a62a19-def7-48a1-ac11-b2b1816a8f72"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== Random Forest ===\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.99      0.99      3852\n","           1       0.99      1.00      0.99      4394\n","\n","    accuracy                           0.99      8246\n","   macro avg       0.99      0.99      0.99      8246\n","weighted avg       0.99      0.99      0.99      8246\n","\n","=== SVM ===\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.70      0.79      3852\n","           1       0.78      0.94      0.85      4394\n","\n","    accuracy                           0.83      8246\n","   macro avg       0.84      0.82      0.82      8246\n","weighted avg       0.84      0.83      0.82      8246\n","\n","=== LSTM ===\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.77      0.85      3852\n","           1       0.83      0.95      0.89      4394\n","\n","    accuracy                           0.87      8246\n","   macro avg       0.88      0.86      0.87      8246\n","weighted avg       0.88      0.87      0.87      8246\n","\n","=== XGBoost ===\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.98      3852\n","           1       0.97      0.99      0.98      4394\n","\n","    accuracy                           0.98      8246\n","   macro avg       0.98      0.98      0.98      8246\n","weighted avg       0.98      0.98      0.98      8246\n","\n","=== Decision Tree ===\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.99      3852\n","           1       0.98      0.99      0.99      4394\n","\n","    accuracy                           0.99      8246\n","   macro avg       0.99      0.99      0.99      8246\n","weighted avg       0.99      0.99      0.99      8246\n","\n","=== Multilayer Perceptron ===\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.90      0.92      3852\n","           1       0.91      0.95      0.93      4394\n","\n","    accuracy                           0.93      8246\n","   macro avg       0.93      0.92      0.93      8246\n","weighted avg       0.93      0.93      0.93      8246\n","\n"]}],"source":["# --- Classification Report Table ---\n","for name, preds in models.items():\n","    print(f\"=== {name} ===\")\n","    print(classification_report(y_test, preds))"]}],"metadata":{"colab":{"provenance":[{"file_id":"1RhkfJFDrMc_rRA-pH41rcCrwnTPDao9F","timestamp":1753250465021},{"file_id":"1oQeQoo6iCqRCG3ak1gI4a2v8_xvYlwMZ","timestamp":1750650518106}],"mount_file_id":"1oQeQoo6iCqRCG3ak1gI4a2v8_xvYlwMZ","authorship_tag":"ABX9TyOg+V8DG+CGSFDKmO+kEQJa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}